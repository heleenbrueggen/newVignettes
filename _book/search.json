[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "newVignettesBook",
    "section": "",
    "text": "Preface\n\n\n\nDOI\n\n\n\n\nMice vignettes in a new style.\nNew and improved quarto-based mice vignettes with ggmice plots, accessible data for sensitivity analysis and many bug-fixes.\nWe suggest going through these vignettes in the following order\n\n1. Ad Hoc methods and the mice algorithm\n2. Convergence and pooling\n3. Inspecting how the observed data and missingness are related\n4. Passive imputation and post-processing\n5. Combining inferences\n6. Imputing multi-level data\n7. Sensitivity analysis with mice\n\nGenerating the synthetic set in Vignette 7\n\n8. futuremice: Wrapper for parallel mice imputation through futures\n9. Multivariate PMM for imputing multiple columns simultaneously\n10. Generating synthetic data with high utility using mice\n\n\nAll the best,\nThe mice development team\n\n\n\nContributors\nWith contributions by (in alphabetical order):\n\nGerko Vink\nHanne Oberman\nHuma Shehwana\nMingyang Cai\nStef van Buuren\nThom Volker\n\n\n\nLicense\nShield: \nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\n\nCC BY 4.0",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1AdHoc/Adhoc_methods_and_mice.html",
    "href": "1AdHoc/Adhoc_methods_and_mice.html",
    "title": "1  Ad hoc methods and mice",
    "section": "",
    "text": "This is the first vignette in a series of ten. It will give you an introduction to the R-package mice, an open-source tool for flexible imputation of incomplete data, developed by Stef van Buuren and Karin Groothuis-Oudshoorn -Van Buuren and Groothuis-Oudshoorn (2011). Over the last decade, mice has become an important piece of imputation software, offering a very flexible environment for dealing with incomplete data. Moreover, the ability to integrate mice with other packages in R, and vice versa, offers many options for applied researchers.\nThe aim of this introduction is to enhance your understanding of multiple imputation, in general. You will learn how to multiply impute simple datasets and how to obtain the imputed data for further analysis. The main objective is to increase your knowledge and understanding on applications of multiple imputation.\nNo previous experience with R is required.\n\n\n1.0.0.1 Working with mice\n\n1. Open R and load the packages mice, ggmice, and ggplot2\n\nlibrary(mice)\nlibrary(ggmice)\nlibrary(ggplot2)\nset.seed(123)\n\nIf mice is not yet installed, run:\n\ninstall.packages(\"mice\")\n\n\n2. Inspect the incomplete data\nThe mice package contains several datasets. Once the package is loaded, these datasets can be used. Have a look at the nhanes dataset (Schafer 1997, Table 6.14) by typing\n\nnhanes\n\n   age  bmi hyp chl\n1    1   NA  NA  NA\n2    2 22.7   1 187\n3    1   NA   1 187\n4    3   NA  NA  NA\n5    1 20.4   1 113\n6    3   NA  NA 184\n7    1 22.5   1 118\n8    1 30.1   1 187\n9    2 22.0   1 238\n10   2   NA  NA  NA\n11   1   NA  NA  NA\n12   2   NA  NA  NA\n13   3 21.7   1 206\n14   2 28.7   2 204\n15   1 29.6   1  NA\n16   1   NA  NA  NA\n17   3 27.2   2 284\n18   2 26.3   2 199\n19   1 35.3   1 218\n20   3 25.5   2  NA\n21   1   NA  NA  NA\n22   1 33.2   1 229\n23   1 27.5   1 131\n24   3 24.9   1  NA\n25   2 27.4   1 186\n\n\nThe nhanes dataset is a small data set with non-monotone missing values. It contains 25 observations on four variables: age group, body mass index, hypertension and cholesterol (mg/dL).\nTo learn more about the data, use one of the two following help commands:\n\nhelp(nhanes)\n?nhanes\n\n\n3. Get an overview of the data by the summary() command:\n\nsummary(nhanes)\n\n      age            bmi             hyp             chl       \n Min.   :1.00   Min.   :20.40   Min.   :1.000   Min.   :113.0  \n 1st Qu.:1.00   1st Qu.:22.65   1st Qu.:1.000   1st Qu.:185.0  \n Median :2.00   Median :26.75   Median :1.000   Median :187.0  \n Mean   :1.76   Mean   :26.56   Mean   :1.235   Mean   :191.4  \n 3rd Qu.:2.00   3rd Qu.:28.93   3rd Qu.:1.000   3rd Qu.:212.0  \n Max.   :3.00   Max.   :35.30   Max.   :2.000   Max.   :284.0  \n                NA's   :9       NA's   :8       NA's   :10     \n\n\n\n4. Inspect the missing data pattern\nCheck the missingness pattern for the nhanes dataset\n\nplot_pattern(nhanes)\n\n\n\n\n\n\n\n\nThe missingness pattern shows that there are 27 missing values in total: 10 for chl , 9 for bmi and 8 for hyp. Moreover, there are thirteen completely observed rows, four rows with 1 missing, one row with 2 missings and seven rows with 3 missings. Looking at the missing data pattern is always useful (but may be difficult for datasets with many variables). It can give you an indication on how much information is missing and how the missingness is distributed.\n\n\n1.0.0.1.1 Ad Hoc imputation methods\n\n5. Form a regression model where age is predicted from bmi. \n\nfit &lt;- with(nhanes, lm(age ~ bmi))\nsummary(fit)\n\n\nCall:\nlm(formula = age ~ bmi)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2660 -0.5614 -0.1225  0.4660  1.2344 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  3.76718    1.31945   2.855   0.0127 *\nbmi         -0.07359    0.04910  -1.499   0.1561  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8015 on 14 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.1383,    Adjusted R-squared:  0.07672 \nF-statistic: 2.246 on 1 and 14 DF,  p-value: 0.1561\n\n\n\n6. Impute the missing data in the nhanes dataset with mean imputation. \n\nimp &lt;- mice(nhanes, method = \"mean\", m = 1, maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\n\nThe imputations are now done. As you can see, the algorithm ran for 1 iteration (maxit = 1) and presented us with only 1 imputation (m = 1) for each missing datum. This is correct, as substituting each missing data multiple times with the observed data mean would not make any sense (the inference would be equal, no matter which imputed dataset we would analyze). Likewise, more iterations would be computationally inefficient as the observed data mean does not change based on our imputations. We named the imputed object imp following the convention used in mice, but if you wish you can name it anything you’d like.\n\n7. Explore the imputed data with the complete() function. What do you think the variable means are? What happened to the regression equation after imputation?\n\ncomplete(imp)\n\n   age     bmi      hyp   chl\n1    1 26.5625 1.235294 191.4\n2    2 22.7000 1.000000 187.0\n3    1 26.5625 1.000000 187.0\n4    3 26.5625 1.235294 191.4\n5    1 20.4000 1.000000 113.0\n6    3 26.5625 1.235294 184.0\n7    1 22.5000 1.000000 118.0\n8    1 30.1000 1.000000 187.0\n9    2 22.0000 1.000000 238.0\n10   2 26.5625 1.235294 191.4\n11   1 26.5625 1.235294 191.4\n12   2 26.5625 1.235294 191.4\n13   3 21.7000 1.000000 206.0\n14   2 28.7000 2.000000 204.0\n15   1 29.6000 1.000000 191.4\n16   1 26.5625 1.235294 191.4\n17   3 27.2000 2.000000 284.0\n18   2 26.3000 2.000000 199.0\n19   1 35.3000 1.000000 218.0\n20   3 25.5000 2.000000 191.4\n21   1 26.5625 1.235294 191.4\n22   1 33.2000 1.000000 229.0\n23   1 27.5000 1.000000 131.0\n24   3 24.9000 1.000000 191.4\n25   2 27.4000 1.000000 186.0\n\n\nWe see the repetitive numbers 26.5625 for bmi, 1.2352594 for hyp, and 191.4 for chl. These can be confirmed as the means of the respective variables (columns):\n\ncolMeans(nhanes, na.rm = TRUE)\n\n       age        bmi        hyp        chl \n  1.760000  26.562500   1.235294 191.400000 \n\n\nWe saw during the inspection of the missing data pattern that variable age has no missings. Therefore nothing is imputed for age because we would not want to alter the observed (and bonafide) values.\nTo inspect the regression model with the imputed data, run:\n\nfit &lt;- with(imp, lm(age ~ bmi))\nsummary(fit)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value  nobs df.residual\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 (Intercept)   3.71      1.33        2.80  0.0103    25          23\n2 bmi          -0.0736    0.0497     -1.48  0.152     25          23\n\n\nIt is clear that nothing changed, but then again this is not surprising as variable bmi is somewhat normally distributed and we are just adding weight to the mean.\nThe imputed data can be inspected visually by comparing the distribution of the observed data to the distribution of the imputed data:\n\nggmice(nhanes, aes(bmi)) +\n        geom_histogram(fill = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 9 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nggmice(imp, aes(bmi)) + \n        geom_histogram(fill = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nEvery missing instance of bmi is filled in with the average observed bmi value.\n\n8. Impute the missing data in the nhanes dataset with regression imputation. \n\nimp &lt;- mice(nhanes, method = \"norm.predict\", m = 1, maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\n\nThe imputations are now done. This code imputes the missing values in the data set by the regression imputation method. The argument method = \"norm.predict\" first fits a regression model for each observed value, based on the corresponding values in other variables and then imputes the missing values with the predicted values.\n\n9. Again, inspect the completed data and investigate the imputed data regression model. \n\ncomplete(imp)\n\n   age      bmi       hyp      chl\n1    1 28.36021 1.0474831 172.4557\n2    2 22.70000 1.0000000 187.0000\n3    1 28.36021 1.0000000 187.0000\n4    3 22.80609 1.5088506 222.7836\n5    1 20.40000 1.0000000 113.0000\n6    3 22.68531 1.5019433 184.0000\n7    1 22.50000 1.0000000 118.0000\n8    1 30.10000 1.0000000 187.0000\n9    2 22.00000 1.0000000 238.0000\n10   2 27.04536 1.3053438 208.0862\n11   1 29.82242 1.0746600 182.9223\n12   2 25.46237 1.2712595 196.7785\n13   3 21.70000 1.0000000 206.0000\n14   2 28.70000 2.0000000 204.0000\n15   1 29.60000 1.0000000 181.6849\n16   1 25.58231 0.8886142 153.1107\n17   3 27.20000 2.0000000 284.0000\n18   2 26.30000 2.0000000 199.0000\n19   1 35.30000 1.0000000 218.0000\n20   3 25.50000 2.0000000 239.8485\n21   1 28.31995 1.0451807 172.1753\n22   1 33.20000 1.0000000 229.0000\n23   1 27.50000 1.0000000 131.0000\n24   3 24.90000 1.0000000 240.5268\n25   2 27.40000 1.0000000 186.0000\n\n\nThe repetitive numbering is gone. We have now obtained a more natural looking set of imputations: instead of filling in the same bmi for all ages, we now take age (as well as hyp and chl) into account when imputing bmi.\nTo inspect the regression model with the imputed data, run:\n\nfit &lt;- with(imp, lm(age ~ bmi))\nsummary(fit)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value  nobs df.residual\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 (Intercept)    4.68     1.12        4.20 0.000345    25          23\n2 bmi           -0.110    0.0417     -2.64 0.0146      25          23\n\n\nIt is clear that something has changed. In fact, we extrapolated (part of) the regression model for the observed data to missing data in bmi. In other words; the relation (read: information) gets stronger and we’ve obtained more observations.\nThe imputed data can be inspected visually with:\n\nggmice(imp, aes(bmi)) + \n  geom_histogram(fill = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe imputed values fall within the range of observed bmi values.\n\n10. Impute the missing data in the nhanes dataset with stochastic regression imputation.\n\nimp &lt;- mice(nhanes, method = \"norm.nob\", m = 1, maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\n\nThe imputations are now done. This code imputes the missing values in the data set by the stochastic regression imputation method. The function does not incorporate the variability of the regression weights, so it is not ‘proper’ in the sense of Rubin (2004). For small samples, the variability of the imputed data will be underestimated.\n\n11. Again, inspect the completed data and investigate the imputed data regression model. \n\ncomplete(imp)\n\n   age      bmi       hyp      chl\n1    1 33.61471 1.0064774 200.0025\n2    2 22.70000 1.0000000 187.0000\n3    1 32.36556 1.0000000 187.0000\n4    3 29.94711 1.2555111 291.6824\n5    1 20.40000 1.0000000 113.0000\n6    3 20.02676 1.5286735 184.0000\n7    1 22.50000 1.0000000 118.0000\n8    1 30.10000 1.0000000 187.0000\n9    2 22.00000 1.0000000 238.0000\n10   2 20.09440 0.9498627 192.3497\n11   1 32.65078 1.1459837 211.3078\n12   2 20.12858 1.3255888 148.9863\n13   3 21.70000 1.0000000 206.0000\n14   2 28.70000 2.0000000 204.0000\n15   1 29.60000 1.0000000 210.7834\n16   1 26.85249 0.7870282 187.5259\n17   3 27.20000 2.0000000 284.0000\n18   2 26.30000 2.0000000 199.0000\n19   1 35.30000 1.0000000 218.0000\n20   3 25.50000 2.0000000 261.4307\n21   1 36.35340 1.4367806 230.8058\n22   1 33.20000 1.0000000 229.0000\n23   1 27.50000 1.0000000 131.0000\n24   3 24.90000 1.0000000 228.5297\n25   2 27.40000 1.0000000 186.0000\n\n\nWe have once more obtained a more natural looking set of imputations, where instead of filling in the same bmi for all ages, we now take age (as well as hyp and chl) into account when imputing bmi. We also add a random error to allow for our imputations to be off the regression line.\nTo inspect the regression model with the imputed data, run:\n\nfit &lt;- with(imp, lm(age ~ bmi))\nsummary(fit)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic   p.value  nobs df.residual\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 (Intercept)   3.91      0.826       4.73 0.0000907    25          23\n2 bmi          -0.0793    0.0300     -2.64 0.0145       25          23\n\n\nThe imputed data can be inspected visually with:\n\nggmice(imp, aes(bmi)) + \n  geom_histogram(fill = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThis imputation method better reflects the uncertainty in the imputed data than previous methods.\n\n12. Re-run the stochastic imputation model with seed 123 and verify if your results are the same as the ones below\n\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic   p.value  nobs df.residual\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 (Intercept)   3.75      0.736       5.10 0.0000362    25          23\n2 bmi          -0.0792    0.0287     -2.77 0.0110       25          23\n\n\nThe imputation procedure uses random sampling, and therefore, the results will be (perhaps slightly) different if we repeat the imputations. In order to get exactly the same result, you can use the seed argument\n\nimp &lt;- mice(nhanes, method = \"norm.nob\", m = 1, maxit = 1, seed = 123)\nfit &lt;- with(imp, lm(age ~ bmi))\nsummary(fit)\n\nwhere 123 is some arbitrary number that you can choose yourself. Re-running this command will always yields the same imputed values. The ability to replicate one’s findings exactly is considered essential in today’s reproducible science.\n\n\n\n\n1.0.0.2 Multiple imputation\n\n13. Let us impute the missing data in the nhanes dataset\n\nimp &lt;- mice(nhanes)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  1   2  bmi  hyp  chl\n  1   3  bmi  hyp  chl\n  1   4  bmi  hyp  chl\n  1   5  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  2   2  bmi  hyp  chl\n  2   3  bmi  hyp  chl\n  2   4  bmi  hyp  chl\n  2   5  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  3   2  bmi  hyp  chl\n  3   3  bmi  hyp  chl\n  3   4  bmi  hyp  chl\n  3   5  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  4   2  bmi  hyp  chl\n  4   3  bmi  hyp  chl\n  4   4  bmi  hyp  chl\n  4   5  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  5   2  bmi  hyp  chl\n  5   3  bmi  hyp  chl\n  5   4  bmi  hyp  chl\n  5   5  bmi  hyp  chl\n\nimp\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\nThe imputations are now done. As you can see, the algorithm ran for 5 iterations (the default) and presented us with 5 imputations for each missing datum. For the rest of this document we will omit printing of the iteration cycle when we run mice. We do so by adding print=F to the mice call.\nThe object imp contains a multiply imputed data set (of class mids). It encapsulates all information from imputing the nhanes dataset, such as the original data, the imputed values, the number of missing values, number of iterations, and so on.\nTo obtain an overview of the information stored in the object imp, use the attributes() function:\n\nattributes(imp)\n\n$names\n [1] \"data\"            \"imp\"             \"m\"               \"where\"          \n [5] \"blocks\"          \"call\"            \"nmis\"            \"method\"         \n [9] \"predictorMatrix\" \"visitSequence\"   \"formulas\"        \"post\"           \n[13] \"blots\"           \"ignore\"          \"seed\"            \"iteration\"      \n[17] \"lastSeedValue\"   \"chainMean\"       \"chainVar\"        \"loggedEvents\"   \n[21] \"version\"         \"date\"           \n\n$class\n[1] \"mids\"\n\n\nFor example, the original data are stored as\n\nimp$data\n\n   age  bmi hyp chl\n1    1   NA  NA  NA\n2    2 22.7   1 187\n3    1   NA   1 187\n4    3   NA  NA  NA\n5    1 20.4   1 113\n6    3   NA  NA 184\n7    1 22.5   1 118\n8    1 30.1   1 187\n9    2 22.0   1 238\n10   2   NA  NA  NA\n11   1   NA  NA  NA\n12   2   NA  NA  NA\n13   3 21.7   1 206\n14   2 28.7   2 204\n15   1 29.6   1  NA\n16   1   NA  NA  NA\n17   3 27.2   2 284\n18   2 26.3   2 199\n19   1 35.3   1 218\n20   3 25.5   2  NA\n21   1   NA  NA  NA\n22   1 33.2   1 229\n23   1 27.5   1 131\n24   3 24.9   1  NA\n25   2 27.4   1 186\n\n\nand the imputations are stored as\n\nimp$imp\n\n$age\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$bmi\n      1    2    3    4    5\n1  24.9 25.5 27.4 22.0 28.7\n3  28.7 22.0 29.6 28.7 22.0\n4  27.4 29.6 20.4 25.5 20.4\n6  25.5 21.7 24.9 20.4 27.4\n10 21.7 20.4 27.4 27.5 28.7\n11 22.5 33.2 22.0 33.2 35.3\n12 27.2 27.5 27.5 27.2 22.5\n16 30.1 22.5 27.5 30.1 27.2\n21 30.1 30.1 28.7 28.7 22.0\n\n$hyp\n   1 2 3 4 5\n1  1 1 1 1 1\n4  2 1 2 1 1\n6  2 2 2 2 2\n10 2 1 1 1 1\n11 1 1 1 1 1\n12 1 2 1 2 1\n16 1 1 1 1 1\n21 1 1 1 1 1\n\n$chl\n     1   2   3   4   5\n1  187 238 187 131 187\n4  206 218 187 186 204\n10 187 113 229 186 206\n11 113 206 187 229 229\n12 206 184 206 218 199\n15 229 184 238 187 187\n16 187 238 187 184 238\n20 186 206 184 284 229\n21 199 229 238 187 238\n24 184 284 186 204 206\n\n\n\n14. Extract the completed data\nBy default, mice() calculates five (m = 5) imputed data sets. In order to get the third imputed data set, use the complete() function\n\nc3 &lt;- complete(imp, 3) \nplot_pattern(c3)\n\n /\\     /\\\n{  `---'  }\n{  O   O  }\n==&gt;  V &lt;==  No need for mice. This data set is completely observed.\n \\  \\|/  /\n  `-----'\n\n\n\n\n\n\n\n\n\nThe collection of the \\(m\\) imputed data sets can be exported by function complete() in long, broad and repeated formats. For example,\n\nc.long &lt;- complete(imp, \"long\")  \nc.long\n\n    age  bmi hyp chl .imp .id\n1     1 24.9   1 187    1   1\n2     2 22.7   1 187    1   2\n3     1 28.7   1 187    1   3\n4     3 27.4   2 206    1   4\n5     1 20.4   1 113    1   5\n6     3 25.5   2 184    1   6\n7     1 22.5   1 118    1   7\n8     1 30.1   1 187    1   8\n9     2 22.0   1 238    1   9\n10    2 21.7   2 187    1  10\n11    1 22.5   1 113    1  11\n12    2 27.2   1 206    1  12\n13    3 21.7   1 206    1  13\n14    2 28.7   2 204    1  14\n15    1 29.6   1 229    1  15\n16    1 30.1   1 187    1  16\n17    3 27.2   2 284    1  17\n18    2 26.3   2 199    1  18\n19    1 35.3   1 218    1  19\n20    3 25.5   2 186    1  20\n21    1 30.1   1 199    1  21\n22    1 33.2   1 229    1  22\n23    1 27.5   1 131    1  23\n24    3 24.9   1 184    1  24\n25    2 27.4   1 186    1  25\n26    1 25.5   1 238    2   1\n27    2 22.7   1 187    2   2\n28    1 22.0   1 187    2   3\n29    3 29.6   1 218    2   4\n30    1 20.4   1 113    2   5\n31    3 21.7   2 184    2   6\n32    1 22.5   1 118    2   7\n33    1 30.1   1 187    2   8\n34    2 22.0   1 238    2   9\n35    2 20.4   1 113    2  10\n36    1 33.2   1 206    2  11\n37    2 27.5   2 184    2  12\n38    3 21.7   1 206    2  13\n39    2 28.7   2 204    2  14\n40    1 29.6   1 184    2  15\n41    1 22.5   1 238    2  16\n42    3 27.2   2 284    2  17\n43    2 26.3   2 199    2  18\n44    1 35.3   1 218    2  19\n45    3 25.5   2 206    2  20\n46    1 30.1   1 229    2  21\n47    1 33.2   1 229    2  22\n48    1 27.5   1 131    2  23\n49    3 24.9   1 284    2  24\n50    2 27.4   1 186    2  25\n51    1 27.4   1 187    3   1\n52    2 22.7   1 187    3   2\n53    1 29.6   1 187    3   3\n54    3 20.4   2 187    3   4\n55    1 20.4   1 113    3   5\n56    3 24.9   2 184    3   6\n57    1 22.5   1 118    3   7\n58    1 30.1   1 187    3   8\n59    2 22.0   1 238    3   9\n60    2 27.4   1 229    3  10\n61    1 22.0   1 187    3  11\n62    2 27.5   1 206    3  12\n63    3 21.7   1 206    3  13\n64    2 28.7   2 204    3  14\n65    1 29.6   1 238    3  15\n66    1 27.5   1 187    3  16\n67    3 27.2   2 284    3  17\n68    2 26.3   2 199    3  18\n69    1 35.3   1 218    3  19\n70    3 25.5   2 184    3  20\n71    1 28.7   1 238    3  21\n72    1 33.2   1 229    3  22\n73    1 27.5   1 131    3  23\n74    3 24.9   1 186    3  24\n75    2 27.4   1 186    3  25\n76    1 22.0   1 131    4   1\n77    2 22.7   1 187    4   2\n78    1 28.7   1 187    4   3\n79    3 25.5   1 186    4   4\n80    1 20.4   1 113    4   5\n81    3 20.4   2 184    4   6\n82    1 22.5   1 118    4   7\n83    1 30.1   1 187    4   8\n84    2 22.0   1 238    4   9\n85    2 27.5   1 186    4  10\n86    1 33.2   1 229    4  11\n87    2 27.2   2 218    4  12\n88    3 21.7   1 206    4  13\n89    2 28.7   2 204    4  14\n90    1 29.6   1 187    4  15\n91    1 30.1   1 184    4  16\n92    3 27.2   2 284    4  17\n93    2 26.3   2 199    4  18\n94    1 35.3   1 218    4  19\n95    3 25.5   2 284    4  20\n96    1 28.7   1 187    4  21\n97    1 33.2   1 229    4  22\n98    1 27.5   1 131    4  23\n99    3 24.9   1 204    4  24\n100   2 27.4   1 186    4  25\n101   1 28.7   1 187    5   1\n102   2 22.7   1 187    5   2\n103   1 22.0   1 187    5   3\n104   3 20.4   1 204    5   4\n105   1 20.4   1 113    5   5\n106   3 27.4   2 184    5   6\n107   1 22.5   1 118    5   7\n108   1 30.1   1 187    5   8\n109   2 22.0   1 238    5   9\n110   2 28.7   1 206    5  10\n111   1 35.3   1 229    5  11\n112   2 22.5   1 199    5  12\n113   3 21.7   1 206    5  13\n114   2 28.7   2 204    5  14\n115   1 29.6   1 187    5  15\n116   1 27.2   1 238    5  16\n117   3 27.2   2 284    5  17\n118   2 26.3   2 199    5  18\n119   1 35.3   1 218    5  19\n120   3 25.5   2 229    5  20\n121   1 22.0   1 238    5  21\n122   1 33.2   1 229    5  22\n123   1 27.5   1 131    5  23\n124   3 24.9   1 206    5  24\n125   2 27.4   1 186    5  25\n\n\nand\n\nc.broad &lt;- complete(imp, \"broad\")\n\nNew names:\n• `age` -&gt; `age...1`\n• `bmi` -&gt; `bmi...2`\n• `hyp` -&gt; `hyp...3`\n• `chl` -&gt; `chl...4`\n• `age` -&gt; `age...5`\n• `bmi` -&gt; `bmi...6`\n• `hyp` -&gt; `hyp...7`\n• `chl` -&gt; `chl...8`\n• `age` -&gt; `age...9`\n• `bmi` -&gt; `bmi...10`\n• `hyp` -&gt; `hyp...11`\n• `chl` -&gt; `chl...12`\n• `age` -&gt; `age...13`\n• `bmi` -&gt; `bmi...14`\n• `hyp` -&gt; `hyp...15`\n• `chl` -&gt; `chl...16`\n• `age` -&gt; `age...17`\n• `bmi` -&gt; `bmi...18`\n• `hyp` -&gt; `hyp...19`\n• `chl` -&gt; `chl...20`\n\nc.broad\n\n   age.1 bmi.1 hyp.1 chl.1 age.2 bmi.2 hyp.2 chl.2 age.3 bmi.3 hyp.3 chl.3\n1      1  24.9     1   187     1  25.5     1   238     1  27.4     1   187\n2      2  22.7     1   187     2  22.7     1   187     2  22.7     1   187\n3      1  28.7     1   187     1  22.0     1   187     1  29.6     1   187\n4      3  27.4     2   206     3  29.6     1   218     3  20.4     2   187\n5      1  20.4     1   113     1  20.4     1   113     1  20.4     1   113\n6      3  25.5     2   184     3  21.7     2   184     3  24.9     2   184\n7      1  22.5     1   118     1  22.5     1   118     1  22.5     1   118\n8      1  30.1     1   187     1  30.1     1   187     1  30.1     1   187\n9      2  22.0     1   238     2  22.0     1   238     2  22.0     1   238\n10     2  21.7     2   187     2  20.4     1   113     2  27.4     1   229\n11     1  22.5     1   113     1  33.2     1   206     1  22.0     1   187\n12     2  27.2     1   206     2  27.5     2   184     2  27.5     1   206\n13     3  21.7     1   206     3  21.7     1   206     3  21.7     1   206\n14     2  28.7     2   204     2  28.7     2   204     2  28.7     2   204\n15     1  29.6     1   229     1  29.6     1   184     1  29.6     1   238\n16     1  30.1     1   187     1  22.5     1   238     1  27.5     1   187\n17     3  27.2     2   284     3  27.2     2   284     3  27.2     2   284\n18     2  26.3     2   199     2  26.3     2   199     2  26.3     2   199\n19     1  35.3     1   218     1  35.3     1   218     1  35.3     1   218\n20     3  25.5     2   186     3  25.5     2   206     3  25.5     2   184\n21     1  30.1     1   199     1  30.1     1   229     1  28.7     1   238\n22     1  33.2     1   229     1  33.2     1   229     1  33.2     1   229\n23     1  27.5     1   131     1  27.5     1   131     1  27.5     1   131\n24     3  24.9     1   184     3  24.9     1   284     3  24.9     1   186\n25     2  27.4     1   186     2  27.4     1   186     2  27.4     1   186\n   age.4 bmi.4 hyp.4 chl.4 age.5 bmi.5 hyp.5 chl.5\n1      1  22.0     1   131     1  28.7     1   187\n2      2  22.7     1   187     2  22.7     1   187\n3      1  28.7     1   187     1  22.0     1   187\n4      3  25.5     1   186     3  20.4     1   204\n5      1  20.4     1   113     1  20.4     1   113\n6      3  20.4     2   184     3  27.4     2   184\n7      1  22.5     1   118     1  22.5     1   118\n8      1  30.1     1   187     1  30.1     1   187\n9      2  22.0     1   238     2  22.0     1   238\n10     2  27.5     1   186     2  28.7     1   206\n11     1  33.2     1   229     1  35.3     1   229\n12     2  27.2     2   218     2  22.5     1   199\n13     3  21.7     1   206     3  21.7     1   206\n14     2  28.7     2   204     2  28.7     2   204\n15     1  29.6     1   187     1  29.6     1   187\n16     1  30.1     1   184     1  27.2     1   238\n17     3  27.2     2   284     3  27.2     2   284\n18     2  26.3     2   199     2  26.3     2   199\n19     1  35.3     1   218     1  35.3     1   218\n20     3  25.5     2   284     3  25.5     2   229\n21     1  28.7     1   187     1  22.0     1   238\n22     1  33.2     1   229     1  33.2     1   229\n23     1  27.5     1   131     1  27.5     1   131\n24     3  24.9     1   204     3  24.9     1   206\n25     2  27.4     1   186     2  27.4     1   186\n\n\nare completed data sets in long and broad format, respectively. See ?complete for more detail.\n\nConclusion\nWe have seen that (multiple) imputation is straightforward with mice. However, don’t let the simplicity of the software fool you into thinking that the problem itself is also straightforward. In the next vignette we will therefore explore how the mice package can flexibly provide us the tools to assess and control the imputation of missing data.\n\n- End of Vignette\n\n\n\n\n\nRubin, Donald B. 2004. Multiple Imputation for Nonresponse in Surveys. Vol. 81. John Wiley & Sons.\n\n\nSchafer, Joseph L. 1997. Analysis of Incomplete Multivariate Data. CRC press.\n\n\nVan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software 45: 1–67.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ad hoc methods and `mice`</span>"
    ]
  },
  {
    "objectID": "2ConvPool/Convergence_and_pooling.html",
    "href": "2ConvPool/Convergence_and_pooling.html",
    "title": "2  mice: Algorithmic convergence and inference pooling",
    "section": "",
    "text": "This is the second vignette in a series of ten.\nThe aim of this vignette is to enhance your understanding of multiple imputation, in general. You will learn how to pool the results of analyses performed on multiply-imputed data, how to approach different types of data and how to avoid the pitfalls researchers may fall into. The main objective is to increase your knowledge and understanding on applications of multiple imputation.\nNo previous experience with R is required. Again, we start by loading the necessary packages and fixing the random seed to allow for our outcomes to be replicable.\n\nlibrary(mice)\nlibrary(ggmice)\nlibrary(ggplot2)\nset.seed(123)\n\n\n1. Vary the number of imputations.\nThe number of imputed data sets can be specified by the m = ... argument. For example, to create just three imputed data sets, specify\n\nimp &lt;- mice(nhanes, m = 3, print = FALSE)\n\n\n2. Change the predictor matrix\nThe predictor matrix is a square matrix that specifies the variables that are used to impute each incomplete variable. Let us have a look at the predictor matrix that was used\n\nimp$pred\n\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\nEach variable in the data has a row and a column in the potential predictor matrix. A value 1 indicates that the column variable can be used to impute the row variable. For example, the 1 at entry [bmi, age] indicates that variable age can be potentially used to impute the incomplete variable bmi. Note that the diagonal is zero because a variable is not allowed to impute itself. The row of age contains all one indicating that all remaining three variables (bmi, hyp, chl) can potentially be used to impute missing values in age variable. However, since there are no missing values in age, there will be no imputation. However, it is important to note that mice gives you complete control over the predictor matrix, enabling you to choose your own predictor relations. This can be very useful, for example, when you have many variables or when you have clear ideas or prior knowledge about relations in the data at hand. You can use mice() to give you the initial potential predictor matrix, and change it afterwards, without running the algorithm. This can be done by typing\n\nini &lt;- mice(nhanes, maxit = 0, print = FALSE)\npred &lt;- ini$pred\npred\n\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\nThe object pred contains the predictor matrix from an initial run of mice with zero iterations, specified by maxit = 0. Altering the predictor matrix and returning it to the mice algorithm is very simple. For example, the following code removes the variable hyp from the set of predictors, but still leaves it to be predicted by the other variables.\n\npred[ ,\"hyp\"] &lt;- 0\npred\n\n    age bmi hyp chl\nage   0   1   0   1\nbmi   1   0   0   1\nhyp   1   1   0   1\nchl   1   1   0   0\n\n\nThe predictor matrix can be visualized with\n\nplot_pred(pred)\n\n\n\n\n\n\n\n\nUse your new predictor matrix in mice() as follows\n\nimp &lt;- mice(nhanes, pred = pred, print = FALSE)\n\nThere is a special function called quickpred() for a quick selection procedure of predictors, which can be handy for datasets containing many variables. See ?quickpred for more info. Selecting predictors according to data relations with a minimum correlation of \\(\\rho=.30\\) can be done by\n\nini &lt;- mice(nhanes, pred = quickpred(nhanes, mincor = .3), print = FALSE)\nini$pred\n\n    age bmi hyp chl\nage   0   0   0   0\nbmi   1   0   0   1\nhyp   1   0   0   1\nchl   1   1   1   0\n\n\nFor large predictor matrices, it can be useful to export them to Microsoft Excel for easier configuration (e.g. see the xlsx package for easy exporting and importing of Excel files).\nVisualizing the predictor matrix in R is done with\n\nplot_pred(ini$pred, method = ini$meth)\n\n\n\n\n\n\n\n\n\n3. Inspect the convergence of the algorithm\nThe mice() function implements an iterative Markov Chain Monte Carlo type of algorithm. Let us have a look at the trace lines generated by the algorithm to study convergence:\n\nimp &lt;- mice(nhanes, print = FALSE)\nplot_trace(imp)\n\n\n\n\n\n\n\n\nThe plot shows the mean (left) and standard deviation (right) of the imputed values only. In general, we would like the streams to intermingle and be free of any trends at the later iterations.\nThe algorithm uses random sampling, and therefore, the results will be (perhaps slightly) different if we repeat the imputations with different seeds. In order to get exactly the same result, use the seed argument\n\nimp &lt;- mice(nhanes, seed = 123, print = FALSE)\n\nwhere 123 is some arbitrary number that you can choose yourself. Rerunning this command will always yields the same imputed values.\n\n4. Change the imputation method\nFor each column, the algorithm requires a specification of the imputation method. To see which method was used by default:\n\nimp$meth\n\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \n\n\nThe variable age is complete and therefore not imputed, denoted by the \"\" empty string. The other variables have method pmm, which stands for predictive mean matching, the default in mice for numerical and integer data. In reality, the data are better described a as mix of numerical and categorical data. Let us take a look at the nhanes2 data frame\n\nsummary(nhanes2)\n\n    age          bmi          hyp          chl       \n 20-39:12   Min.   :20.40   no  :13   Min.   :113.0  \n 40-59: 7   1st Qu.:22.65   yes : 4   1st Qu.:185.0  \n 60-99: 6   Median :26.75   NA's: 8   Median :187.0  \n            Mean   :26.56             Mean   :191.4  \n            3rd Qu.:28.93             3rd Qu.:212.0  \n            Max.   :35.30             Max.   :284.0  \n            NA's   :9                 NA's   :10     \n\n\nand the structure of the data frame\n\nstr(nhanes2)\n\n'data.frame':   25 obs. of  4 variables:\n $ age: Factor w/ 3 levels \"20-39\",\"40-59\",..: 1 2 1 3 1 3 1 1 2 2 ...\n $ bmi: num  NA 22.7 NA NA 20.4 NA 22.5 30.1 22 NA ...\n $ hyp: Factor w/ 2 levels \"no\",\"yes\": NA 1 1 NA 1 NA 1 1 1 NA ...\n $ chl: num  NA 187 187 NA 113 184 118 187 238 NA ...\n\n\nVariable age consists of 3 age categories, while variable hyp is binary. The mice() function takes these properties automatically into account. Impute the nhanes2 dataset\n\nimp &lt;- mice(nhanes2, print = FALSE)\nimp$meth\n\n     age      bmi      hyp      chl \n      \"\"    \"pmm\" \"logreg\"    \"pmm\" \n\n\nNotice that mice has set the imputation method for variable hyp to logreg, which implements multiple imputation by logistic regression.\nAn up-to-date overview of the methods in mice can be found by\n\nmethods(mice)\n\n [1] mice.impute.2l.bin              mice.impute.2l.lmer            \n [3] mice.impute.2l.norm             mice.impute.2l.pan             \n [5] mice.impute.2lonly.mean         mice.impute.2lonly.norm        \n [7] mice.impute.2lonly.pmm          mice.impute.cart               \n [9] mice.impute.jomoImpute          mice.impute.lasso.logreg       \n[11] mice.impute.lasso.norm          mice.impute.lasso.select.logreg\n[13] mice.impute.lasso.select.norm   mice.impute.lda                \n[15] mice.impute.logreg              mice.impute.logreg.boot        \n[17] mice.impute.mean                mice.impute.midastouch         \n[19] mice.impute.mnar.logreg         mice.impute.mnar.norm          \n[21] mice.impute.mpmm                mice.impute.norm               \n[23] mice.impute.norm.boot           mice.impute.norm.nob           \n[25] mice.impute.norm.predict        mice.impute.panImpute          \n[27] mice.impute.passive             mice.impute.pmm                \n[29] mice.impute.polr                mice.impute.polyreg            \n[31] mice.impute.quadratic           mice.impute.rf                 \n[33] mice.impute.ri                  mice.impute.sample             \n[35] mice.mids                       mice.theme                     \nsee '?methods' for accessing help and source code\n\n\nLet us change the imputation method for bmi to Bayesian normal linear regression imputation\n\nini &lt;- mice(nhanes2, maxit = 0)\nmeth &lt;- ini$meth\nmeth\n\n     age      bmi      hyp      chl \n      \"\"    \"pmm\" \"logreg\"    \"pmm\" \n\nmeth[\"bmi\"] &lt;- \"norm\"\nmeth\n\n     age      bmi      hyp      chl \n      \"\"   \"norm\" \"logreg\"    \"pmm\" \n\n\nand run the imputations again.\n\nimp &lt;- mice(nhanes2, meth = meth, print = FALSE)\n\nWe may now again plot trace lines to study convergence\n\nplot_trace(imp)\n\n\n\n\n\n\n\n\n\n5. Extend the number of iterations\nThough using just five iterations (the default) often works well in practice, we need to extend the number of iterations of the mice algorithm to confirm that there is no trend and that the trace lines intermingle well. We can increase the number of iterations to 40 by running 35 additional iterations using the mice.mids() function.\n\nimp40 &lt;- mice.mids(imp, maxit = 35, print = FALSE)\nplot_trace(imp40)\n\n\n\n\n\n\n\n\n\n6. Further diagnostic checking. Use function ggmice().\nGenerally, one would prefer for the imputed data to be plausible values, i.e. values that could have been observed if they had not been missing. In order to form an idea about plausibility, one may check the imputations and compare them against the observed values. If we are willing to assume that the data are missing completely at random (MCAR), then the imputations should have the same distribution as the observed data. In general, distributions may be different because the missing data are MAR (or even MNAR). However, very large discrepancies need to be screened. Let us plot the observed and imputed data of chl by\n\nggmice(imp, aes(.imp, chl)) +\n  geom_jitter(width = 0.1, height = 0)\n\n\n\n\n\n\n\n\nThe convention is to plot observed data in blue and the imputed data in red. The figure graphs the observed data values of chl and the imputed values. Since the PMM method draws imputations from the observed data, imputed values have the same gaps as in the observed data, and are always within the range of the observed data. The figure indicates that the distributions of the imputed and the observed values are similar. The observed data have a particular feature that, for some reason, the data cluster around the value of 187. The imputations reflect this feature, and are close to the data. Under MCAR, univariate distributions of the observed and imputed data are expected to be identical. Under MAR, they can be different, both in location and spread, but their multivariate distribution is assumed to be identical. There are many other ways to look at the imputed data.\nThe following command creates the graph from the previous step for each of the variables in the dataset.\n\npurrr::map(names(imp$data), ~ {\n  ggmice(imp, aes(x = .imp, y = .data[[.x]])) +\n    geom_jitter(width = 0.1, height = 0) +\n    labs(x = \"Imputation number\")\n})\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\nRemember that bmi was imputed by Bayesian linear regression and (the range of) imputed values may therefore be different than observed values.\n\n\n2.0.1 Repeated analysis in mice\n\n7. Perform the following regression analysis on the multiply imputed data. Store the solution in object fit.\n[ = _0 + _1 + ]\n\nfit &lt;- with(imp, lm(bmi ~ chl))\nfit\n\ncall :\nwith.mids(data = imp, expr = lm(bmi ~ chl))\n\ncall1 :\nmice(data = nhanes2, method = meth, printFlag = FALSE)\n\nnmis :\nage bmi hyp chl \n  0   9   8  10 \n\nanalyses :\n[[1]]\n\nCall:\nlm(formula = bmi ~ chl)\n\nCoefficients:\n(Intercept)          chl  \n   18.79775      0.03798  \n\n\n[[2]]\n\nCall:\nlm(formula = bmi ~ chl)\n\nCoefficients:\n(Intercept)          chl  \n   21.27123      0.02701  \n\n\n[[3]]\n\nCall:\nlm(formula = bmi ~ chl)\n\nCoefficients:\n(Intercept)          chl  \n    22.1682       0.0246  \n\n\n[[4]]\n\nCall:\nlm(formula = bmi ~ chl)\n\nCoefficients:\n(Intercept)          chl  \n   22.52437      0.02371  \n\n\n[[5]]\n\nCall:\nlm(formula = bmi ~ chl)\n\nCoefficients:\n(Intercept)          chl  \n   12.73382      0.06572  \n\n\nThe fit object contains the regression summaries for each data set. The new object fit is actually of class mira (multiply imputed repeated analyses).\n\nclass(fit)\n\n[1] \"mira\"   \"matrix\"\n\n\nUse the ls() function to what out what is in the object.\n\nls(fit)\n\n[1] \"analyses\" \"call\"     \"call1\"    \"nmis\"    \n\n\nSuppose we want to find the regression model fitted to the second imputed data set. It can be found as\n\nsummary(fit$analyses[[2]])\n\n\nCall:\nlm(formula = bmi ~ chl)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.258 -1.959 -1.291  2.690  8.140 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 21.27123    4.04316   5.261 2.45e-05 ***\nchl          0.02701    0.02105   1.283    0.212    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.23 on 23 degrees of freedom\nMultiple R-squared:  0.0668,    Adjusted R-squared:  0.02623 \nF-statistic: 1.646 on 1 and 23 DF,  p-value: 0.2122\n\n\n\n8. Pool the analyses from object fit.\nPooling the repeated regression analyses can be done simply by typing\n\npool.fit &lt;- pool(fit)\nsummary(pool.fit)\n\n         term    estimate  std.error statistic       df   p.value\n1 (Intercept) 19.49906484 6.30846802  3.090935 6.469642 0.0193880\n2         chl  0.03580471 0.03001395  1.192936 8.072772 0.2667745\n\n\nwhich gives the relevant pooled regression coefficients and parameters, as well as the fraction of information about the coefficients missing due to nonresponse (fmi) and the proportion of the variation attributable to the missing data (lambda). The pooled fit object is of class mipo, which stands for multiply imputed pooled object.\nmice is able to pool many analyses from a variety of packages for you, as long as the functions adhere to the coef method convention in R. For flexibility and in order to run custom pooling functions, mice also incorporates a function pool.scalar() which pools univariate estimates of \\(m\\) repeated complete data analysis conform Rubin’s pooling rules (Rubin 2004, vol. 81, para. 3.1)\n\n- End of Vignette\n\n\n\n\n\nRubin, Donald B. 2004. Multiple Imputation for Nonresponse in Surveys. Vol. 81. John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`mice`: Algorithmic convergence and inference pooling</span>"
    ]
  },
  {
    "objectID": "3Inspect/The_imputation_and_nonresponse_models.html",
    "href": "3Inspect/The_imputation_and_nonresponse_models.html",
    "title": "3  mice: The imputation and nonresponse models",
    "section": "",
    "text": "This is the third vignette in a series of ten.\nIn this vignette we will focus on analyzing the relation between the data and the missingness. For non-R users: In R one can simply call the help function for any specific function func by typing help(func). E.g. help(mice) directs you to the help page of the mice function.\n\n1. Open R and load the packages mice, ggmice and ggplot2. Also, fix the random seed.\n\nlibrary(mice)\nlibrary(ggmice)\nlibrary(ggplot2)\nset.seed(123)\n\nWe choose seed value 123. This is an arbitrary value; any value would be an equally good seed value. Fixing the random seed enables you (and others) to exactly replicate anything that involves random number generators. If you set the seed in your R instance to 123, you will get the exact same results and plots as we present in this document.\n\n2. The boys dataset is part of mice. It is a subset of a large Dutch dataset containing growth measures from the Fourth Dutch Growth Study. Inspect the help for boys dataset and make yourself familiar with its contents.\nTo learn more about the contents of the data, use one of the two following help commands:\n\nhelp(boys)\n?boys\n\n\n3. Get an overview of the data. Find information about the size of the data, the variables measured and the amount of missingness.\n\nhead(boys)\n\n     age  hgt   wgt   bmi   hc  gen  phb tv   reg\n3  0.035 50.1 3.650 14.54 33.7 &lt;NA&gt; &lt;NA&gt; NA south\n4  0.038 53.5 3.370 11.77 35.0 &lt;NA&gt; &lt;NA&gt; NA south\n18 0.057 50.0 3.140 12.56 35.2 &lt;NA&gt; &lt;NA&gt; NA south\n23 0.060 54.5 4.270 14.37 36.7 &lt;NA&gt; &lt;NA&gt; NA south\n28 0.062 57.5 5.030 15.21 37.3 &lt;NA&gt; &lt;NA&gt; NA south\n36 0.068 55.5 4.655 15.11 37.0 &lt;NA&gt; &lt;NA&gt; NA south\n\nnrow(boys)\n\n[1] 748\n\nsummary(boys)\n\n      age              hgt              wgt              bmi       \n Min.   : 0.035   Min.   : 50.00   Min.   :  3.14   Min.   :11.77  \n 1st Qu.: 1.581   1st Qu.: 84.88   1st Qu.: 11.70   1st Qu.:15.90  \n Median :10.505   Median :147.30   Median : 34.65   Median :17.45  \n Mean   : 9.159   Mean   :132.15   Mean   : 37.15   Mean   :18.07  \n 3rd Qu.:15.267   3rd Qu.:175.22   3rd Qu.: 59.58   3rd Qu.:19.53  \n Max.   :21.177   Max.   :198.00   Max.   :117.40   Max.   :31.74  \n                  NA's   :20       NA's   :4        NA's   :21     \n       hc          gen        phb            tv           reg     \n Min.   :33.70   G1  : 56   P1  : 63   Min.   : 1.00   north: 81  \n 1st Qu.:48.12   G2  : 50   P2  : 40   1st Qu.: 4.00   east :161  \n Median :53.00   G3  : 22   P3  : 19   Median :12.00   west :239  \n Mean   :51.51   G4  : 42   P4  : 32   Mean   :11.89   south:191  \n 3rd Qu.:56.00   G5  : 75   P5  : 50   3rd Qu.:20.00   city : 73  \n Max.   :65.00   NA's:503   P6  : 41   Max.   :25.00   NA's :  3  \n NA's   :46                 NA's:503   NA's   :522                \n\n\n\n4. As we have seen before, the function plot_pattern() can be used to display all different missing data patterns. How many different missing data patterns are present in the boys dataframe and which pattern occurs most frequently in the data?\n\nplot_pattern(boys)\n\n\n\n\n\n\n\n\nThere are 13 patterns in total, with the pattern where gen, phb and tv are missing occurring the most.\n\n5. How many patterns occur for which the variable gen (genital Tannerstage) is missing?\n\nmpat &lt;- plot_pattern(boys)\nsum(mpat$data$x == \"gen\" & mpat$data$.where == \"missing\")\n\n[1] 8\n\n\nAnswer: 8 patterns (503 cases)\n\n6. Let us focus more precisely on the missing data patterns. Does the missing data of gen depend on age? One could for example check this by making a histogram of age separately for the cases with known genital stages and for cases with missing genital stages.\nTo create said histogram in R, a missingness indicator for gen has to be created. A missingness indicator is a dummy variable with value 1 for observed values (in this case genital status) and 0 for missing values. Create a missingness indicator for gen by typing\n\nR &lt;- is.na(boys$gen) \nR\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[313]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[325] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n[337] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[349] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n[361]  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE\n[373] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n[385] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[397] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n[409] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[421] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n[433] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n[445] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[457]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n[469] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n[481] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[493] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[505] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n[517]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n[529] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE\n[541]  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[553] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\n[565]  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n[577] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n[589]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n[601] FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n[613] FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[625]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n[637]  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE\n[649]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n[661]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\n[673] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n[685]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n[697]  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n[709] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n[721] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[733] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[745]  TRUE  TRUE  TRUE  TRUE\n\n\nAs we can see, the missingness indicator tells us for each value in gen whether it is missing (TRUE) or observed (FALSE).\nA histogram can be made with the function ggmice.\n\nggmice(boys, aes(age)) + \n  geom_histogram(fill = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe code for a conditional histogram of age given R is\n\nggmice(boys, aes(age)) + \n  geom_histogram(fill = \"white\") +\n  facet_wrap(~is.na(gen))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAnd the same graph with more informative labels is\n\nggmice(boys, aes(age)) + \n  geom_histogram(fill = \"white\") +\n  facet_wrap(~ factor(\n    is.na(gen), \n    levels = c(FALSE, TRUE), \n    labels = c(\"observed gen\", \"missing gen\")))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe histogram shows that the missingness in gen is not equally distributed across age.\n\n7. Impute the boys dataset with mice using all default settings and name the mids (multiply imputed data set) object imp1.\n\nimp1 &lt;- mice(boys, print = FALSE)\n\n\n8. Compare the means of the imputed data with the means of the incomplete data. One can use the function complete() with a mids-object as argument to obtain an imputed dataset. As default, the first imputed dataset will be given by this function.\n\nsummary(boys)\n\n      age              hgt              wgt              bmi       \n Min.   : 0.035   Min.   : 50.00   Min.   :  3.14   Min.   :11.77  \n 1st Qu.: 1.581   1st Qu.: 84.88   1st Qu.: 11.70   1st Qu.:15.90  \n Median :10.505   Median :147.30   Median : 34.65   Median :17.45  \n Mean   : 9.159   Mean   :132.15   Mean   : 37.15   Mean   :18.07  \n 3rd Qu.:15.267   3rd Qu.:175.22   3rd Qu.: 59.58   3rd Qu.:19.53  \n Max.   :21.177   Max.   :198.00   Max.   :117.40   Max.   :31.74  \n                  NA's   :20       NA's   :4        NA's   :21     \n       hc          gen        phb            tv           reg     \n Min.   :33.70   G1  : 56   P1  : 63   Min.   : 1.00   north: 81  \n 1st Qu.:48.12   G2  : 50   P2  : 40   1st Qu.: 4.00   east :161  \n Median :53.00   G3  : 22   P3  : 19   Median :12.00   west :239  \n Mean   :51.51   G4  : 42   P4  : 32   Mean   :11.89   south:191  \n 3rd Qu.:56.00   G5  : 75   P5  : 50   3rd Qu.:20.00   city : 73  \n Max.   :65.00   NA's:503   P6  : 41   Max.   :25.00   NA's :  3  \n NA's   :46                 NA's:503   NA's   :522                \n\nsummary(complete(imp1))\n\n      age              hgt              wgt              bmi       \n Min.   : 0.035   Min.   : 50.00   Min.   :  3.14   Min.   :11.77  \n 1st Qu.: 1.581   1st Qu.: 83.53   1st Qu.: 11.85   1st Qu.:15.89  \n Median :10.505   Median :145.75   Median : 34.55   Median :17.39  \n Mean   : 9.159   Mean   :131.12   Mean   : 37.11   Mean   :18.03  \n 3rd Qu.:15.267   3rd Qu.:175.00   3rd Qu.: 59.35   3rd Qu.:19.48  \n Max.   :21.177   Max.   :198.00   Max.   :117.40   Max.   :31.74  \n       hc        gen      phb            tv            reg     \n Min.   :33.70   G1:305   P1:325   Min.   : 1.000   north: 82  \n 1st Qu.:48.45   G2:149   P2:112   1st Qu.: 2.000   east :161  \n Median :53.20   G3: 55   P3: 45   Median : 3.000   west :240  \n Mean   :51.64   G4: 83   P4: 69   Mean   : 8.239   south:192  \n 3rd Qu.:56.00   G5:156   P5:112   3rd Qu.:15.000   city : 73  \n Max.   :65.00            P6: 85   Max.   :25.000              \n\n\nMost means are roughly equal, except the mean of tv, which is much lower in the first imputed data set, when compared to the incomplete data. This makes sense because most genital measures are unobserved for the lower ages. When imputing these values, the means should decrease.\nInvestigating univariate properties by using functions such as summary(), may not be ideal in the case of hundreds of variables. To extract just the information you need, for all imputed datasets, we can make use of the with() function. To obtain summaries for each imputed tv only, type\n\nsummary(with(imp1, mean(tv)))\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n# A tibble: 5 × 2\n      x df.residual\n  &lt;dbl&gt;       &lt;dbl&gt;\n1  8.24         Inf\n2  8.52         Inf\n3  8.39         Inf\n4  8.52         Inf\n5  8.45         Inf\n\n\n\n\n3.0.1 The importance of the imputation model\nThe mammalsleep dataset is part of mice. It contains the Allison and Cicchetti (1976) data for mammalian species. To learn more about this data, type\n\nhelp(mammalsleep)\n\n\n9. Get an overview of the data. Find information about the size of the data, the variables measured and the amount of missingness.\n\nhead(mammalsleep)\n\n                    species       bw    brw sws  ps   ts  mls  gt pi sei odi\n1          African elephant 6654.000 5712.0  NA  NA  3.3 38.6 645  3   5   3\n2 African giant pouched rat    1.000    6.6 6.3 2.0  8.3  4.5  42  3   1   3\n3                Arctic Fox    3.385   44.5  NA  NA 12.5 14.0  60  1   1   1\n4    Arctic ground squirrel    0.920    5.7  NA  NA 16.5   NA  25  5   2   3\n5            Asian elephant 2547.000 4603.0 2.1 1.8  3.9 69.0 624  3   5   4\n6                    Baboon   10.550  179.5 9.1 0.7  9.8 27.0 180  4   4   4\n\nsummary(mammalsleep)\n\n                      species         bw                brw         \n African elephant         : 1   Min.   :   0.005   Min.   :   0.14  \n African giant pouched rat: 1   1st Qu.:   0.600   1st Qu.:   4.25  \n Arctic Fox               : 1   Median :   3.342   Median :  17.25  \n Arctic ground squirrel   : 1   Mean   : 198.790   Mean   : 283.13  \n Asian elephant           : 1   3rd Qu.:  48.202   3rd Qu.: 166.00  \n Baboon                   : 1   Max.   :6654.000   Max.   :5712.00  \n (Other)                  :56                                       \n      sws               ps              ts             mls         \n Min.   : 2.100   Min.   :0.000   Min.   : 2.60   Min.   :  2.000  \n 1st Qu.: 6.250   1st Qu.:0.900   1st Qu.: 8.05   1st Qu.:  6.625  \n Median : 8.350   Median :1.800   Median :10.45   Median : 15.100  \n Mean   : 8.673   Mean   :1.972   Mean   :10.53   Mean   : 19.878  \n 3rd Qu.:11.000   3rd Qu.:2.550   3rd Qu.:13.20   3rd Qu.: 27.750  \n Max.   :17.900   Max.   :6.600   Max.   :19.90   Max.   :100.000  \n NA's   :14       NA's   :12      NA's   :4       NA's   :4        \n       gt               pi             sei             odi       \n Min.   : 12.00   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.: 35.75   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000  \n Median : 79.00   Median :3.000   Median :2.000   Median :2.000  \n Mean   :142.35   Mean   :2.871   Mean   :2.419   Mean   :2.613  \n 3rd Qu.:207.50   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :645.00   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n NA's   :4                                                       \n\nstr(mammalsleep)\n\n'data.frame':   62 obs. of  11 variables:\n $ species: Factor w/ 62 levels \"African elephant\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ bw     : num  6654 1 3.38 0.92 2547 ...\n $ brw    : num  5712 6.6 44.5 5.7 4603 ...\n $ sws    : num  NA 6.3 NA NA 2.1 9.1 15.8 5.2 10.9 8.3 ...\n $ ps     : num  NA 2 NA NA 1.8 0.7 3.9 1 3.6 1.4 ...\n $ ts     : num  3.3 8.3 12.5 16.5 3.9 9.8 19.7 6.2 14.5 9.7 ...\n $ mls    : num  38.6 4.5 14 NA 69 27 19 30.4 28 50 ...\n $ gt     : num  645 42 60 25 624 180 35 392 63 230 ...\n $ pi     : int  3 3 1 5 3 4 1 4 1 1 ...\n $ sei    : int  5 1 1 2 5 4 1 5 2 1 ...\n $ odi    : int  3 3 1 3 4 4 1 4 1 1 ...\n\n\nAs we have seen before, the mice function md.pattern() (and the ggmice function plot_pattern()) can be used to display all different missing data patterns. How many different missing data patterns are present in the mammalsleep dataframe and which pattern occurs most frequently in the data?\n\nplot_pattern(mammalsleep)\n\n\n\n\n\n\n\n\nAnswer: 8 patterns in total, with the pattern where everything is observed occurring the most (42 times).\n\n10. Generate five imputed datasets with the default method pmm. Give the algorithm 10 iterations.\n\nimp &lt;- mice(mammalsleep, maxit = 10, print = FALSE)\n\nWarning: Number of logged events: 525\n\n\nInspect the trace lines\n\nplot_trace(imp)\n\n\n\n\n\n\n\n\n\n11. Perform a regression analysis on the imputed dataset with sws as dependent variable and log(bw) and odi as independent variables.\n\nfit1 &lt;- with(imp, lm(sws ~ log10(bw) + odi), print = FALSE)\n\n\n12. Pool the regression analysis and inspect the pooled analysis.\n\npool(fit1)\n\nClass: mipo    m = 5 \n         term m   estimate       ubar           b          t dfcom       df\n1 (Intercept) 5  9.8541774 0.67165805 0.350563293 1.09233400    59 15.25266\n2   log10(bw) 5 -1.6037986 0.09481760 0.071400400 0.18049807    59 11.15164\n3         odi 5 -0.5128291 0.08397637 0.003868949 0.08861911    59 52.16868\n         riv     lambda        fmi\n1 0.62632459 0.38511659 0.45249127\n2 0.90363481 0.47468916 0.54892946\n3 0.05528626 0.05238982 0.08674301\n\nsummary(pool(fit1))\n\n         term   estimate std.error statistic       df      p.value\n1 (Intercept)  9.8541774 1.0451478  9.428501 15.25266 9.345076e-08\n2   log10(bw) -1.6037986 0.4248506 -3.774970 11.15164 3.001831e-03\n3         odi -0.5128291 0.2976896 -1.722697 52.16868 9.086711e-02\n\n\nThe fmi and lambda are much too high. This is due to species being included in the imputation model. Because there are 62 species and mice automatically converts factors (categorical variables) to dummy variables, each species is modeled by its own imputation model.\n\n13. Impute mammalsleep again, but now exclude species from the data. Name the new imputed dataset impnew.\n\nimpnew &lt;- mice(mammalsleep[ , -1], maxit = 10, print = FALSE)\n\nWarning: Number of logged events: 25\n\n\n\n14. Compute and pool the regression analysis again.\n\nfit2 &lt;- with(impnew, lm(sws ~ log10(bw) + odi))\npool(fit2)\n\nClass: mipo    m = 5 \n         term m   estimate       ubar           b          t dfcom       df\n1 (Intercept) 5 11.4527165 0.61190409 0.139559346 0.77937530    59 29.54143\n2   log10(bw) 5 -1.1746524 0.08638216 0.001638406 0.08834825    59 55.44294\n3         odi 5 -0.8519964 0.07650542 0.024393675 0.10577783    59 23.06225\n         riv     lambda        fmi\n1 0.27368867 0.21487878 0.26313242\n2 0.02276034 0.02225384 0.05571369\n3 0.38261877 0.27673483 0.33223772\n\nsummary(pool(fit2))\n\n         term   estimate std.error statistic       df      p.value\n1 (Intercept) 11.4527165 0.8828223 12.972844 29.54143 9.931861e-14\n2   log10(bw) -1.1746524 0.2972343 -3.951940 55.44294 2.215440e-04\n3         odi -0.8519964 0.3252350 -2.619633 23.06225 1.530114e-02\n\n\nNote that the fmi and lambda have dramatically decreased. The imputation model has been greatly improved.\n\n15. Plot the trace lines for impnew\n\nplot_trace(impnew)\n\n\n\n\n\n\n\n\nEven though the fraction of information missing due to nonresponse (fmi) and the relative increase in variance due to nonresponse (lambda) are nice and low, the convergence turns out to be a real problem. The reason is the structure in the data. Total sleep (ts) is the sum of paradoxical sleep (ps) and short wave sleep (sws). This relation is ignored in the imputations, but it is necessary to take this relation into account. mice offers a routine called passive imputation, which allows users to take transformations, combinations and recoded variables into account when imputing their data.\nWe explain passive imputation in detail in Chapter 4.\n\n\n3.0.2 Conclusion\nWe have seen that the practical execution of multiple imputation and pooling is straightforward with the R package mice. The package is designed to allow you to assess and control the imputations themselves, the convergence of the algorithm and the distributions and multivariate relations of the observed and imputed data.\nIt is important to ‘gain’ this control as a user. After all, we are imputing values and we aim to properly address the uncertainty about the missingness problem.\n\n- End of Vignette",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>`mice`: The imputation and nonresponse models</span>"
    ]
  },
  {
    "objectID": "4PassivePostProcess/Passive_imputation_and_Post-processing.html",
    "href": "4PassivePostProcess/Passive_imputation_and_Post-processing.html",
    "title": "4  mice: Passive imputation and Post-processing",
    "section": "",
    "text": "This is the fourth vignette in a series of ten.\nIn this vignette we will walk you through the more advanced features of mice, such as post-processing of imputations and passive imputation.\n\n1. Open R and load the packages mice, ggmice and ggplot2.\n\nlibrary(mice)\nlibrary(ggmice)\nlibrary(ggplot2)\nset.seed(123)\n\nWe choose seed value 123. This is an arbitrary value; any value would be an equally good seed value. Fixing the random seed enables you (and others) to exactly replicate anything that involves random number generators. If you set the seed in your R instance to 123, you will get the exact same results and plots as we present in this document.\n\nPassive Imputation\nThere is often a need for transformed, combined or recoded versions of the data. In the case of incomplete data, one could impute the original, and transform the completed original afterwards, or transform the incomplete original and impute the transformed version. If, however, both the original and the transformed version are needed within the imputation algorithm, neither of these approaches work: One cannot be sure that the transformation holds between the imputed values of the original and transformed versions. mice has a built-in approach, called passive imputation, to deal with situations as described above. The goal of passive imputation is to maintain the consistency among different transformations of the same data. As an example, consider the following deterministic function in the boys data [ = ] or the compositional relation in the mammalsleep data: [ = +]\n\n2. Use passive imputation to impute the deterministic sleep relation in the mammalsleep data. Name the new multiply imputed dataset pas.imp.\n\nini &lt;- mice(mammalsleep[, -1], maxit=0, print=F)\nmeth&lt;- ini$meth\nmeth\n\n   bw   brw   sws    ps    ts   mls    gt    pi   sei   odi \n   \"\"    \"\" \"pmm\" \"pmm\" \"pmm\" \"pmm\" \"pmm\"    \"\"    \"\"    \"\" \n\npred &lt;- ini$pred\npred\n\n    bw brw sws ps ts mls gt pi sei odi\nbw   0   1   1  1  1   1  1  1   1   1\nbrw  1   0   1  1  1   1  1  1   1   1\nsws  1   1   0  1  1   1  1  1   1   1\nps   1   1   1  0  1   1  1  1   1   1\nts   1   1   1  1  0   1  1  1   1   1\nmls  1   1   1  1  1   0  1  1   1   1\ngt   1   1   1  1  1   1  0  1   1   1\npi   1   1   1  1  1   1  1  0   1   1\nsei  1   1   1  1  1   1  1  1   0   1\nodi  1   1   1  1  1   1  1  1   1   0\n\npred[c(\"sws\", \"ps\"), \"ts\"] &lt;- 0\npred\n\n    bw brw sws ps ts mls gt pi sei odi\nbw   0   1   1  1  1   1  1  1   1   1\nbrw  1   0   1  1  1   1  1  1   1   1\nsws  1   1   0  1  0   1  1  1   1   1\nps   1   1   1  0  0   1  1  1   1   1\nts   1   1   1  1  0   1  1  1   1   1\nmls  1   1   1  1  1   0  1  1   1   1\ngt   1   1   1  1  1   1  0  1   1   1\npi   1   1   1  1  1   1  1  0   1   1\nsei  1   1   1  1  1   1  1  1   0   1\nodi  1   1   1  1  1   1  1  1   1   0\n\nmeth[\"ts\"]&lt;- \"~ I(sws + ps)\"\npas.imp &lt;- mice(mammalsleep[, -1], meth=meth, pred=pred, maxit=10, seed=123, print=F)\n\nWe used a custom predictor matrix and method vector to tailor our imputation approach to the passive imputation problem. We made sure to exclude ts as a predictor for the imputation of sws and ps to avoid circularity.\nWe also gave the imputation algorithm 10 iterations to converge and fixed the seed to 123 for this mice instance. This means that even when people do not fix the overall R seed for a session, exact replication of results can be obtained by simply fixing the seed for the random number generator within mice. Naturally, the same input (data) is each time required to yield the same output (mids-object).\n\n3. Inspect the trace lines for pas.imp.\n\nplot_trace(pas.imp)\n\n\n\n\n\n\n\n\nWe can see that the pathological nonconvergence we experienced before has been properly dealt with. The trace lines for the sleep variable look okay now and convergence can be inferred by studying the trace lines.\n\nPost-processing of the imputations\nRemember that we imputed the boys data in the previous tutorial with pmm and with norm. One of the problems with the imputed values of tv with norm is that there are negative values among the imputations. Somehow we should be able to lay a constraint on the imputed values of tv.\nThe mice() function has an argument called post that takes a vector of strings of R commands. These commands are parsed and evaluated after the univariate imputation function returns, and thus provides a way of post-processing the imputed values while using the processed version in the imputation algorithm. In other words; the post-processing allows us to manipulate the imputations for a particular variable that are generated within each iteration. Such manipulations directly affect the imputed values of that variable and the imputations for other variables. Naturally, such a procedure should be handled with care.\n\n4. Post-process the values to constrain them between 1 and 25, use norm as the imputation method for tv.\n\nini &lt;- mice(boys, maxit = 0)\nmeth &lt;- ini$meth\nmeth[\"tv\"] &lt;- \"norm\"\npost &lt;- ini$post\npost[\"tv\"] &lt;- \"imp[[j]][, i] &lt;- squeeze(imp[[j]][, i], c(1, 25))\"\nimp &lt;- mice(boys, meth=meth, post=post, print=FALSE)\n\nIn this way the imputed values of tv are constrained (squeezed by function squeeze()) between 1 and 25.\n\n5. Compare the imputed values and histograms of tv for the solution obtained by pmm to the constrained solution (created with norm, constrained between 1 and 25).\nFirst, we recreate the default pmm solution\n\nimp.pmm &lt;- mice(boys, print=FALSE)\n\nand we inspect the imputed values for the pmm solution\n\ntable(complete(imp.pmm)$tv)\n\n\n  1   2   3   4   5   6   8   9  10  12  13  14  15  16  17  18  20  25 \n 64 220  92  31   8  22  31   1  32  33   2   1  64   2   3   5  82  55 \n\n\nNow, we inspect the imputed values for the norm solution\n\ntable(complete(imp)$tv)\n\n\n               1 1.04765626076076 1.06850744132516 1.10592998910862 \n             295                1                1                1 \n1.20730719285169 1.24187503731931 1.25217703521875  1.3431238400032 \n               1                1                1                1 \n1.43340328493328 1.56390724622927 1.92459899905457                2 \n               1                1                1               26 \n2.22362050518801 2.25183115782234 2.34547451259226 2.35357321335211 \n               1                1                1                1 \n2.58040160876244 2.69673415107593 2.78052682429925 2.96407460219848 \n               1                1                1                1 \n               3 3.00650554011491 3.17764057130238 3.18083020006692 \n              19                1                1                1 \n3.19389551375974 3.21667310935236 3.28626617581969 3.51924658014597 \n               1                1                1                1 \n3.53901770668924 3.61977669204177  3.8797990088058 3.97926934844286 \n               1                1                1                1 \n 3.9900157454526                4 4.06042095256628 4.22694246630602 \n               1               17                1                1 \n4.29130000928007 4.30862947298446 4.37387945763616 4.37537718505784 \n               1                1                1                1 \n4.55235899105437 4.81019273467669 4.83908926626029                5 \n               1                1                1                5 \n5.04777694212134 5.14146238911168 5.19146144165499 5.33712260877789 \n               1                1                1                1 \n5.34978402872541 5.39306678805702 5.41771633425926 5.55496083813505 \n               1                1                1                1 \n5.56166676641967 5.62471111732166 5.76826187764073 5.90344832732374 \n               1                1                1                1 \n 5.9125972433633                6 6.06171454825624 6.17188152401789 \n               1               10                1                1 \n6.50027689106417  6.6121473242796 6.62366972680022 6.71009258164643 \n               1                1                1                1 \n6.78660561533969 6.82023291191748 6.93969443361718 7.20670254516833 \n               1                1                1                1 \n7.31833998419389 7.44337884207534 7.51257425309894  7.5235240441055 \n               1                1                1                1 \n7.52360629488164 7.66455874369213                8 8.04006995448565 \n               1                1               13                1 \n8.16570903547774 8.35250937024199  8.4579188283566 8.90575593715416 \n               1                1                1                1 \n               9 9.10934592101191 9.20885854479667 9.26156587553364 \n               1                1                1                1 \n9.29355874917571  9.5812867087336 9.78905873169104 9.83456792046614 \n               1                1                1                1 \n9.86765016234437 9.93634100590388 9.94638005921571               10 \n               1                1                1               16 \n10.1722306905023 10.3246561167553 10.4090794123788 10.4928534007062 \n               1                1                1                1 \n10.5675202054327 10.5769768123748 10.7511961751585 10.7690320389986 \n               1                1                1                1 \n10.8669515807312 10.9113698228937 10.9335673899359 11.2550096934402 \n               1                1                1                1 \n11.3928306269214 11.3938064531829 11.4196038098713 11.4318468440325 \n               1                1                1                1 \n11.6787996459044 11.7908946367069  11.794811127912               12 \n               1                1                1               15 \n12.0805561600205  12.092530545071  12.141016124024 12.2269177760078 \n               1                1                1                1 \n 12.250545177925 12.3073763054301 12.4148260278839 12.4269150733802 \n               1                1                1                1 \n12.5443605993662 12.9527866090667               13 13.0298612954944 \n               1                1                1                1 \n13.2321196893653 13.2653923692569 13.3504640851202 13.5026934067198 \n               1                1                1                1 \n13.5239821605957 13.6158749951967 13.8398296856953 13.8953836190308 \n               1                1                1                1 \n              14  14.019257607944 14.0829876461656 14.1143959504111 \n               1                1                1                1 \n14.2063759884187 14.3394758779936 14.4471926764635 14.6927756681367 \n               1                1                1                1 \n14.8928157890629 14.8962036483512 14.9501291162801  14.972908230154 \n               1                1                1                1 \n              15 15.0789547864371 15.2718126964326 15.5526903837337 \n              27                1                1                1 \n15.6012690386797 15.6897198216087 15.7529932431904 15.7715817799218 \n               1                1                1                1 \n15.8093564899728 15.9226633031357  15.969308978326 15.9978651210228 \n               1                1                1                1 \n              16 16.2153210139347 16.2584574800652 16.4262291321834 \n               1                1                1                1 \n16.7544670049143 16.7700444611989 16.7739176686877 16.8322671901901 \n               1                1                1                1 \n16.9452728183799               17 17.0444227170178 17.1176586367753 \n               1                1                1                1 \n17.1736002358148 17.3003659157477 17.3026817301974 17.6789492601488 \n               1                1                1                1 \n17.7608297532485 17.7675556999934 17.9718184295639               18 \n               1                1                1                1 \n18.0704421648198 18.1363269757502 18.1696855616768 18.2589708709957 \n               1                1                1                1 \n18.3299823125621 18.5106883762578 18.5943047346889 18.8546565511861 \n               1                1                1                1 \n18.9400391210311 18.9932502846499 18.9951466483198 19.0702677065951 \n               1                1                1                1 \n 19.201455766491 19.3581609824787 19.3581889266686  19.425031885728 \n               1                1                1                1 \n 19.484897504882 19.5137835792275 19.6610436670894 19.7798256236903 \n               1                1                1                1 \n19.9113559715646               20 20.0104643213723 20.0369151770013 \n               1               38                1                1 \n20.0612713387309 20.1705319178841 20.1769950583502 20.2637722890758 \n               1                1                1                1 \n  20.28299932319 20.3582808714222  20.370601227242 20.3716831859469 \n               1                1                1                1 \n20.3952051441773  20.475284456376  20.500054938507 20.5353231957725 \n               1                1                1                1 \n20.6117389835994 20.8262316742591 20.8804426140199 20.9998353716783 \n               1                1                1                1 \n21.0738901027307 21.2643563150311 21.4376680828186 21.4847217765191 \n               1                1                1                1 \n21.5019274012566 21.6098089985845 21.7180256142761 21.7375984101356 \n               1                1                1                1 \n21.7700288479096 21.8196575131195 21.9628365645312 21.9929117616866 \n               1                1                1                1 \n22.1088882855881 22.1570207400041 22.3317862170905 22.3424357181246 \n               1                1                1                1 \n22.4841886661655 22.7138476548499 23.0411535782128 23.1230004599173 \n               1                1                1                1 \n23.2198832589767 23.6137661270982 24.2170617211054 24.4061064356474 \n               1                1                1                1 \n24.5359791919579 24.8057661659448               25 \n               1                1               36 \n\n\nIt is clear that the norm solution does not give us integer data as imputations. Next, we inspect and compare the density of the incomplete and imputed data for the constrained solution\n\ndensityplot(imp, ~tv)\n\nHint: Did you know, an equivalent figure can be created with `ggmice()`?\nFor example, to plot a variable named 'my_vrb' from a mids object called\n'my_mids', run:\n  ggmice(my_mids, ggplot2::aes(x = my_vrb, group = .imp)) +\n  ggplot2::geom_density()\nℹ See amices.org/ggmice for more info.\nThis message is displayed once per session.\n\n\n\n\n\n\n\n\n\nThe ggmice equivalent is\n\nggmice(imp, aes(tv, group = .imp)) +\n        geom_density()\n\n\n\n\n\n\n\n\n\nA nice way of plotting the histograms of both datasets simultaneously is by creating first the dataframe (here we named it tvm) that contains the data in one column and the imputation method in another column.\n\ntv &lt;- c(complete(imp.pmm)$tv, complete(imp)$tv)\nmethod &lt;- rep(c(\"pmm\", \"norm\"), each = nrow(boys))\ntvm &lt;- data.frame(tv = tv, method = method)\n\nand then plotting a histogram of tv conditional on method.\n\nggmice(tvm, aes(tv)) + \n        geom_histogram(fill = \"white\") + \n        facet_wrap(~method)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIs there still a difference in distribution between the two different imputation methods? Which imputations are more plausible do you think?\n\n6. Make a missing data indicator (name it miss) for bmi and check the relation of bmi, wgt and hgt for the boys in the imputed data. To do so, plot the imputed values against their respective calculated values.\n\nmiss &lt;- is.na(imp$data$bmi)\nxyplot(imp, bmi ~ I (wgt / (hgt / 100)^2),\n       na.groups = miss, cex = c(0.8, 1.2), pch = c(1, 20),\n       ylab = \"BMI (kg/m2) Imputed\", xlab = \"BMI (kg/m2) Calculated\")\n\nHint: Did you know, an equivalent figure can be created with `ggmice()`?\nFor example, to plot 2 variables named 'my_x' and 'my_y' from a mids object\ncalled 'my_mids', run:\n  ggmice(my_mids, ggplot2::aes(x = my_x, y = my_y)) +\n  ggplot2::geom_point()\nℹ See amices.org/ggmice for more info.\nThis message is displayed once per session.\n\n\n\n\n\n\n\n\n\nThe ggmice equivalent is\n\nggmice(imp, aes(I(wgt/(hgt/100)^2), bmi)) + \n        geom_point() +\n        labs(x = \"BMI (kg/m2) calculated\", y = \"BMI (kg/m2) Imputed\")\n\nWarning: Mapping variable 'I(wgt/(hgt/100)^2)' recognized internally as hgt and wgt.\nPlease verify whether this matches the requested mapping variable.\n\n\n\n\n\n\n\n\n\nWith this plot we show that the relation between hgt, wgt and bmi is not preserved in the imputed values. In order to preserve this relation, we should use passive imputation.\n\n7. Use passive imputation to conserve the relation between imputed bmi, wgt and hgt by setting the imputation method for bmi to meth[\"bmi\"]&lt;- \"~ I(wgt / (hgt / 100)^2)\".\n\nmeth&lt;- ini$meth\nmeth[\"bmi\"]&lt;- \"~ I(wgt / (hgt / 100)^2)\"\nimp &lt;- mice(boys, meth=meth, print=FALSE)\n\n\n8. Again, plot the imputed values of bmi versus the calculated values and check whether there is convergence for bmi.\nTo inspect the relation:\n\nggmice(imp, aes(I(wgt/(hgt/100)^2), bmi)) + \n        geom_point() +\n        labs(x = \"BMI (kg/m2) calculated\", y = \"BMI (kg/m2) Imputed\")\n\nWarning: Mapping variable 'I(wgt/(hgt/100)^2)' recognized internally as hgt and wgt.\nPlease verify whether this matches the requested mapping variable.\n\n\n\n\n\n\n\n\n\nTo study convergence for bmi alone:\n\nplot_trace(imp, c(\"bmi\"))\n\n\n\n\n\n\n\n\nAlthough the relation of bmi is preserved now in the imputations we get absurd imputations and on top of that we clearly see there are some problems with the convergence of bmi. The problem is that we have circularity in the imputations. We used passive imputation for bmi but bmi is also automatically used as predictor for wgt and hgt. This can be solved by adjusting the predictor matrix.\n\n9. Solve the problem of circularity (if you did not already do so) and plot once again the imputed values of bmi versus the calculated values.\nFirst, we remove bmi as a predictor for hgt and wgt to remove circularity.\n\npred &lt;- ini$pred\npred\n\n    age hgt wgt bmi hc gen phb tv reg\nage   0   1   1   1  1   1   1  1   1\nhgt   1   0   1   1  1   1   1  1   1\nwgt   1   1   0   1  1   1   1  1   1\nbmi   1   1   1   0  1   1   1  1   1\nhc    1   1   1   1  0   1   1  1   1\ngen   1   1   1   1  1   0   1  1   1\nphb   1   1   1   1  1   1   0  1   1\ntv    1   1   1   1  1   1   1  0   1\nreg   1   1   1   1  1   1   1  1   0\n\npred[c(\"hgt\", \"wgt\"), \"bmi\"] &lt;- 0\npred\n\n    age hgt wgt bmi hc gen phb tv reg\nage   0   1   1   1  1   1   1  1   1\nhgt   1   0   1   0  1   1   1  1   1\nwgt   1   1   0   0  1   1   1  1   1\nbmi   1   1   1   0  1   1   1  1   1\nhc    1   1   1   1  0   1   1  1   1\ngen   1   1   1   1  1   0   1  1   1\nphb   1   1   1   1  1   1   0  1   1\ntv    1   1   1   1  1   1   1  0   1\nreg   1   1   1   1  1   1   1  1   0\n\n\nand we run the mice algorithm again with the new predictor matrix (we still ‘borrow’ the imputation methods object meth from before)\n\nimp &lt;- mice(boys, meth = meth, pred = pred, print = FALSE)\n\nSecond, we recreate the plots from Assignment 8. We start with the plot to inspect the relations in the observed and imputed data\n\nggmice(imp, aes(I(wgt/(hgt/100)^2), bmi)) + \n        geom_point() +\n        labs(x = \"BMI (kg/m2) calculated\", y = \"BMI (kg/m2) Imputed\")\n\nWarning: Mapping variable 'I(wgt/(hgt/100)^2)' recognized internally as hgt and wgt.\nPlease verify whether this matches the requested mapping variable.\n\n\n\n\n\n\n\n\n\nand continue with the trace plot to study convergence\n\nplot_trace(imp, c(\"bmi\"))\n\n\n\n\n\n\n\n\nAll is well now!\n\nConclusion\nWe have seen that the practical execution of multiple imputation and pooling is straightforward with the R package mice. The package is designed to allow you to assess and control the imputations themselves, the convergence of the algorithm and the distributions and multivariate relations of the observed and imputed data.\nIt is important to ‘gain’ this control as a user. After all, we are imputing values and taking their uncertainty properly into account. Being also uncertain about the process that generated those values is therefor not a valid option.\n\nFor fun: what you shouldn’t do with passive imputation\nNever set all relations fixed. You will remain with the starting values and waste your computer’s energy (and your own).\n\nini &lt;- mice(boys, maxit=0)\nmeth&lt;- ini$meth\npred &lt;- ini$pred\npred\n\n    age hgt wgt bmi hc gen phb tv reg\nage   0   1   1   1  1   1   1  1   1\nhgt   1   0   1   1  1   1   1  1   1\nwgt   1   1   0   1  1   1   1  1   1\nbmi   1   1   1   0  1   1   1  1   1\nhc    1   1   1   1  0   1   1  1   1\ngen   1   1   1   1  1   0   1  1   1\nphb   1   1   1   1  1   1   0  1   1\ntv    1   1   1   1  1   1   1  0   1\nreg   1   1   1   1  1   1   1  1   0\n\nmeth[\"bmi\"]&lt;- \"~ I(wgt/(hgt/100)^2)\"\nmeth[\"wgt\"]&lt;- \"~ I(bmi*(hgt/100)^2)\"\nmeth[\"hgt\"]&lt;- \"~ I(sqrt(wgt/bmi)*100)\"\nimp.path &lt;- mice(boys, meth=meth, pred=pred, seed=123)\n\n\n iter imp variable\n  1   1  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  1   2  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  1   3  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  1   4  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  1   5  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  2   1  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  2   2  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  2   3  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  2   4  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  2   5  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  3   1  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  3   2  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  3   3  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  3   4  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  3   5  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  4   1  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  4   2  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  4   3  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  4   4  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  4   5  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  5   1  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  5   2  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  5   3  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  5   4  hgt  wgt  bmi  hc  gen  phb  tv  reg\n  5   5  hgt  wgt  bmi  hc  gen  phb  tv  reg\n\nplot_trace(imp.path, c(\"hgt\", \"wgt\", \"bmi\"))\n\n\n\n\n\n\n\n\nWe named the mids- object imp.path, because the nonconvergence is pathological in this example!\n\n- End of Vignette",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>`mice`: Passive imputation and Post-processing</span>"
    ]
  },
  {
    "objectID": "5CombInference/CombiningInferences.html",
    "href": "5CombInference/CombiningInferences.html",
    "title": "5  mice: Combining inferences",
    "section": "",
    "text": "6 Correlations\n3. Calculate a correlation between all continuous variables for the imputed boys data\nThere are two ways in which we can calculate the correlation on the imputed data:\nQuite often people are suggesting that using the average imputed dataset - so taking the average over the imputed data set such that any realized cell depicts the average over the corresponding data in the imputed data - would be efficient and conform Rubin’s rules. This is not true. Doing this will yield false inference.\nTo demonstrate this, let’s create the averaged data set and exclude the non-numerical columns:\nave &lt;- imp |&gt;\n  mice::complete(\"long\") |&gt;\n  group_by(.id) |&gt;\n  summarise_all(.funs = mean)|&gt;\n  select(-.id, -.imp, -phb, -gen, -reg)\n\nhead(ave)\n\n# A tibble: 6 × 6\n    age   hgt   wgt   bmi    hc    tv\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.035  50.1  3.65  14.5  33.7   2.5\n2 0.038  53.5  3.37  11.8  35     3.5\n3 0.057  50    3.14  12.6  35.2   2.8\n4 0.06   54.5  4.27  14.4  36.7   2.5\n5 0.062  57.5  5.03  15.2  37.3   2  \n6 0.068  55.5  4.66  15.1  37     2.1\nIf we now calculate Pearson’s correlation, rounded to two digits:\ncor.wrong &lt;- ave |&gt;\n  cor() |&gt;\n  round(digits = 2)\nwe obtain:\ncor.wrong\n\n     age  hgt  wgt  bmi   hc   tv\nage 1.00 0.98 0.95 0.63 0.86 0.86\nhgt 0.98 1.00 0.94 0.60 0.91 0.81\nwgt 0.95 0.94 1.00 0.79 0.84 0.87\nbmi 0.63 0.60 0.79 1.00 0.60 0.64\nhc  0.86 0.91 0.84 0.60 1.00 0.67\ntv  0.86 0.81 0.87 0.64 0.67 1.00\nIt is best to do a Fisher transformation before pooling the correlation estimates - and a backtransformation afterwards. Therefore we define the following two functions that allow us to transform and backtransform any value:\nfisher.trans &lt;- function(x) 1/2 * log((1 + x) / (1 - x))\nfisher.backtrans &lt;- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)\nNow, to calculate the correlation on the imputed data\ncor &lt;- imp |&gt;\n  mice::complete(\"all\")|&gt;\n  map(select, -phb, -gen, -reg) |&gt;  \n  map(stats::cor) |&gt;\n  map(fisher.trans)\ncor\n\n$`1`\n          age       hgt      wgt       bmi        hc        tv\nage       Inf 2.1971683 1.837486 0.7306936 1.2875134 1.1764013\nhgt 2.1971683       Inf 1.771810 0.6796283 1.5304395 1.0150829\nwgt 1.8374860 1.7718099      Inf 1.0661101 1.2173486 1.1632394\nbmi 0.7306936 0.6796283 1.066110       Inf 0.6755864 0.6965303\nhc  1.2875134 1.5304395 1.217349 0.6755864       Inf 0.7662706\ntv  1.1764013 1.0150829 1.163239 0.6965303 0.7662706       Inf\n\n$`2`\n          age       hgt      wgt       bmi        hc        tv\nage       Inf 2.1928290 1.839154 0.7396715 1.2805717 1.1540150\nhgt 2.1928290       Inf 1.772569 0.6866849 1.5335605 1.0247420\nwgt 1.8391536 1.7725691      Inf 1.0777861 1.2168596 1.1924681\nbmi 0.7396715 0.6866849 1.077786       Inf 0.6757144 0.7175002\nhc  1.2805717 1.5335605 1.216860 0.6757144       Inf 0.7716682\ntv  1.1540150 1.0247420 1.192468 0.7175002 0.7716682       Inf\n\n$`3`\n         age       hgt      wgt       bmi        hc        tv\nage      Inf 2.1980513 1.839317 0.7384530 1.2750962 1.1623767\nhgt 2.198051       Inf 1.772417 0.6877455 1.5248987 1.0039437\nwgt 1.839317 1.7724172      Inf 1.0774889 1.2124082 1.1611924\nbmi 0.738453 0.6877455 1.077489       Inf 0.6812475 0.7128481\nhc  1.275096 1.5248987 1.212408 0.6812475       Inf 0.7624198\ntv  1.162377 1.0039437 1.161192 0.7128481 0.7624198       Inf\n\n$`4`\n          age       hgt      wgt       bmi        hc        tv\nage       Inf 2.1961017 1.838918 0.7408736 1.2897920 1.1832366\nhgt 2.1961017       Inf 1.774843 0.6907368 1.5444453 1.0383982\nwgt 1.8389176 1.7748429      Inf 1.0800728 1.2256015 1.2125146\nbmi 0.7408736 0.6907368 1.080073       Inf 0.6827374 0.7359459\nhc  1.2897920 1.5444453 1.225601 0.6827374       Inf 0.7837830\ntv  1.1832366 1.0383982 1.212515 0.7359459 0.7837830       Inf\n\n$`5`\n          age      hgt      wgt       bmi        hc        tv\nage       Inf 2.195998 1.839240 0.7359324 1.2753551 1.1843788\nhgt 2.1959978      Inf 1.772784 0.6840920 1.5270459 1.0261064\nwgt 1.8392403 1.772784      Inf 1.0709307 1.2095539 1.1910395\nbmi 0.7359324 0.684092 1.070931       Inf 0.6742064 0.7114433\nhc  1.2753551 1.527046 1.209554 0.6742064       Inf 0.7587066\ntv  1.1843788 1.026106 1.191039 0.7114433 0.7587066       Inf\n\n$`6`\n          age       hgt      wgt       bmi        hc        tv\nage       Inf 2.1960817 1.839802 0.7380297 1.2683845 1.1098175\nhgt 2.1960817       Inf 1.773711 0.6872310 1.5185628 0.9710611\nwgt 1.8398023 1.7737114      Inf 1.0753483 1.2056839 1.1312176\nbmi 0.7380297 0.6872310 1.075348       Inf 0.6778270 0.6868663\nhc  1.2683845 1.5185628 1.205684 0.6778270       Inf 0.7000833\ntv  1.1098175 0.9710611 1.131218 0.6868663 0.7000833       Inf\n\n$`7`\n          age       hgt      wgt       bmi        hc        tv\nage       Inf 2.1962046 1.835994 0.7315759 1.2772518 1.1987818\nhgt 2.1962046       Inf 1.772959 0.6825245 1.5256606 1.0441973\nwgt 1.8359941 1.7729588      Inf 1.0685706 1.2150211 1.2020363\nbmi 0.7315759 0.6825245 1.068571       Inf 0.6805006 0.7139035\nhc  1.2772518 1.5256606 1.215021 0.6805006       Inf 0.7789061\ntv  1.1987818 1.0441973 1.202036 0.7139035 0.7789061       Inf\n\n$`8`\n          age       hgt      wgt       bmi        hc        tv\nage       Inf 2.1929580 1.840578 0.7439899 1.2888151 1.2043697\nhgt 2.1929580       Inf 1.773407 0.6923113 1.5404146 1.0468795\nwgt 1.8405779 1.7734068      Inf 1.0811815 1.2216658 1.2005007\nbmi 0.7439899 0.6923113 1.081182       Inf 0.6844591 0.7123828\nhc  1.2888151 1.5404146 1.221666 0.6844591       Inf 0.7906234\ntv  1.2043697 1.0468795 1.200501 0.7123828 0.7906234       Inf\n\n$`9`\n          age       hgt      wgt       bmi        hc        tv\nage       Inf 2.1955320 1.838406 0.7384463 1.2842531 1.1620413\nhgt 2.1955320       Inf 1.773995 0.6878719 1.5270096 1.0147341\nwgt 1.8384062 1.7739947      Inf 1.0758542 1.2183190 1.1719916\nbmi 0.7384463 0.6878719 1.075854       Inf 0.6857832 0.7065790\nhc  1.2842531 1.5270096 1.218319 0.6857832       Inf 0.7578655\ntv  1.1620413 1.0147341 1.171992 0.7065790 0.7578655       Inf\n\n$`10`\n          age      hgt      wgt       bmi        hc        tv\nage       Inf 2.194992 1.840380 0.7412752 1.2769159 1.1855404\nhgt 2.1949924      Inf 1.773964 0.6897810 1.5248482 1.0468461\nwgt 1.8403795 1.773964      Inf 1.0783437 1.2142874 1.2146691\nbmi 0.7412752 0.689781 1.078344       Inf 0.6849574 0.7124170\nhc  1.2769159 1.524848 1.214287 0.6849574       Inf 0.7733416\ntv  1.1855404 1.046846 1.214669 0.7124170 0.7733416       Inf\nThe object cor is a list over the \\(m\\) imputations where each listed index is a correlation matrix. To calculate the average over the correlation matrices, we can add the \\(m\\) listed indices and divide them by \\(m\\):\ncor.rect &lt;- Reduce(\"+\", cor) / length(cor) # m is equal to the length of the list\ncor.rect &lt;- fisher.backtrans(cor.rect)\nIf we compare the wrong estimates in cor.wrong\ncor.wrong\n\n     age  hgt  wgt  bmi   hc   tv\nage 1.00 0.98 0.95 0.63 0.86 0.86\nhgt 0.98 1.00 0.94 0.60 0.91 0.81\nwgt 0.95 0.94 1.00 0.79 0.84 0.87\nbmi 0.63 0.60 0.79 1.00 0.60 0.64\nhc  0.86 0.91 0.84 0.60 1.00 0.67\ntv  0.86 0.81 0.87 0.64 0.67 1.00\nwith the correct estimates in cor.rect\nround(cor.rect, digits = 2)\n\n     age  hgt  wgt  bmi   hc   tv\nage  NaN 0.98 0.95 0.63 0.86 0.82\nhgt 0.98  NaN 0.94 0.60 0.91 0.77\nwgt 0.95 0.94  NaN 0.79 0.84 0.83\nbmi 0.63 0.60 0.79  NaN 0.59 0.61\nhc  0.86 0.91 0.84 0.59  NaN 0.64\ntv  0.82 0.77 0.83 0.61 0.64  NaN\nWe see that the wrong estimates in cor.wrong have the tendency to overestimate the correlation coefficient that is correctly combined following Rubin’s rules.\nThe correct estimates have a diagonal of NaN’s, because the tranformation of a correlation of 1 yields Inf and the backtransformation of Inf has no representation in real number space. We know the diagonal is supposed to be 1, so we can simply correct this\ndiag(cor.rect) &lt;- 1\ncor.rect\n\n          age       hgt       wgt       bmi        hc        tv\nage 1.0000000 0.9755309 0.9506921 0.6278711 0.8565901 0.8249429\nhgt 0.9755309 1.0000000 0.9439641 0.5959615 0.9103713 0.7711664\nwgt 0.9506921 0.9439641 1.0000000 0.7914006 0.8383737 0.8287360\nbmi 0.6278711 0.5959615 0.7914006 1.0000000 0.5917157 0.6110790\nhc  0.8565901 0.9103713 0.8383737 0.5917157 1.0000000 0.6436419\ntv  0.8249429 0.7711664 0.8287360 0.6110790 0.6436419 1.0000000",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>`mice`: Combining inferences</span>"
    ]
  },
  {
    "objectID": "5CombInference/CombiningInferences.html#why-does-the-average-data-set-not-serve-as-a-good-basis-for-analysis",
    "href": "5CombInference/CombiningInferences.html#why-does-the-average-data-set-not-serve-as-a-good-basis-for-analysis",
    "title": "5  mice: Combining inferences",
    "section": "6.1 Why does the average data set not serve as a good basis for analysis?",
    "text": "6.1 Why does the average data set not serve as a good basis for analysis?\nIn FIMD v2, paragraph 5.1.2 Stef mentions the following:\n\nThe average workflow is faster and easier than the correct methods, since there is no need to replicate the analyses \\(m\\) times. In the words of Dempster and Rubin (1983), this workflow is\nseductive because it can lull the user into the pleasurable state of believing that the data are complete after all.\nThe ensuing statistical analysis does not know which data are observed and which are missing, and treats all data values as real, which will underestimate the uncertainty of the parameters. The reported standard errors and p-values after data-averaging are generally too low. The correlations between the variables of the averaged data will be too high. For example, the correlation matrix in the average data are more extreme than the average of the \\(m\\) correlation matrices, which is an example of ecological fallacy. As researchers tend to like low p-values and high correlations, there is a cynical reward for the analysis of the average data. However, analysis of the average data cannot give a fair representation of the uncertainties associated with the underlying data, and hence is not recommended.\n\n\nSo, please stay away from averaging the imputed data sets. Instead, use the correct workflow of analyzing the imputed sets separately and combining the inference afterwards.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>`mice`: Combining inferences</span>"
    ]
  },
  {
    "objectID": "6MultiLevel/Imputing_multilevel_data.html",
    "href": "6MultiLevel/Imputing_multilevel_data.html",
    "title": "6  mice: Imputing multi-level data",
    "section": "",
    "text": "This is the sixth vignette in a series of ten.\nIn this vignette we will focus on multi-level imputation. You need to have package pan installed. You can install it by running: install.packages(\"pan\").\n\n1. Open R and load the packages mice, ggmice, ggplot2 and pan.\n\nlibrary(mice)\nlibrary(ggmice)\nlibrary(ggplot2)\nlibrary(pan)\nset.seed(123)\n\nWe choose seed value 123. This is an arbitrary value; any value would be an equally good seed value. Fixing the random seed enables you (and others) to exactly replicate anything that involves random number generators. If you set the seed in your R instance to 123, you will get the exact same results and plots as we present in this document.\n\nWe are going to work with the popularity data from Joop Hox -Hox, Moerbeek, and Van de Schoot (2010). The variables in this data set are described as follows:\n\n\n\npupil\nPupil number within class\n\n\nclass\nClass number\n\n\nextrav\nPupil extraversion\n\n\nsex\nPupil gender\n\n\ntexp\nTeacher experience (years)\n\n\npopular\nPupil popularity\n\n\npopteach\nTeacher popularity\n\n\n\n\n\n6.0.1 Inspection of the incomplete data\n\n1. Open the popular.RData workspace. A workspace with complete and incomplete versions of the popularity data can be obtained here or can be loaded into the Global Environment by running:\n\ncon &lt;- url(\"https://www.gerkovink.com/mimp/popular.RData\")\nload(con)\n\nThis workspace contains several datasets and functions that, when loaded, are available to you in R. If you’d like to see what is inside: run the following code\n\nls()\n\n [1] \"con\"                      \"icc\"                     \n [3] \"mice.impute.2lonly.mean2\" \"pandoc_dir\"              \n [5] \"popMCAR\"                  \"popMCAR2\"                \n [7] \"popNCR\"                   \"popNCR2\"                 \n [9] \"popNCR3\"                  \"popular\"                 \n[11] \"quarto_bin_path\"         \n\n\nThe dataset popNCR is a variation on the Hox (2010) data, where the missingness in the variables is either missing at random (MAR) or missing not at random (MNAR).\n\n2. Check with the functions head(), dim() - alternatively one could use nrow() and ncol() instead of dim() - and summary() how large the dataset is, of which variables the data frame consists and if there are missing values in a variable.\n\nhead(popNCR)\n\n  pupil class extrav  sex texp popular popteach\n1     1     1      5    1   NA     6.3       NA\n2     2     1     NA    0   24     4.9       NA\n3     3     1      4    1   NA     5.3        6\n4     4     1      3 &lt;NA&gt;   NA     4.7        5\n5     5     1      5    1   24      NA        6\n6     6     1     NA    0   NA     4.7        5\n\ndim(popNCR)\n\n[1] 2000    7\n\nnrow(popNCR)\n\n[1] 2000\n\nncol(popNCR)\n\n[1] 7\n\nsummary(popNCR)\n\n     pupil           class          extrav         sex           texp     \n Min.   : 1.00   17     :  26   Min.   : 1.000   0   :661   Min.   : 2.0  \n 1st Qu.: 6.00   63     :  25   1st Qu.: 4.000   1   :843   1st Qu.: 7.0  \n Median :11.00   10     :  24   Median : 5.000   NA's:496   Median :12.0  \n Mean   :10.65   15     :  24   Mean   : 5.313              Mean   :11.8  \n 3rd Qu.:16.00   4      :  23   3rd Qu.: 6.000              3rd Qu.:16.0  \n Max.   :26.00   21     :  23   Max.   :10.000              Max.   :25.0  \n                 (Other):1855   NA's   :516                 NA's   :976   \n    popular         popteach     \n Min.   :0.000   Min.   : 1.000  \n 1st Qu.:3.900   1st Qu.: 4.000  \n Median :4.800   Median : 5.000  \n Mean   :4.829   Mean   : 4.834  \n 3rd Qu.:5.800   3rd Qu.: 6.000  \n Max.   :9.100   Max.   :10.000  \n NA's   :510     NA's   :528     \n\n\nThe data set has 2000 rows and 7 columns (variables). The variables extrav, sex, texp, popular and popteach contain missing values. About a quarter of these variables is missing, except for texp where 50 % is missing.\n\n3. As we have seen before, the function md.pattern() (or plot_pattern()) is used to display all different missing data patterns. How many different missing data patterns are present in the popNCR dataframe and which patterns occur most frequently in the data? Also find out how many patterns we would observe when variable texp (teacher experience) is not considered.\n\nplot_pattern(popNCR, rotate = TRUE)\n\n\n\n\n\n\n\n\nThere are 32 unique patterns. The pattern where everything is observed and the pattern where only texp is missing occur most frequently.\nIf we omit texp, then the following pattern matrix is realized:\n\nplot_pattern(popNCR[ , -5])\n\n\n\n\n\n\n\n\nWithout texp, there are only 16 patterns.\n\n4. Let’s focus more precisely on the missing data patterns. Does the missing data of popular depend on popteach? One could for example check this by making a histogram of popteach separately for the pupils with known popularity and missing popularity.\nIn R the missingness indicator\n\nis.na(popNCR$popular)\n\n   [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n  [13] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n  [25] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [37]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n  [49] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [61] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [73] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n  [85] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n  [97] FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [109] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [121] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [133]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [157]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [169]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [181] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [193]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n [205] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [217]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [229] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [241] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [253]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n [265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [277]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [289] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n [301]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [313] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [325] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n [337] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [349] FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE\n [361] FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [373] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [385] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [397] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [409] FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [421] FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n [433] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [445] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [457]  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [469] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [481] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [493] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE\n [505] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n [517] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE\n [529] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [541] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n [553] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [565]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE\n [577] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\n [589]  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [601] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [613] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [625] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [637] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n [649]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n [661]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [673] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [685]  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [697]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n [709]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [721] FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n [733] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n [745] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [757] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n [769] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n [793]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [817] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [829] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [841]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [853] FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [877] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [889]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [901]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE\n [913] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [925] FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n [937] FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [949] FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [961] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [973] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n [985] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [997]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE\n[1009] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1033] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n[1045]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n[1057] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[1069] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1081]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1093] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[1105] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1117] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[1129] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1141] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n[1153] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[1165] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n[1177] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[1189] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1201]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[1213] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[1225] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n[1237] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1249] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1261] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[1273]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[1285] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n[1297] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n[1309] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n[1321] FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE\n[1333]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[1345] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[1357] FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[1369] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n[1381] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1393] FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n[1405]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n[1417] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n[1429]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1441]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[1453]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[1465] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n[1477]  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[1489] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[1501]  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[1513] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[1525] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[1537]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1549]  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n[1561] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n[1573] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[1585] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[1597] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[1609] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1621] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1633] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1645] FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n[1657] FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[1669]  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[1681] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE\n[1693] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n[1705] FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[1717] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\n[1729] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1741] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[1753] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1765] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[1777] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[1789]  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE\n[1801]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[1813] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[1825] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1837] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n[1849] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[1861] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1873] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1885] FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[1897] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n[1909] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[1921]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1933]  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[1945] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE\n[1957] FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n[1969] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[1981] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[1993] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n\n\nis a dummy variable of the same length as popular with value 0 (FALSE) for observed pupil popularity and 1 (TRUE) for missing pupil popularity. The code for a conditional histogram of popteach given the missingness indicator for popular is\n\nggmice(popNCR, aes(popteach)) + \n  geom_histogram(fill = \"white\") +\n  facet_wrap(~ is.na(popular))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 528 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThe same figure with more informative labels is generated with\n\nggmice(popNCR, aes(popteach)) + \n  geom_histogram(fill = \"white\") +\n  facet_wrap(~ factor(\n    is.na(popular), \n    levels = c(FALSE, TRUE), \n    labels = c(\"popular observed\", \"popular missing\")))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 528 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWe do see that the histogram for the missing popular (TRUE) is further to the right than the histogram for observed popular (FALSE). This would indicate a right-tailed MAR missingness. In fact this is exactly what happens, because we created the missingness in these data ourselves. But we can make it observable by examining the relations between the missingness in popular and the observed data in popteach.\n\n5. Does the missingness of the other incomplete variables depend on popteach? If yes, what is the direction of the relation?\n\nggmice(popNCR, aes(popteach)) + \n  geom_histogram(fill = \"white\") +\n  facet_wrap(~ factor(\n    is.na(sex), \n    levels = c(FALSE, TRUE), \n    labels = c(\"sex observed\", \"sex missing\")))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 528 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThere seems to be a left-tailed relation between popteach and the missingness in sex.\n\nggmice(popNCR, aes(popteach)) + \n  geom_histogram(fill = \"white\") +\n  facet_wrap(~ factor(\n    is.na(extrav), \n    levels = c(FALSE, TRUE), \n    labels = c(\"extrav observed\", \"extrav missing\")))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 528 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThere also seems to be a left-tailed relation between popteach and the missingness in extrav.\n\nggmice(popNCR, aes(popteach)) + \n  geom_histogram(fill = \"white\") +\n  facet_wrap(~ factor(\n    is.na(texp), \n    levels = c(FALSE, TRUE), \n    labels = c(\"texp observed\", \"texp missing\")))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 528 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThere seems to be no observable relation between popteach and the missingness in texp. It might be MCAR or even MNAR.\n\n6. Find out if the missingness in teacher popularity depends on pupil popularity.\n\nggmice(popNCR, aes(popular)) + \n  geom_histogram(fill = \"white\") +\n  facet_wrap(~ factor(\n    is.na(popteach), \n    levels = c(FALSE, TRUE), \n    labels = c(\"popteach observed\", \"popteach missing\")))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 510 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nYes: there is a dependency. The relation seems to be right-tailed.\n\n7. Have a look at the intraclass correlation (ICC) for the incomplete variables popular, popteach and texp.\n\nicc(aov(popular ~ as.factor(class), data = popNCR))\n\n[1] 0.328007\n\nicc(aov(popteach ~ class, data = popNCR))\n\n[1] 0.3138658\n\nicc(aov(texp ~ class, data = popNCR))\n\n[1] 1\n\n\nPlease note that the function icc() comes from the package multilevel (function ICC1()), but is included in the workspace popular.RData. Write down the ICCs, you’ll need them later.\n\n7b. Do you think it is necessary to take the multilevel structure into account?\nYES! There is a strong cluster structure going on. If we ignore the clustering in our imputation model, we may run into invalid inference. To stay as close to the true data model, we must take the cluster structure into account during imputation.\n\n8. Impute the popNCR dataset with mice using imputation method norm for popular, popteach, texp and extrav. Exclude class as a predictor for all variables. Call the mids-object imp1.\n\nini &lt;- mice(popNCR, maxit = 0)\nmeth &lt;- ini$meth\nmeth\n\n   pupil    class   extrav      sex     texp  popular popteach \n      \"\"       \"\"    \"pmm\" \"logreg\"    \"pmm\"    \"pmm\"    \"pmm\" \n\nmeth[c(3, 5, 6, 7)] &lt;- \"norm\"\nmeth\n\n   pupil    class   extrav      sex     texp  popular popteach \n      \"\"       \"\"   \"norm\" \"logreg\"   \"norm\"   \"norm\"   \"norm\" \n\npred &lt;- ini$pred\npred\n\n         pupil class extrav sex texp popular popteach\npupil        0     1      1   1    1       1        1\nclass        1     0      1   1    1       1        1\nextrav       1     1      0   1    1       1        1\nsex          1     1      1   0    1       1        1\ntexp         1     1      1   1    0       1        1\npopular      1     1      1   1    1       0        1\npopteach     1     1      1   1    1       1        0\n\npred[, \"class\"] &lt;- 0\npred[, \"pupil\"] &lt;- 0\npred\n\n         pupil class extrav sex texp popular popteach\npupil        0     0      1   1    1       1        1\nclass        0     0      1   1    1       1        1\nextrav       0     0      0   1    1       1        1\nsex          0     0      1   0    1       1        1\ntexp         0     0      1   1    0       1        1\npopular      0     0      1   1    1       0        1\npopteach     0     0      1   1    1       1        0\n\nimp1 &lt;- mice(popNCR, meth = meth, pred = pred, print = FALSE)\n\n\n9. Compare the means of the variables in the first imputed dataset and in the incomplete dataset.\n\nsummary(complete(imp1))\n\n     pupil           class          extrav       sex           texp       \n Min.   : 1.00   17     :  26   Min.   : 1.000   0: 985   Min.   :-6.465  \n 1st Qu.: 6.00   63     :  25   1st Qu.: 4.139   1:1015   1st Qu.: 8.000  \n Median :11.00   10     :  24   Median : 5.000            Median :12.253  \n Mean   :10.65   15     :  24   Mean   : 5.269            Mean   :12.509  \n 3rd Qu.:16.00   4      :  23   3rd Qu.: 6.000            3rd Qu.:16.698  \n Max.   :26.00   21     :  23   Max.   :10.000            Max.   :35.745  \n                 (Other):1855                                             \n    popular          popteach     \n Min.   : 0.000   Min.   : 1.000  \n 1st Qu.: 4.100   1st Qu.: 4.000  \n Median : 5.000   Median : 5.000  \n Mean   : 5.006   Mean   : 5.021  \n 3rd Qu.: 5.971   3rd Qu.: 6.000  \n Max.   :10.547   Max.   :10.000  \n                                  \n\nsummary(popNCR)\n\n     pupil           class          extrav         sex           texp     \n Min.   : 1.00   17     :  26   Min.   : 1.000   0   :661   Min.   : 2.0  \n 1st Qu.: 6.00   63     :  25   1st Qu.: 4.000   1   :843   1st Qu.: 7.0  \n Median :11.00   10     :  24   Median : 5.000   NA's:496   Median :12.0  \n Mean   :10.65   15     :  24   Mean   : 5.313              Mean   :11.8  \n 3rd Qu.:16.00   4      :  23   3rd Qu.: 6.000              3rd Qu.:16.0  \n Max.   :26.00   21     :  23   Max.   :10.000              Max.   :25.0  \n                 (Other):1855   NA's   :516                 NA's   :976   \n    popular         popteach     \n Min.   :0.000   Min.   : 1.000  \n 1st Qu.:3.900   1st Qu.: 4.000  \n Median :4.800   Median : 5.000  \n Mean   :4.829   Mean   : 4.834  \n 3rd Qu.:5.800   3rd Qu.: 6.000  \n Max.   :9.100   Max.   :10.000  \n NA's   :510     NA's   :528     \n\n\n\n9b. The missingness in texp is MNAR: higher values for texp have a larger probability to be missing. Can you see this in the imputed data? Do you think this is a problem?\nYes, we can see this in the imputed data: teacher experience increases slightly after imputation. However, texp is the same for all pupils in a class. But not all pupils have this information recorded (as if some pupils did not remember, or were not present during data collection). This is not a problem, because as long as at least one pupil in each class has teacher experience recorded, we can deductively impute the correct (i.e. true) value for every pupil in the class.\n\n10. Compare the ICCs of the variables in the first imputed dataset with those in the incomplete dataset (use popular, popteach and texp). Make a notation of the ICCs after imputation.\n\ndata.frame(vars = names(popNCR[c(6, 7, 5)]), \n           observed = c(icc(aov(popular ~ class, popNCR)), \n                        icc(aov(popteach ~ class, popNCR)), \n                        icc(aov(texp ~ class, popNCR))), \n           norm     = c(icc(aov(popular ~ class, complete(imp1))), \n                        icc(aov(popteach ~ class, complete(imp1))), \n                        icc(aov(texp ~ class, complete(imp1)))))\n\n      vars  observed      norm\n1  popular 0.3280070 0.2798518\n2 popteach 0.3138658 0.2639095\n3     texp 1.0000000 0.4595004\n\n\n\n11. Now impute the popNCR dataset again with mice using imputation method norm for popular, popteach, texp and extrav, but now include class as a predictor for all variables. Call the mids-object imp2.\n\npred &lt;- ini$pred\npred[, \"pupil\"] &lt;- 0\nimp2 &lt;- mice(popNCR, meth = meth, pred = pred, print = FALSE)\n\nWarning: Number of logged events: 90\n\n\n\n12. Compare the ICCs of the variables in the first imputed dataset from imp2 with those of imp1 and the incomplete dataset (use popular, popteach and texp). Make a notation of the ICCs after imputation.\n\ndata.frame(vars = names(popNCR[c(6, 7, 5)]), \n           observed  = c(icc(aov(popular ~ class, popNCR)), \n                         icc(aov(popteach ~ class, popNCR)), \n                         icc(aov(texp ~ class, popNCR))), \n           norm      = c(icc(aov(popular ~ class, complete(imp1))), \n                         icc(aov(popteach ~ class, complete(imp1))), \n                         icc(aov(texp ~ class, complete(imp1)))), \n           normclass = c(icc(aov(popular ~ class, complete(imp2))), \n                         icc(aov(popteach ~ class, complete(imp2))), \n                         icc(aov(texp ~ class, complete(imp2)))))\n\n      vars  observed      norm normclass\n1  popular 0.3280070 0.2798518 0.3629046\n2 popteach 0.3138658 0.2639095 0.3326133\n3     texp 1.0000000 0.4595004 1.0000000\n\n\nBy simply forcing the algorithm to use the class variable during estimation we adopt a fixed effects approach. This conforms to formulating seperate regression models for each class and imputing within classes from these models.\n\n\n\n6.0.2 Checking Convergence of the imputations\n\n13. Inspect the trace lines for the variables popular, texp and extrav.\n\nplot_trace(imp2, c(\"popular\", \"texp\", \"popteach\"))\n\n\n\n\n\n\n\n\n\n14. Add another 10 iterations and inspect the trace lines again. What do you observe with respect to the convergence of the sampler?\n\nimp3 &lt;- mice.mids(imp2, maxit = 10)\n\n\n iter imp variable\n  6   1  extrav  sex  texp  popular  popteach\n  6   2  extrav  sex  texp  popular  popteach\n  6   3  extrav  sex  texp  popular  popteach\n  6   4  extrav  sex  texp  popular  popteach\n  6   5  extrav  sex  texp  popular  popteach\n  7   1  extrav  sex  texp  popular  popteach\n  7   2  extrav  sex  texp  popular  popteach\n  7   3  extrav  sex  texp  popular  popteach\n  7   4  extrav  sex  texp  popular  popteach\n  7   5  extrav  sex  texp  popular  popteach\n  8   1  extrav  sex  texp  popular  popteach\n  8   2  extrav  sex  texp  popular  popteach\n  8   3  extrav  sex  texp  popular  popteach\n  8   4  extrav  sex  texp  popular  popteach\n  8   5  extrav  sex  texp  popular  popteach\n  9   1  extrav  sex  texp  popular  popteach\n  9   2  extrav  sex  texp  popular  popteach\n  9   3  extrav  sex  texp  popular  popteach\n  9   4  extrav  sex  texp  popular  popteach\n  9   5  extrav  sex  texp  popular  popteach\n  10   1  extrav  sex  texp  popular  popteach\n  10   2  extrav  sex  texp  popular  popteach\n  10   3  extrav  sex  texp  popular  popteach\n  10   4  extrav  sex  texp  popular  popteach\n  10   5  extrav  sex  texp  popular  popteach\n  11   1  extrav  sex  texp  popular  popteach\n  11   2  extrav  sex  texp  popular  popteach\n  11   3  extrav  sex  texp  popular  popteach\n  11   4  extrav  sex  texp  popular  popteach\n  11   5  extrav  sex  texp  popular  popteach\n  12   1  extrav  sex  texp  popular  popteach\n  12   2  extrav  sex  texp  popular  popteach\n  12   3  extrav  sex  texp  popular  popteach\n  12   4  extrav  sex  texp  popular  popteach\n  12   5  extrav  sex  texp  popular  popteach\n  13   1  extrav  sex  texp  popular  popteach\n  13   2  extrav  sex  texp  popular  popteach\n  13   3  extrav  sex  texp  popular  popteach\n  13   4  extrav  sex  texp  popular  popteach\n  13   5  extrav  sex  texp  popular  popteach\n  14   1  extrav  sex  texp  popular  popteach\n  14   2  extrav  sex  texp  popular  popteach\n  14   3  extrav  sex  texp  popular  popteach\n  14   4  extrav  sex  texp  popular  popteach\n  14   5  extrav  sex  texp  popular  popteach\n  15   1  extrav  sex  texp  popular  popteach\n  15   2  extrav  sex  texp  popular  popteach\n  15   3  extrav  sex  texp  popular  popteach\n  15   4  extrav  sex  texp  popular  popteach\n  15   5  extrav  sex  texp  popular  popteach\n\nplot_trace(imp3, c(\"popular\", \"texp\", \"popteach\"))\n\n\n\n\n\n\n\n\nIt seems OK. Adding another 20 iterations confirms this.\n\nimp3b &lt;- mice.mids(imp3, maxit = 20, print = FALSE)\nplot_trace(imp3b, c(\"popular\", \"texp\", \"popteach\"))\n\n\n\n\n\n\n\n\n\nFurther inspection\nSeveral plotting methods based on the package lattice for Trellis graphics are implemented in mice for imputed data. Equivalent graphs with ggplot2 can be created using the ggmice package.\n\n15. Plot the densities of the observed and imputed data (use imp2) with the function densityplot().\nTo obtain all densities of the different imputed datasets use\n\ndensityplot(imp2)\n\nHint: Did you know, an equivalent figure can be created with `ggmice()`?\nFor example, to plot a variable named 'my_vrb' from a mids object called\n'my_mids', run:\n  ggmice(my_mids, ggplot2::aes(x = my_vrb, group = .imp)) +\n  ggplot2::geom_density()\nℹ See amices.org/ggmice for more info.\nThis message is displayed once per session.\n\n\n\n\n\n\n\n\n\nTo obtain just the densities for popular one can use\n\ndensityplot(imp2, ~ popular)\n\n\n\n\n\n\n\n\nor\n\ndensityplot(imp2, ~ popular | .imp)\n\n\n\n\n\n\n\n\nThe latter case results in a conditional plot (conditional on the different imputed datasets).\nThe ggmice alternative for a density plot is\n\nggmice(imp2, aes(popular, group = .imp)) + \n  geom_density() \n\n\n\n\n\n\n\n\nand for the conditional plot is\n\nggmice(imp2, aes(popular)) + \n  geom_density() +\n  facet_wrap(~.imp)\n\n\n\n\n\n\n\n\nThe full set of density plots for the variables of interest can be generated with\n\npurrr::map(c(\"popular\", \"texp\", \"popteach\"), ~ {\n  ggmice(imp2, aes(x = .data[[.x]], group = .imp)) +\n    geom_density() \n})\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n16. Have a look at the imputed dataset by asking the first 15 rows of the first completed dataset for imp2. What do you think of the imputed values?\n\ncomplete(imp2, 1)[1:15, ]\n\n   pupil class   extrav sex texp  popular popteach\n1      1     1 5.000000   1   24 6.300000 6.314991\n2      2     1 3.616012   0   24 4.900000 4.343070\n3      3     1 4.000000   1   24 5.300000 6.000000\n4      4     1 3.000000   0   24 4.700000 5.000000\n5      5     1 5.000000   1   24 5.656993 6.000000\n6      6     1 3.564456   0   24 4.700000 5.000000\n7      7     1 5.000000   0   24 5.900000 5.000000\n8      8     1 4.000000   0   24 4.484762 4.389721\n9      9     1 5.000000   0   24 4.686309 5.000000\n10    10     1 5.000000   0   24 3.900000 3.000000\n11    11     1 3.217854   1   24 5.700000 5.000000\n12    12     1 5.000000   0   24 4.800000 5.000000\n13    13     1 5.000000   0   24 5.000000 5.000000\n14    14     1 5.000000   1   24 6.157194 6.000000\n15    15     1 5.000000   1   24 6.000000 5.000000\n\n\nor, alternatively\n\nhead(complete(imp2, 1), n = 15)\n\n   pupil class   extrav sex texp  popular popteach\n1      1     1 5.000000   1   24 6.300000 6.314991\n2      2     1 3.616012   0   24 4.900000 4.343070\n3      3     1 4.000000   1   24 5.300000 6.000000\n4      4     1 3.000000   0   24 4.700000 5.000000\n5      5     1 5.000000   1   24 5.656993 6.000000\n6      6     1 3.564456   0   24 4.700000 5.000000\n7      7     1 5.000000   0   24 5.900000 5.000000\n8      8     1 4.000000   0   24 4.484762 4.389721\n9      9     1 5.000000   0   24 4.686309 5.000000\n10    10     1 5.000000   0   24 3.900000 3.000000\n11    11     1 3.217854   1   24 5.700000 5.000000\n12    12     1 5.000000   0   24 4.800000 5.000000\n13    13     1 5.000000   0   24 5.000000 5.000000\n14    14     1 5.000000   1   24 6.157194 6.000000\n15    15     1 5.000000   1   24 6.000000 5.000000\n\n\n\n17. Impute the popNCR data once more where you use predictive mean matching and include all variables as predictors. Name the object imp4.\n\nimp4 &lt;- mice(popNCR)\n\n\n iter imp variable\n  1   1  extrav  sex  texp  popular  popteach\n  1   2  extrav  sex  texp  popular  popteach\n  1   3  extrav  sex  texp  popular  popteach\n  1   4  extrav  sex  texp  popular  popteach\n  1   5  extrav  sex  texp  popular  popteach\n  2   1  extrav  sex  texp  popular  popteach\n  2   2  extrav  sex  texp  popular  popteach\n  2   3  extrav  sex  texp  popular  popteach\n  2   4  extrav  sex  texp  popular  popteach\n  2   5  extrav  sex  texp  popular  popteach\n  3   1  extrav  sex  texp  popular  popteach\n  3   2  extrav  sex  texp  popular  popteach\n  3   3  extrav  sex  texp  popular  popteach\n  3   4  extrav  sex  texp  popular  popteach\n  3   5  extrav  sex  texp  popular  popteach\n  4   1  extrav  sex  texp  popular  popteach\n  4   2  extrav  sex  texp  popular  popteach\n  4   3  extrav  sex  texp  popular  popteach\n  4   4  extrav  sex  texp  popular  popteach\n  4   5  extrav  sex  texp  popular  popteach\n  5   1  extrav  sex  texp  popular  popteach\n  5   2  extrav  sex  texp  popular  popteach\n  5   3  extrav  sex  texp  popular  popteach\n  5   4  extrav  sex  texp  popular  popteach\n  5   5  extrav  sex  texp  popular  popteach\n\n\nWarning: Number of logged events: 90\n\n\n\n18. Plot again the densities of the observed and imputed data with the function densityplot(), but now use imp4. Is there a difference between the imputations obtained with pmm and norm and can you explain this?\n\ndensityplot(imp4)\n\n\n\n\n\n\n\n\nYes, pmm samples from the observed values and this clearly shows: imputations follow the shape of the observed data.\n\n19. Compare the ICCs of the variables in the first imputed dataset from imp4 with those of imp1, imp2 and the incomplete dataset (use popular, popteach and texp).\nSee Exercise 20 for the solution.\n\n20. Finally, compare the ICCs of the imputations to the ICCs in the original data. The original data can be found in dataset popular. What do you conclude?\n\ndata.frame(vars      = names(popNCR[c(6, 7, 5)]), \n           observed  = c(icc(aov(popular ~ class, popNCR)), \n                         icc(aov(popteach ~ class, popNCR)), \n                         icc(aov(texp ~ class, popNCR))), \n           norm      = c(icc(aov(popular ~ class, complete(imp1))), \n                         icc(aov(popteach ~ class, complete(imp1))), \n                         icc(aov(texp ~ class, complete(imp1)))), \n           normclass = c(icc(aov(popular ~ class, complete(imp2))), \n                         icc(aov(popteach ~ class, complete(imp2))), \n                         icc(aov(texp ~ class, complete(imp2)))), \n           pmm       = c(icc(aov(popular ~ class, complete(imp4))), \n                         icc(aov(popteach ~ class, complete(imp4))), \n                         icc(aov(texp ~ class, complete(imp4)))), \n           orig      = c(icc(aov(popular ~ as.factor(class), popular)), \n                         icc(aov(popteach ~ as.factor(class), popular)), \n                         icc(aov(texp ~ as.factor(class), popular))))\n\n      vars  observed      norm normclass       pmm      orig\n1  popular 0.3280070 0.2798518 0.3629046 0.3700960 0.3629933\n2 popteach 0.3138658 0.2639095 0.3326133 0.3385374 0.3414766\n3     texp 1.0000000 0.4595004 1.0000000 1.0000000 1.0000000\n\n\nNote: these display only the first imputed data set.\n\n\n\n6.0.3 Changing the imputation method\nMice includes several imputation methods for imputing multilevel data:\n\n2l.norm: Imputes univariate missing data using a two-level normal model with heterogeneous within group variances\n2l.pan: Imputes univariate missing data using a two-level normal model with homogeneous within group variances\n2lonly.mean: Imputes the mean of the class within the class\n2lonly.norm: Imputes univariate missing data at level 2 using Bayesian linear regression analysis\n2lonly.pmm: Imputes univariate missing data at level 2 using predictive mean matching\n\nThe latter two methods aggregate level 1 variables at level 2, but in combination with mice.impute.2l.pan, allow switching regression imputation between level 1 and level 2 as described in Yucel (2008) or Gelman and Hill (2007, 541). For more information on these imputation methods see the help.\n\n21. Impute the variable popular by means of 2l.norm. Use dataset popNCR2.\n\nini &lt;- mice(popNCR2, maxit = 0)\npred &lt;- ini$pred\npred[\"popular\", ] &lt;- c(0, -2, 2, 2, 2, 0, 2)\n\nIn the predictor matrix, -2 denotes the class variable, a value 1 indicates a fixed effect and a value 2 indicates a random effect. However, the currently implemented algorithm does not handle predictors that are specified as fixed effects (type = 1). When using mice.impute.2l.norm(), the current advice is to specify all predictors as random effects (type = ``2).\n\nmeth &lt;- ini$meth\nmeth &lt;- c(\"\", \"\", \"\", \"\", \"\", \"2l.norm\", \"\")\nimp5 &lt;- mice(popNCR2, pred = pred, meth=meth, print = FALSE)\n\n22. Inspect the imputations. Did the algorithm converge? Plot the distribution of the imputed values for the variable `pop\n\ndensityplot(imp5, ~popular, ylim = c(0, 0.35), xlim = c(-1.5, 10))\n\n\n\n\n\n\n\ndensityplot(imp4, ~popular, ylim = c(0, 0.35), xlim = c(-1.5, 10))\n\n\n\n\n\n\n\n\nAlternatively, these figures can be created with\n\nggmice(imp5, aes(popular, group = .imp)) + \n  geom_density()+\n  scale_x_continuous(limits = c(0, 10))\n\n\n\n\n\n\n\nggmice(imp4, aes(popular, group = .imp)) + \n  geom_density() +\n  scale_x_continuous(limits = c(0, 10))\n\n\n\n\n\n\n\n\nThe imputations generated with 2l.norm are very similar to the ones obtained by pmm with class as a fixed effect. If we plot the first imputed dataset from imp4 and imp5 against the original (true) data:\n\nplot(density(popular$popular))  #true data \nlines(density(complete(imp5)$popular), col = \"red\", lwd = 2)  #2l.norm\nlines(density(complete(imp4)$popular), col = \"green\", lwd = 2)  #PMM\n\n\n\n\n\n\n\n\nWe can see that the imputations are very similar. When studying the convergence\n\nplot_trace(imp5)\n\n\n\n\n\n\n\n\nwe conclude that it may be wise to run additional iterations. Convergence is not apparent from this plot.\n\nimp5.b &lt;- mice.mids(imp5, maxit = 10, print = FALSE)\nplot_trace(imp5.b)\n\n\n\n\n\n\n\n\nAfter running another 10 iterations, convergence is more convincing.\n\n23. In the original data, the group variances for popular are homogeneous. Use 2l.pan to impute the variable popular in dataset popNCR2. Inspect the imputations. Did the algorithm converge?\n\nini &lt;- mice(popNCR2, maxit = 0)\npred &lt;- ini$pred\npred[\"popular\", ] &lt;- c(0, -2, 2, 2, 1, 0, 2)\nmeth &lt;- ini$meth\nmeth &lt;- c(\"\", \"\", \"\", \"\", \"\", \"2l.pan\", \"\")\nimp6 &lt;- mice(popNCR2, pred = pred, meth = meth, print = FALSE)\n\nLet us create the densityplot for imp6\n\ndensityplot(imp6, ~popular, ylim = c(0, 0.35), xlim = c(-1.5, 10))\n\n\n\n\n\n\n\n\nand compare it to the one for imp4\n\ndensityplot(imp4, ~popular, ylim = c(0, 0.35), xlim = c(-1.5, 10))\n\n\n\n\n\n\n\n\nIf we plot the first imputed dataset from both objects against the original (true) density, we obtain the following plot:\n\nplot(density(popular$popular), main = \"black = truth | green = PMM | red = 2l.pan\")  # \nlines(density(complete(imp6)$popular), col = \"red\", lwd = 2)  #2l.pan\nlines(density(complete(imp4)$popular), col = \"green\", lwd = 2)  #PMM\n\n\n\n\n\n\n\n\nWe can see that the imputations are very similar. When studying the convergence\n\nplot_trace(imp6)\n\n\n\n\n\n\n\n\nwe conclude that it may be wise to run additional iterations. Convergence is not apparent from this plot.\n\nimp6.b &lt;- mice.mids(imp5, maxit = 10, print = FALSE)\nplot_trace(imp6.b)\n\n\n\n\n\n\n\n\nAgain, after running another 10 iterations, convergence is more convincing.\n\n24. Now inspect dataset popNCR3 and impute the incomplete variables according to the following imputation methods:\n\n\n\nVariable\nMethod\n\n\n\n\nextrav\n2l.norm\n\n\ntexp\n2lonly.mean\n\n\nsex\nlogreg\n\n\npopular\n2l.pan\n\n\npopteach\n2l.pan\n\n\n\n\nini &lt;- mice(popNCR3, maxit = 0)\npred &lt;- ini$pred\npred[\"extrav\", ] &lt;- c(0, -2, 0, 2, 2, 2, 2)  #2l.norm\npred[\"sex\", ] &lt;- c(0, 1, 1, 0, 1, 1, 1)  #2logreg\npred[\"texp\", ] &lt;- c(0, -2, 1, 1, 0, 1, 1)  #2lonly.mean\npred[\"popular\", ] &lt;- c(0, -2, 2, 2, 1, 0, 2)  #2l.pan\npred[\"popteach\", ] &lt;- c(0, -2, 2, 2, 1, 2, 0)  #2l.pan\nmeth &lt;- ini$meth\nmeth &lt;- c(\"\", \"\", \"2l.norm\", \"logreg\", \"2lonly.mean\", \"2l.pan\", \"2l.pan\")\nimp7 &lt;- mice(popNCR3, pred = pred, meth = meth, print = FALSE)\n\n\n25. Evaluate the imputations by means of convergence, distributions and plausibility.\n\ndensityplot(imp7)\n\n\n\n\n\n\n\n\nGiven what we know about the missingness, the imputed densities look very reasonable.\n\nplot_trace(imp7)\n\n\n\n\n\n\n\n\nConvergence has not yet been reached. more iterations are advisable.\n\n26. Repeat the same imputations as in the previous step, but now use pmm for everything.\n\npmmdata &lt;- popNCR3\npmmdata$class &lt;- as.factor(popNCR3$class)\nimp8 &lt;- mice(pmmdata, m = 5, print = FALSE)\n\nWarning: Number of logged events: 90\n\n\nWith pmm, the imputations are very similar and conform to the shape of the observed data.\n\ndensityplot(imp8)\n\n\n\n\n\n\n\n\nWhen looking at the convergence of pmm, more iterations are advisable:\n\nplot_trace(imp8)\n\n\n\n\n\n\n\n\n\nConclusions\nThere are ways to ensure that imputations are not just “guesses of unobserved values”. Imputations can be checked by using a standard of reasonability. We are able to check the differences between observed and imputed values, the differences between their distributions as well as the distribution of the completed data as a whole. If we do this, we can see whether imputations make sense in the context of the problem being studied.\n\n- End of Vignette\n\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge university press.\n\n\nHox, Joop, Mirjam Moerbeek, and Rens Van de Schoot. 2010. Multilevel Analysis: Techniques and Applications. Routledge.\n\n\nYucel, Recai M. 2008. “Multiple Imputation Inference for Multivariate Multilevel Continuous Data with Ignorable Non-Response.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 366 (1874): 2389–2403.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>`mice`: Imputing multi-level data</span>"
    ]
  },
  {
    "objectID": "7Sensitivity/SenstivityAnalysis.html",
    "href": "7Sensitivity/SenstivityAnalysis.html",
    "title": "7  mice: An approach to sensitivity analysis",
    "section": "",
    "text": "This is the seventh vignette in the series.\nThe focus of this document is on sensitivity analysis in the context of missing data. The goal of sensitivity analysis is to study the influence that violations of the missingness assumptions have on the obtained inference.\n\nData set\nThis is a synthetic dataset derived from 956 members of a very old (85+) cohort in Leiden. Unfortunately, we can not share the original leiden dataset with you. Thats why we have provided a synthetic version of leiden dataset. Multiple imputation of original leiden data set has been described in Boshuizen et al (1998), Van Buuren et al (1999) and Van Buuren (2012), chapter 7.\nThe main question is how blood pressure affects mortality risk in the oldest old. We have reasons to mistrust the MAR assumption in this case. In particular, we worried whether the imputations of blood pressure under MAR would be low enough. The sensitivity analysis explores the effect of artificially lowering the imputed blood pressure by deducting an amount of δ from the values imputed under MAR. In order to preserve the relations between the variables, this needs to be done during the iterations.\n\n1. Open R and load the packages mice, lattice and survival.\n\nset.seed(123)\nlibrary(\"mice\")\nlibrary(\"lattice\")\nlibrary(\"survival\")\n\n\n2. Overview of data set.\n\nsummary(synthetic_leiden)\n\n sexe       lftanam           rrsyst       rrdiast            dwa        \n 0:672   Min.   : 85.48   Min.   : 90   Min.   : 50.00   Min.   :0.0000  \n 1:284   1st Qu.: 87.41   1st Qu.:135   1st Qu.: 75.00   1st Qu.:0.0000  \n         Median : 89.03   Median :150   Median : 80.00   Median :0.0000  \n         Mean   : 89.76   Mean   :152   Mean   : 82.49   Mean   :0.2312  \n         3rd Qu.: 91.53   3rd Qu.:170   3rd Qu.: 90.00   3rd Qu.:0.0000  \n         Max.   :103.54   Max.   :260   Max.   :135.00   Max.   :1.0000  \n                          NA's   :121   NA's   :126                      \n     survda            alb             chol             mmse       woon   \n Min.   :   2.0   Min.   :25.00   Min.   : 2.900   Min.   : 1.00   0:399  \n 1st Qu.: 541.5   1st Qu.:39.00   1st Qu.: 4.800   1st Qu.:21.00   1: 35  \n Median :1175.5   Median :41.00   Median : 5.700   Median :26.00   2: 41  \n Mean   :1188.8   Mean   :40.71   Mean   : 5.704   Mean   :23.76   3:349  \n 3rd Qu.:1875.0   3rd Qu.:43.00   3rd Qu.: 6.400   3rd Qu.:28.00   4:132  \n Max.   :2610.0   Max.   :49.00   Max.   :10.900   Max.   :30.00          \n                  NA's   :229     NA's   :232      NA's   :85             \n\nstr(synthetic_leiden)\n\n'data.frame':   956 obs. of  10 variables:\n $ sexe   : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 1 1 2 1 1 ...\n $ lftanam: num  94.6 87.5 87.3 87 89.6 ...\n $ rrsyst : num  160 120 120 180 120 130 170 135 200 160 ...\n $ rrdiast: num  90 70 75 95 60 85 90 80 75 70 ...\n $ dwa    : num  0 0 0 0 0 0 0 0 0 0 ...\n $ survda : num  1199 871 905 1035 300 ...\n $ alb    : num  38 NA 43 44 43 NA 39 NA 43 44 ...\n $ chol   : num  5.5 NA 4.2 5.3 6.7 NA 6.1 NA 5.3 7.3 ...\n $ mmse   : num  22 25 29 29 14 NA 28 30 25 14 ...\n $ woon   : Factor w/ 5 levels \"0\",\"1\",\"2\",\"3\",..: 4 5 5 1 1 4 1 5 2 4 ...\n\nhead(synthetic_leiden)\n\n  sexe lftanam rrsyst rrdiast dwa survda alb chol mmse woon\n1    0   94.56    160      90   0   1199  38  5.5   22    3\n2    0   87.48    120      70   0    871  NA   NA   25    4\n3    1   87.32    120      75   0    905  43  4.2   29    4\n4    0   87.05    180      95   0   1035  44  5.3   29    0\n5    0   89.61    120      60   0    300  43  6.7   14    0\n6    0   91.39    130      85   0    926  NA   NA   NA    3\n\ntail(synthetic_leiden)\n\n    sexe lftanam rrsyst rrdiast dwa survda alb chol mmse woon\n951    1   92.30    125      80   0      6  42  2.9   15    0\n952    0   94.64    190      80   0   1452  45  5.7   30    3\n953    0   93.08    150      95   0    800  34  6.9   21    0\n954    0   87.98    140      65   0    587  40  5.4   23    0\n955    1   86.43    135      80   0    376  37  5.2   28    0\n956    1   86.86    140      80   0    573  44  6.7   16    3\n\n\n\n3. Perform a dry run (using maxit = 0) in mice. List the number of missing values per variable.\n\nini &lt;- mice(synthetic_leiden, maxit = 0)\nsort(ini$nmis)\n\n   sexe lftanam     dwa  survda    woon    mmse  rrsyst rrdiast     alb    chol \n      0       0       0       0       0      85     121     126     229     232 \n\n\nThere are 121 missings (NA’s) for rrsyst, 126 missings for rrdiast, 229 missings for alb, 232 missings for chol and 85 missing values for mmse.\n\n4. Study the missing data pattern in more detail using md.pattern() and fluxplot(). The interest here focusses on imputing systolic blood pressure (rrsyst) and diastolic blood pressure (rrdiast).\n\nmd.pattern(synthetic_leiden)\n\n\n\n\n\n\n\n\n    sexe lftanam dwa survda woon mmse rrsyst rrdiast alb chol    \n621    1       1   1      1    1    1      1       1   1    1   0\n2      1       1   1      1    1    1      1       1   1    0   1\n1      1       1   1      1    1    1      1       1   0    1   1\n149    1       1   1      1    1    1      1       1   0    0   2\n2      1       1   1      1    1    1      1       0   1    1   1\n2      1       1   1      1    1    1      1       0   0    0   3\n72     1       1   1      1    1    1      0       0   1    1   2\n2      1       1   1      1    1    1      0       0   1    0   3\n20     1       1   1      1    1    1      0       0   0    0   4\n21     1       1   1      1    1    0      1       1   1    1   1\n36     1       1   1      1    1    0      1       1   0    0   3\n1      1       1   1      1    1    0      1       0   0    0   4\n7      1       1   1      1    1    0      0       0   1    1   3\n20     1       1   1      1    1    0      0       0   0    0   5\n       0       0   0      0    0   85    121     126 229  232 793\n\nfx &lt;- fluxplot(synthetic_leiden)\n\n\n\n\n\n\n\n\nVariables with higher outflux are (potentially) the more powerful predictors. Variables with higher influx depend stronger on the imputation model. When points are relatively close to the diagonal, it indicates that influx and outflux are balanced.\nThe variables in the upper left corner have the more complete information, so the number of missing data problems for this group is relatively small. The variables in the middle have an outflux between 0.5 and 0.8, which is small. Missing data problems are thus more severe, but potentially this group could also contain important variables. The lower (bottom) variables have an outflux with 0.5 or lower, so their predictive power is limited. Also, this group has a higher influx, and, thus, depend more highly on the imputation model.\nIf you’d like this information in tabulated form, you can simply ask\n\nfx\n\n             pobs     influx   outflux      ainb       aout      fico\nsexe    1.0000000 0.00000000 1.0000000 0.0000000 0.09216643 0.3504184\nlftanam 1.0000000 0.00000000 1.0000000 0.0000000 0.09216643 0.3504184\nrrsyst  0.8734310 0.09798107 0.5573770 0.7887971 0.05881570 0.2562874\nrrdiast 0.8682008 0.10231550 0.5422446 0.7910053 0.05756359 0.2518072\ndwa     1.0000000 0.00000000 1.0000000 0.0000000 0.09216643 0.3504184\nsurvda  1.0000000 0.00000000 1.0000000 0.0000000 0.09216643 0.3504184\nalb     0.7604603 0.19311053 0.2471627 0.8214459 0.02995568 0.1458047\nchol    0.7573222 0.19573400 0.2383354 0.8218391 0.02900552 0.1422652\nmmse    0.9110879 0.06798221 0.6796974 0.7790850 0.06875877 0.2870264\nwoon    1.0000000 0.00000000 1.0000000 0.0000000 0.09216643 0.3504184\n\n\n\n5. The cases with and without blood pressure observed have very different survival rates. Show this.\nWe can see this easily from the Kaplan-Meier plot.\n\nsynthetic_leiden$dwa &lt;- as.numeric(synthetic_leiden$dwa)\nkm &lt;- survfit(Surv(survda/365, dwa) ~ is.na(rrsyst), data = synthetic_leiden) \nplot(km, \n     lty  = 1, \n     lwd  = 1.5, \n     xlab = \"Years since intake\",\n     ylab = \"K-M Survival probability\", las=1, \n     col  = c(mdc(4), mdc(5)), \n     mark.time = FALSE)\ntext(6.5, 0.7, \"BP measured\")\ntext(5, 0.3, \"BP missing\")\n\n\n\n\n\n\n\n\nIn the next steps we are going to impute rrsyst and rrdiast under two scenarios: MAR and MNAR. We will use the delta adjustment technique described in paragraph 7.2.3 in Van Buuren (2012)\n\n6. Create a \\(\\delta\\) vector that represent the following adjustment values for mmHg: 0 for MAR, and -5, -10, -15, and -20 for MNAR.\n\ndelta &lt;- c(0, -5, -10, -15, -20)\n\nThe recipe for creating MNAR imputations for \\(\\delta \\neq 0\\) uses the post-processing facility of mice. This allows to change the imputations on the fly by deducting a value of \\(\\delta\\) from the values just imputed.\n\n7. Impute the leiden data using the delta adjustment technique. We only have to deduct from rrsyst, because rrdiast will adapt to the changed rrsyst when it is imputed using rrsyst as predictor. Store the five imputed scenarios (adjustment) in a list called imp.all.\n\nimp.all &lt;- vector(\"list\", length(delta))\npost &lt;- ini$post\nfor (i in 1:length(delta)){\n  d &lt;- delta[i]\n  cmd &lt;- paste(\"imp[[j]][,i] &lt;- imp[[j]][,i] +\", d)\n  post[\"rrsyst\"] &lt;- cmd\n  imp &lt;- mice(synthetic_leiden, post = post, maxit = 5, seed = i, print = FALSE)\n  imp.all[[i]] &lt;- imp\n}\n\n\n8. Inspect the imputations. Compare the imputations for blood pressure under the most extreme scenarios with a box-and-whiskers plot. Is this as expected?\nFor the scenario where \\(\\delta = 0\\) we can plot the first object from the list. This object is the mids-object that considers imputations under no adjustment.\n\nbwplot(imp.all[[1]])\n\n\n\n\n\n\n\n\nFor the scenario where \\(\\delta = -20\\) we can plot the fifth object from the list. This object is the mids-object that considers imputations under the largest adjustment.\n\nbwplot(imp.all[[5]])\n\n\n\n\n\n\n\n\nWe can clearly see that the adjustment has an effect on the imputations for rrsyst and, thus, on those for rrdiast.\n\n9. Use the density plot for another inspection.\nFor the scenario where\\(\\delta = 0\\) we can plot the first object from the list. This object is the mids-object that considers imputations under no adjustment.\n\ndensityplot(imp.all[[1]], lwd = 3)\n\n\n\n\n\n\n\n\nFor the scenario where \\(\\delta = -20\\) we can plot the fifth object from the list. This object is the mids-object that considers imputations under the largest adjustment.\n\ndensityplot(imp.all[[5]], lwd = 3)\n\n\n\n\n\n\n\n\nWe can once more clearly see that the adjustment has an effect on the imputations for rrsyst and, thus, on those for rrdiast.\n\n10. Also create a scatter plot of rrsyst and rrdiast by imputation number and missingness.\n\nxyplot(imp.all[[1]], rrsyst ~ rrdiast | .imp)\n\n\n\n\n\n\n\nxyplot(imp.all[[5]], rrsyst ~ rrdiast | .imp)\n\n\n\n\n\n\n\n\nThe scatter plot comparison between rrsyst and rrdiast shows us that the adjustment has an effect on the imputations and that the imputations are lower for the situation where \\(\\delta = -20\\).\n\nWe are now going to perform a complete-data analysis. This involves several steps:\n\nCreate two categorical variables sbpgp and agegp that divide the observations into groups based on, respectively, systolic blood pressure and age.\nCalculate whether person died or not.\nFit a Cox proportional hazards model to estimate the relative mortality risk corrected for sex and age group.\n\nIn order to automate this step we should create an expression object that performs these stepd for us. The following object does so:\n\ncda &lt;- expression(\n  sbpgp &lt;- cut(rrsyst, breaks = c(50, 124, 144, 164, 184, 200, 500)),\n  agegp &lt;- cut(lftanam, breaks = c(85, 90, 95, 110)),\n  dead  &lt;- 1 - dwa,\n  coxph(Surv(survda, dead) ~ C(sbpgp, contr.treatment(6, base = 3)) + strata(sexe, agegp))\n  )\n\nSee Van Buuren (2012, 186) for more information.\n\n11. Create five fit objects that run the expression cda on the five imputed adjustment scenarios. Use function with().\n\nfit1 &lt;- with(imp.all[[1]], cda)\nfit2 &lt;- with(imp.all[[2]], cda)\nfit3 &lt;- with(imp.all[[3]], cda)\nfit4 &lt;- with(imp.all[[4]], cda)\nfit5 &lt;- with(imp.all[[5]], cda)\n\nEach fit object contains the five imputed Cox proportional hazards models for the adjustment scenario at hand. For example, the \\(\\delta=-10\\) scenario is contained in fit3.\n\nfit3\n\ncall :\nwith.mids(data = imp.all[[3]], expr = cda)\n\ncall1 :\nmice(data = synthetic_leiden, post = post, maxit = 5, printFlag = FALSE, \n    seed = i)\n\nnmis :\n   sexe lftanam  rrsyst rrdiast     dwa  survda     alb    chol    mmse    woon \n      0       0     121     126       0       0     229     232      85       0 \n\nanalyses :\n[[1]]\nCall:\ncoxph(formula = Surv(survda, dead) ~ C(sbpgp, contr.treatment(6, \n    base = 3)) + strata(sexe, agegp))\n\n                                            coef exp(coef) se(coef)      z\nC(sbpgp, contr.treatment(6, base = 3))1  0.48788   1.62886  0.11376  4.289\nC(sbpgp, contr.treatment(6, base = 3))2  0.27687   1.31899  0.10209  2.712\nC(sbpgp, contr.treatment(6, base = 3))4  0.15158   1.16367  0.11738  1.291\nC(sbpgp, contr.treatment(6, base = 3))5  0.02372   1.02401  0.14105  0.168\nC(sbpgp, contr.treatment(6, base = 3))6 -0.21825   0.80392  0.34220 -0.638\n                                              p\nC(sbpgp, contr.treatment(6, base = 3))1 1.8e-05\nC(sbpgp, contr.treatment(6, base = 3))2 0.00669\nC(sbpgp, contr.treatment(6, base = 3))4 0.19660\nC(sbpgp, contr.treatment(6, base = 3))5 0.86644\nC(sbpgp, contr.treatment(6, base = 3))6 0.52362\n\nLikelihood ratio test=22.93  on 5 df, p=0.000348\nn= 956, number of events= 735 \n\n[[2]]\nCall:\ncoxph(formula = Surv(survda, dead) ~ C(sbpgp, contr.treatment(6, \n    base = 3)) + strata(sexe, agegp))\n\n                                            coef exp(coef) se(coef)      z\nC(sbpgp, contr.treatment(6, base = 3))1  0.39924   1.49070  0.11408  3.500\nC(sbpgp, contr.treatment(6, base = 3))2  0.37326   1.45246  0.10195  3.661\nC(sbpgp, contr.treatment(6, base = 3))4  0.13042   1.13930  0.11917  1.094\nC(sbpgp, contr.treatment(6, base = 3))5  0.05550   1.05707  0.14221  0.390\nC(sbpgp, contr.treatment(6, base = 3))6 -0.07855   0.92446  0.34216 -0.230\n                                               p\nC(sbpgp, contr.treatment(6, base = 3))1 0.000466\nC(sbpgp, contr.treatment(6, base = 3))2 0.000251\nC(sbpgp, contr.treatment(6, base = 3))4 0.273796\nC(sbpgp, contr.treatment(6, base = 3))5 0.696353\nC(sbpgp, contr.treatment(6, base = 3))6 0.818434\n\nLikelihood ratio test=20.93  on 5 df, p=0.0008343\nn= 956, number of events= 735 \n\n[[3]]\nCall:\ncoxph(formula = Surv(survda, dead) ~ C(sbpgp, contr.treatment(6, \n    base = 3)) + strata(sexe, agegp))\n\n                                            coef exp(coef) se(coef)      z\nC(sbpgp, contr.treatment(6, base = 3))1  0.42758   1.53355  0.11396  3.752\nC(sbpgp, contr.treatment(6, base = 3))2  0.29895   1.34844  0.10246  2.918\nC(sbpgp, contr.treatment(6, base = 3))4  0.16385   1.17804  0.11712  1.399\nC(sbpgp, contr.treatment(6, base = 3))5  0.03112   1.03161  0.14253  0.218\nC(sbpgp, contr.treatment(6, base = 3))6 -0.09107   0.91296  0.34192 -0.266\n                                               p\nC(sbpgp, contr.treatment(6, base = 3))1 0.000175\nC(sbpgp, contr.treatment(6, base = 3))2 0.003525\nC(sbpgp, contr.treatment(6, base = 3))4 0.161801\nC(sbpgp, contr.treatment(6, base = 3))5 0.827175\nC(sbpgp, contr.treatment(6, base = 3))6 0.789979\n\nLikelihood ratio test=18.6  on 5 df, p=0.002282\nn= 956, number of events= 735 \n\n[[4]]\nCall:\ncoxph(formula = Surv(survda, dead) ~ C(sbpgp, contr.treatment(6, \n    base = 3)) + strata(sexe, agegp))\n\n                                             coef exp(coef)  se(coef)      z\nC(sbpgp, contr.treatment(6, base = 3))1  0.396511  1.486629  0.114339  3.468\nC(sbpgp, contr.treatment(6, base = 3))2  0.307997  1.360697  0.101600  3.031\nC(sbpgp, contr.treatment(6, base = 3))4  0.153366  1.165752  0.116203  1.320\nC(sbpgp, contr.treatment(6, base = 3))5  0.006772  1.006795  0.142059  0.048\nC(sbpgp, contr.treatment(6, base = 3))6 -0.100275  0.904588  0.341764 -0.293\n                                               p\nC(sbpgp, contr.treatment(6, base = 3))1 0.000525\nC(sbpgp, contr.treatment(6, base = 3))2 0.002434\nC(sbpgp, contr.treatment(6, base = 3))4 0.186897\nC(sbpgp, contr.treatment(6, base = 3))5 0.961979\nC(sbpgp, contr.treatment(6, base = 3))6 0.769212\n\nLikelihood ratio test=17.9  on 5 df, p=0.00307\nn= 956, number of events= 735 \n\n[[5]]\nCall:\ncoxph(formula = Surv(survda, dead) ~ C(sbpgp, contr.treatment(6, \n    base = 3)) + strata(sexe, agegp))\n\n                                            coef exp(coef) se(coef)      z\nC(sbpgp, contr.treatment(6, base = 3))1  0.40865   1.50479  0.11365  3.596\nC(sbpgp, contr.treatment(6, base = 3))2  0.28764   1.33328  0.10079  2.854\nC(sbpgp, contr.treatment(6, base = 3))4  0.14105   1.15148  0.11677  1.208\nC(sbpgp, contr.treatment(6, base = 3))5  0.01149   1.01156  0.14236  0.081\nC(sbpgp, contr.treatment(6, base = 3))6 -0.10340   0.90177  0.34168 -0.303\n                                               p\nC(sbpgp, contr.treatment(6, base = 3))1 0.000323\nC(sbpgp, contr.treatment(6, base = 3))2 0.004320\nC(sbpgp, contr.treatment(6, base = 3))4 0.227072\nC(sbpgp, contr.treatment(6, base = 3))5 0.935670\nC(sbpgp, contr.treatment(6, base = 3))6 0.762181\n\nLikelihood ratio test=17.79  on 5 df, p=0.003218\nn= 956, number of events= 735 \n\n\n\n12. Pool the results for each of the five scenarios.\n\nr1 &lt;- as.vector(t(exp(summary(pool(fit1))[, c(2)])))\nr2 &lt;- as.vector(t(exp(summary(pool(fit2))[, c(2)])))\nr3 &lt;- as.vector(t(exp(summary(pool(fit3))[, c(2)])))\nr4 &lt;- as.vector(t(exp(summary(pool(fit4))[, c(2)])))\nr5 &lt;- as.vector(t(exp(summary(pool(fit5))[, c(2)])))\n\nsummary(pool(fit1))\n\n                                     term     estimate std.error   statistic\n1 C(sbpgp, contr.treatment(6, base = 3))1  0.384561979 0.1210874  3.17590376\n2 C(sbpgp, contr.treatment(6, base = 3))2  0.273964679 0.1137322  2.40885787\n3 C(sbpgp, contr.treatment(6, base = 3))4  0.101681327 0.1146571  0.88682971\n4 C(sbpgp, contr.treatment(6, base = 3))5  0.003223223 0.1372274  0.02348819\n5 C(sbpgp, contr.treatment(6, base = 3))6 -0.117472167 0.3319156 -0.35392175\n         df     p.value\n1 307.39511 0.001645321\n2  78.83454 0.018335658\n3 720.90741 0.375466336\n4 626.37618 0.981268339\n5 568.42823 0.723528764\n\n\nThis code grabs the information from the tabulated pooled results that are produced by summary. In order to make sense about these numbers, and to see what exactly is extracted in the above code, laying out the numbers in a proper table may be useful.\n\npars &lt;- round(t(matrix(c(r1,r2,r3,r4,r5), nrow = 5)),2)\npars &lt;- pars[, c(1, 2, 5)]\ndimnames(pars) &lt;- list(delta, c(\"&lt;125\", \"125-140\", \"&gt;200\"))\npars\n\n    &lt;125 125-140 &gt;200\n0   1.47    1.32 0.89\n-5  1.52    1.36 0.91\n-10 1.53    1.36 0.89\n-15 1.50    1.39 0.91\n-20 1.58    1.37 0.92\n\n\nAll in all, it seems that even big changes to the imputations (e.g. deducting 20 mmHg) has little influence on the results. This suggests that the results are stable relatively to this type of MNAR-mechanism.\n\n13. Perform sensitivity analysis analysis on the mammalsleep dataset by adding and subtracting some amount from the imputed values for sws. Use delta &lt;- c(8, 6, 4, 2, 0, -2, -4, -6, -8) and investigating the influence on the following regression model:\n\nlm(sws ~ log10(bw) + odi, data = mammalsleep)\n\nSensitivity analysis is an important tool for investigating the plausibility of the MAR assumption. We again use the \\(\\delta\\)-adjustment technique described in Van Buuren (2012), p. 185] as an informal, simple and direct method to create imputations under nonignorable models. We do so by simply adding and substracting some amount from the imputations.\n\ndelta &lt;- c(8, 6, 4, 2, 0, -2, -4, -6, -8)\nini &lt;- mice(mammalsleep[, -1], maxit=0, print=F)\nmeth&lt;- ini$meth\nmeth[\"ts\"]&lt;- \"~ I(sws + ps)\"\npred &lt;- ini$pred\npred[c(\"sws\", \"ps\"), \"ts\"] &lt;- 0\npost &lt;- ini$post\nimp.all.undamped &lt;- vector(\"list\", length(delta))\nfor (i in 1:length(delta)) {\n  d &lt;- delta[i]\n  cmd &lt;- paste(\"imp[[j]][, i] &lt;- imp[[j]][, i] +\", d)\n  post[\"sws\"] &lt;- cmd\n  imp &lt;- mice(mammalsleep[, -1], meth=meth, pred=pred, post = post, maxit = 10, seed = i * 22, print=FALSE)\n  imp.all.undamped[[i]] &lt;- imp\n}\noutput &lt;- sapply(imp.all.undamped, function(x) pool(with(x, lm(sws ~ log10(bw) + odi)))$pooled$estimate)\nrownames(output) &lt;- summary(pool(with(imp.all.undamped[[1]], lm(sws ~ log10(bw) + odi))))$term\ncbind(delta, as.data.frame(t(output)))\n\n  delta (Intercept)  log10(bw)        odi\n1     8    13.38503 -0.1349983 -1.1122847\n2     6    12.96965 -0.2546588 -1.1249810\n3     4    12.47611 -0.6279854 -0.9881585\n4     2    12.18507 -0.8777009 -1.0337294\n5     0    11.79013 -0.9135497 -0.9815233\n6    -2    11.16237 -1.4025229 -0.8425622\n7    -4    10.78238 -1.6825684 -0.8408006\n8    -6    10.55405 -1.8748218 -0.8283291\n9    -8    10.10635 -2.1090423 -0.8664012\n\n\nThe estimates for different \\(\\delta\\) are not close. A clear trend for the estimates for the intercept and for bw emerges. Thus, the results are not essentially the same under all specified mechanisms and the outcomes can be deemed sensitive to the assumed mechanism.\nHowever, in this scenario, the \\(\\delta\\) adjustment is completely unrealistic. If we look at the descriptive information for observed sws\n\nsummary(mammalsleep$sws)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  2.100   6.250   8.350   8.673  11.000  17.900      14 \n\n\nwe find that even our smallest adjustment (\\(\\delta=|2|\\)) already makes up almost a quarter of the average sws. Choosing unreasonably large values may always influence your estimates. Therefore; choosing values that are reasonable given your suspicions of an assumed breach of the MAR assumption is vital.\nWe only used a shift parameter here. In other applications, scale or shape parameters could be more natural (see e.g. Van Buuren (2012, Ch. 3.9.1)). The calculations are easily adapted to such cases.\n\nConclusion\nWe have seen that we can create multiple imputations in multivariate missing data problems that imitate deviations from MAR. The analysis used the post argument of the mice() function as a hook to alter the imputations just after they have been created by a univariate imputation function. The diagnostics shows that the trick works. The relative mortality estimates are however robust to this type of alteration.\n\n- End of Vignette\n\n\n\n\n\nVan Buuren, Stef. 2012. Flexible Imputation of Missing Data. CRC press.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`mice`: An approach to sensitivity analysis</span>"
    ]
  },
  {
    "objectID": "8FutureMice/Vignette_futuremice.html",
    "href": "8FutureMice/Vignette_futuremice.html",
    "title": "8  Wrapper function futuremice",
    "section": "",
    "text": "8.0.1 The future starts today\nThis is the eighth vignette in a series of ten.\n\nFor big datasets or high number of imputations, performing multiple imputation with function mice from package mice (Van Buuren and Groothuis-Oudshoorn 2011) might take a long time. As a solution, wrapper function futuremice was created to enable the imputation procedure to be run in parallel. This is done by dividing the imputations over multiple cores (or CPUs), thus potentially speeding up the process. The function futuremice is a sequel to parlMICE (Schouten & Vink, 2017), developed to improve user-friendliness.\nThis vignette demonstrates two applications of the futuremice function. The first application shows the tradeoff between time and increasing number of imputations (\\(m\\)) for a small dataset; the second application does the same, but for a relatively large dataset. We also discuss futuremice’s arguments.\nThe function futuremice depends on packages future, furrr and mice. For more information about running functions in futures, see e.g. the future manual or the furrr manual. Function futuremice found its inspiration from Max’s useful suggestions on parallelization of mice’s chains on stackoverflow.\n\n\n\n8.0.2 Time gain with small datasets\nWe demonstrate the potential gain in computing efficiency on simulated data. To this end we sample 1,000 cases from a multivariate normal distribution with mean vector\n\\[\n\\mu = \\left[\\begin{array}\n{r}\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right]\n\\]\nand covariance matrix\n\\[\n\\Sigma = \\left[\\begin{array}\n{rrrr}\n1&0.5&0.5&0.5 \\\\\n0.5&1&0.5&0.5 \\\\\n0.5&0.5&1&0.5 \\\\\n0.5&0.5&0.5&1\n\\end{array}\\right].\n\\]\nA MCAR missingness mechanism is imposed on the data where 80 percent of the cases (i.e. rows) has missingness on one variable. All variables have missing values. The missingness is randomly generated with the following arguments from function mice::ampute:\n\nset.seed(123)\n\nsmall_covmat &lt;- diag(4)\nsmall_covmat[small_covmat == 0] &lt;- 0.5\nsmall_data &lt;- MASS::mvrnorm(1000, \n                      mu = c(0, 0, 0, 0),\n                      Sigma = small_covmat)\n\nsmall_data_with_missings &lt;- ampute(small_data, prop = 0.8, mech = \"MCAR\")$amp\nhead(small_data_with_missings)\n\n           [,1]       [,2]       [,3]       [,4]\n[1,] -0.1667048  0.9165856  0.6389869         NA\n[2,] -0.4548685  0.4313280         NA  0.5753627\n[3,] -1.2432777 -0.4162831 -1.9552769         NA\n[4,] -0.1366822         NA -0.5998099  0.7553689\n[5,] -1.6633582 -0.7137484  1.8412701  0.1269927\n[6,]         NA -1.3018272 -1.4972105 -1.9058145\n\n\nWe compare the default ‘sequential’ function mice with function futuremice. In both functions we use the defaults arguments for the mice algorithm, although these could very easily be changed if desired by the user. To demonstrate the increased efficiency when putting more than one computing core to work, we repeat the procedure with futuremice for 1, 2, 3 and 4 cores. Figure 1 shows a graphical representation of the results.\n\n\n\n\n\n\n\n\n\n\nFigure 1. Processing time for small datasets. Multiple imputations are performed with mice (conventional) and wrapper function futureMICE (1, 2, 3 and 4 cores, respectively). The dataset has 1000 cases and 4 variables with a correlation of 0.5. 80 percent of the cases has one missing value based on MCAR missingness.\n\nIt becomes apparent that for a small to moderate number of imputations, the conventional mice function is faster than the wrapper function futuremice. This is the case until the number of imputations \\(m = 120\\). For higher \\(m\\), wrapper function futuremice returns the imputations somewhat faster.\n\n\n\n8.0.3 Time gain with large datasets\nWe replicated the above detailed simulation setup with a larger dataset of 10,000 cases and 8 variables. The mean and covariance structure follow the sampling scheme of the smaller data set. We show the results of this simulation in Figure 2.\n\n\n\n           [,1]       [,2]       [,3]       [,4]       [,5]       [,6]\n[1,] -0.7891224  0.6662847  0.2765961  1.4430855         NA -0.2653717\n[2,]  1.2257427  0.7970424  0.7721632  2.5516444  0.8170605  1.0243246\n[3,] -0.2922743 -0.6664302 -1.3484499 -0.9736449  0.1881570  0.1598344\n[4,] -0.7151888  0.2721259  0.6044130         NA  0.2374834 -0.3803037\n[5,] -1.2170593 -1.7371123 -1.1995254 -1.4162931 -0.7217540 -0.9175987\n[6,]  0.4221454  0.9604065 -0.3323604  0.1190414  0.2192011  1.0113553\n            [,7]        [,8]\n[1,]  0.25154854  0.48127292\n[2,]  1.12375199          NA\n[3,]  0.78083640 -0.07665967\n[4,] -0.01697637  1.31384693\n[5,] -1.50140012 -0.78570553\n[6,] -0.70099336  0.64118618\n\n\n\n\n\n\n\n\n\nFigure 2. Processing time for large datasets. Multiple imputations are performed with mice (conventional) and wrapper function parlMICE (1, 2 and 3 cores respectively). The dataset has 10000 cases and 8 variables with a correlation of 0.5. 80 percent of the cases has one missing value based on MCAR missingness.\n\nWhen datasets are sufficiently large, function futuremice works faster than mice for all \\(m\\). In such cases, even for very small numbers of imputations, running mice in parallel with futuremice saves a significant amount of time. This gain in efficiency can increase to more than 50 percent for \\(100\\) imputations and more.\nThere is not a large difference between using 2 and 3 cores with wrapper function parlMICE. For all number of imputations, the procedure runs faster with 3 cores, even though the imputations have to be divided over the cores. It might therefore be desirable to use always as many cores as possible, while leaving 1 core out to govern any overhead computing. For example, on a hexacore machine, use only 5 cores to run the mice algorithm in parallel with futuremice.\n\n\n\n8.0.4 Default settings\nWe will now discuss the arguments of function futuremice. Easy imputation of an incomplete dataset (say, nhanes) can be performed with futuremice in the following way.\n\nimp &lt;- futuremice(nhanes)\nclass(imp)\n\n[1] \"mids\"\n\n\nThe function returns a mids object as created by mice. In fact, futuremice makes use of function mice::ibind to combine the mids objects returned by the different cores. Therefore, the call of the mids object has slightly changed.\n\nimp$call\n\n[[1]]\nmice(data = data, m = x, printFlag = FALSE, seed = seed)\n\n[[2]]\nibind(x = imp, y = imps[[i]])\n\n[[3]]\nibind(x = imp, y = imps[[i]])\n\n[[4]]\nibind(x = imp, y = imps[[i]])\n\n[[5]]\nibind(x = imp, y = imps[[i]])\n\n\nAdditionally, futuremice makes use of a parallelseed argument that is stored in imp$parallelseed.\n\nimp$parallelseed\n\n  [1]       10403         281  1826279530   908648249  1140721631  -260041114\n  [7]  -238418316  -670161387  1368457960    36818080  1517725334   493670394\n [13] -1046015996   910574875  -900894818  -121988577  -770718113  1593432097\n [19]   676959770  -616553340 -1470130872 -1446565417  -132614412  -274380591\n [25]  -778085385 -1805085047  -969091378  2036525925   895742532  1808871536\n [31]   701584976  -700062762  1501221314   110839157  1930854003   805632034\n [37] -1285653794 -1893753524  -875495990  -688146030  1222635737  1577067082\n [43]  1155812169   786165877  1051391471   -22253171  1582301439 -1974613530\n [49]   485505778  1645008053  1117942589  1720954067  1054863571 -1278871277\n [55]  1787447610   533071478 -1383885753  -556300963   253036311 -1290237711\n [61]  -180914663  1891422422 -1937364415  -440203668  -374515262  1931393645\n [67] -2134066338  -353506915   430628155   -54861558  -427910818  -967344978\n [73]  -641912556 -1101407715   -54673539 -1997386449   -45540661 -1289228687\n [79]   838071861  -438508778 -1762967452   283247710 -1867693800 -1220938828\n [85]  -307411418 -1775213738  2093658519  1942064081  -421744133  2057771980\n [91] -1102976285  -745602174   109209347  1909425633  -292100485  -495538772\n [97]   211165409  -928828752 -1893393545 -1435230094   302224558  -435607351\n[103]  -131101172   365439314  1086838107 -1401663424 -1636085260 -1504115601\n[109]  1616140395  1884309273   495201011 -1271482769 -2091178078  -358541243\n[115]   325244620  1580267621 -1998187489  1683432317   287336143  -796323459\n[121] -1921997928  1721185729  1759135401  1531409850    46772677   -11284143\n[127] -1400806398 -1924209435     -153686 -1624252944  1525785776   511649152\n[133] -2104979025  -328222935 -1005761241   330025890 -1411239103  1107722648\n[139]   675707729   183893510    69810816  -784984921  1990831234   567614780\n[145]  -684714589 -1108386374   467574344   977750168  2112532548  1487709490\n[151]  1297036034  -698087576   997985539   464404039 -2111167704  1890588090\n[157]  2057141364  1954491494 -1595410790 -1198331635  1809398830  1963663165\n[163] -1532938292   834217153  1969023887 -2023624635    41161914   911142613\n[169]  1103310603   991993959 -1971219504  -783849161   135001165 -1387403550\n[175]   308903491  -353872878  2013444360  -279926731 -1553463031  -441303005\n[181]  1598263047  -998084496  -568939357   -76269296  -259792902 -1855428781\n[187]  1759261231 -2039002260 -1423859890 -1333084794   428670227   147131114\n[193]  -436573926  -425623247  2064788009   523632913  1533165803  -117448534\n[199]   109554674   616826169 -2103957796  -218961204  -945902714  -472059586\n[205]  -723037255  -686238438 -1263370578 -1652753343 -1191402086 -1000185620\n[211]  -406913730 -1597827498   558080872  1409351951 -1943565905   542738222\n[217] -1183917495 -1432020739  -399177667  -594825952  -332771159 -1209998360\n[223]  1437542142   601335496 -1371862787  1312813626  1179892643   556896881\n[229]   354729712  1800492169  1118885144   -75576979  -616794688  1397981880\n[235]  -116205911   822815511 -1536603803   266368077  -252062349  1763357523\n[241] -1625118442   782743259 -2072578095  -638830115   939615483 -1625125164\n[247]  1679337509 -1949579802 -1784317896   316425688 -2025942008   -55793990\n[253]  -309295799   -36694223  -551160230  1669204210  1610950317  -250635071\n[259]   818104202  -722199716 -1709294804 -1929553852   929011857 -1329081435\n[265] -1316719377  1135984102 -1646453926  1308496588   -29552366   608448546\n[271] -1417886391  1380083943  -434204518  -516997957  1315348291 -2075303777\n[277]  -430529001  2018483825   917735788   159669491  -544087038  1504239156\n[283] -1190381524   479043166    65897497  1870215697 -1246799042  1085916730\n[289]  1823115235  -138067950   189752402 -1445497829  1088486454   315757267\n[295]  1922096700   831956766 -1447193383   767522813  -873861189  -612376720\n[301] -1900094770 -1332810704  1001818168  1494718475   829099608 -2068775573\n[307]  -167672533   624565792 -2022132298  -984570850   -12931190  1394636812\n[313]   -55326250  1001818587  2100011930 -2088115513   455606360  -614886963\n[319]   156329068  1196848811 -1778284257  1342443593  1639885678  1679580870\n[325] -2039638951 -1213176233   737584323    15145250  -175814617   863156081\n[331] -1413268217  1617584476  -832599155  1452008509   -29509425   174412409\n[337]   -56189475  1485295604  -175169096  -356469771   725641911   142901248\n[343]  1748321153  1561635452  1882811264   404110370 -1270692970  1634451532\n[349]    -9178760  1612639477  1489666838 -1561552673   245363371   584758234\n[355]   451449934  1222858380  1693933778  1622228665   780058131  1584418178\n[361]  1615309538 -1971438150  1937163611   511727628  1389889760  1737946546\n[367]  1506325313   791296241  1694500862  1309196659    99511954  -324456661\n[373]  2126547194 -2006167829 -1688049095 -1870620674   461302341   927445193\n[379]  1531545541   861190251 -1608981546 -1533946759   628375338 -1148386980\n[385]    40834545 -2087955629  1970823947 -1329615796   937120686   317926319\n[391]  1372832798  -255727212  1000736865   793764606  1412072737 -2085595522\n[397]  -586227456  1540553555 -1660408140  -993942701   531857878   993590427\n[403]  -970370632    50179066  -150739758  -732140920  1299621367  -578392151\n[409]  1775000735   219158231  1453567697   748203987   884687381  -827633540\n[415]  1566351514 -1179445534   -90606029  -931945183  1695309094  -402413267\n[421]  1880158628 -1067613423  -402355528 -1538210900  1343029775   -52208534\n[427]  2070852756  1661050232   475595171   118539809   505109426   818100221\n[433] -1211962824 -1242073961  1948329808 -2025196266  1953068224  2046167055\n[439]  2095629955   447156300  1979439655  1491399536  2089433987  -554706557\n[445]   209176772   508808789   610834540   927655264  -154179500 -1872808024\n[451]  -638926704    22499991  -931063259  1636279665  2098580108   210244558\n[457]  2057420414   -80964611  -987662268    -4046107  1361763662  -987417249\n[463]  1847239746    -9459071 -1687420884  2007744977   450163523   320675674\n[469]  1273871875  1665884246  -242053373 -1484436231  1830373435  1792844109\n[475]   236062234  -972238285  1956168168  1950382085 -1043247556  -202997016\n[481]  1797572067  2124605113  1273078164  -940992810   -25241904  1986762505\n[487]  1547085229   960214979  -777917759   -42458394   -89858980  -213754766\n[493]  -229788778  -232604505 -2025982183  -660686212  1842709363   741565969\n[499]   958315958  1564218094    37990238   572281167 -1368221983   119490303\n[505] -1854296764 -1925207403  1353194304 -1616572581  1462801647  -590232147\n[511]   945259157  1918465969  1987555248 -1562604698  1516727249   168508301\n[517]  -114420370    27856226  1442284289  1564468416   849511155     6602604\n[523]   476395150  1268671438  -815601167  -426459442  1407418329  1163119494\n[529]  -674408417  -587890838 -1492596876  1696669262   579750152  1066387763\n[535]  1109784416   598076172 -1255051777  1455273521   124378295 -1525774165\n[541]   796201745   643976450  1139803102   362419533  1978075378  1252517828\n[547]   -32441466 -1431187988  1494956513  -360756004  2044804659  1995679606\n[553]  -536635963  1627753001  -736041185  -303757793  1384528899   398637101\n[559]   202791041   948067079 -1780261345  1044757775  -876884095  1548648081\n[565]  1597259326 -1523639687  -289073412  -815427273 -1543950960  1998690723\n[571]  1519425744  -950312124 -1016614987   587488002   505705088 -2130489418\n[577] -1928263486 -1341066775  1747615296   575352457   624767442  1772551357\n[583] -1090911944   407237273  1784976830 -1310917877   522708735  1868387969\n[589]    96604843   454596834  1323212855  -871416606   469166058 -1185774481\n[595]  1016443457  1102892260  -543768582 -1114984384  1882839954 -1243975291\n[601]   707250831   467295297 -1144670459  1207887677  1123511782 -2012791518\n[607]   -45889929  1382249588  1427909629 -1520385285 -2130714530  -958704059\n[613]  1086985376  1754630543   536386164  1012748109  -758590104  1656501370\n[619]   609683540   187882649   387960510  1003551871  1010792375  -170541339\n[625]  1519158362  -345684351\n\n\nIf no seed is specified by the user, a seed will be drawn randomly from a uniform distribution \\(U(-999999999,999999999)\\), and this seed will be returned, such that the user can reproduce the obtained results even when no seed is specified. See section Argument parallelseed for more information.\nAll other parts of the mids object are standard.\n\n\n\n8.0.5 Using mice arguments\nFunction futuremice is able to deal with the conventional mice arguments. In order to change the imputation method from its default (predictive mean matching) to, for example, Bayesian linear regression, the method argument can be adjusted. For other possibilities with mice, we refer to the mice manual.\n\nimp &lt;- futuremice(nhanes, method = \"norm\")\nimp$method\n\n   age    bmi    hyp    chl \n    \"\" \"norm\" \"norm\" \"norm\" \n\n\nIn mice, the number of imputations is specified with argument m. In futuremice, the same argument should be used, and futuremice takes care of dividing the imputations equally over the cores. The next section discusses these arguments.\n\n\n\n8.0.6 Argument n.core\nWith n.core, the number of cores (or CPUs) is given, and the number of imputations m is (about) equally distributed over the cores.\nAs a default, n.core is specified as the number of available, logical cores minus 1. The default number of imputations has been set to m = 5, just as in a regular mice call. Hence, on machines with 4 available, logical cores, \\(5\\) imputations are divided over 3 cores, leaving 1 core available for any overhead computations. This results in a number of imputations per core of: \\(core1, core1, core2, core3, core3\\), respectively.\nThe computer with which this vignette is run, has\n\nparallelly::availableCores(logical = TRUE)\n\nsystem \n     8 \n\n\navailable, logical cores. Accordingly, the number of imputations per core equals core1, core1, core2, core3, core3 We can check this by evaluating the \\(m\\) that is shown in the mids object.\n\nimp$m\n\n\n\n\n8.0.7 Argument parallelseed\nIn simulation studies, it is often desired to set a seed to make the results reproducible. Similarly to mice, the seed value for futuremice can be defined outside the function. Under the hood, futuremice makes use of the furrr::furrr_options(seed = TRUE) argument, which recognizes that a seed has been specified in the global environment. Hence users can specify the following code to obtain identical results.\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nset.seed(123)\nimp1 &lt;- futuremice(nhanes, n.core = 3)\nset.seed(123)\nimp2 &lt;- futuremice(nhanes, n.core = 3)\n\nimp1 %$% lm(chl ~ bmi) %&gt;% pool %$% pooled\n\n         term m   estimate       ubar          b          t dfcom       df\n1 (Intercept) 5 129.350091 3029.97733 798.544710 3988.23098    23 13.08385\n2         bmi 5   2.439893    4.20187   1.086733    5.50595    23 13.20235\n        riv    lambda       fmi\n1 0.3162577 0.2402703 0.3347415\n2 0.3103571 0.2368492 0.3310517\n\nimp2 %$% lm(chl ~ bmi) %&gt;% pool %$% pooled\n\n         term m   estimate       ubar          b          t dfcom       df\n1 (Intercept) 5 129.350091 3029.97733 798.544710 3988.23098    23 13.08385\n2         bmi 5   2.439893    4.20187   1.086733    5.50595    23 13.20235\n        riv    lambda       fmi\n1 0.3162577 0.2402703 0.3347415\n2 0.3103571 0.2368492 0.3310517\n\n\nA user can also specify a seed within the futuremice call, by specifying the argument parallelseed. This seed is parsed to withr::local_seed(), such that the global environment is not affected by a different seed within the futuremice function. Hence, users can also specify a seed as follows.\n\nimp3 &lt;- futuremice(nhanes, parallelseed = 123, n.core = 3)\nimp4 &lt;- futuremice(nhanes, parallelseed = 123, n.core = 3)\n\nimp3 %$% lm(chl ~ bmi) %&gt;% pool %$% pooled\n\n         term m   estimate       ubar          b          t dfcom       df\n1 (Intercept) 5 129.350091 3029.97733 798.544710 3988.23098    23 13.08385\n2         bmi 5   2.439893    4.20187   1.086733    5.50595    23 13.20235\n        riv    lambda       fmi\n1 0.3162577 0.2402703 0.3347415\n2 0.3103571 0.2368492 0.3310517\n\nimp4 %$% lm(chl ~ bmi) %&gt;% pool %$% pooled\n\n         term m   estimate       ubar          b          t dfcom       df\n1 (Intercept) 5 129.350091 3029.97733 798.544710 3988.23098    23 13.08385\n2         bmi 5   2.439893    4.20187   1.086733    5.50595    23 13.20235\n        riv    lambda       fmi\n1 0.3162577 0.2402703 0.3347415\n2 0.3103571 0.2368492 0.3310517\n\n\nThis also yields identical results for imp3 and imp4. However, note that this does not result in the exact same imputations as the procedure where the seed is specified in the global environment.\nIf no seed is specified in the global environment, or in the call itself, the results are still reproducible, because in such circumstances, futuremice randomly draws a seed from a uniform distribution \\(U(-999999999,999999999)\\). This randomly drawn seed is stored under $parallelseed in the output object, such that reproducible results can be obtained as follows.\n\nimp5 &lt;- futuremice(nhanes, n.core = 3)\nparallelseed &lt;- imp5$parallelseed[1]\nimp6 &lt;- futuremice(nhanes, parallelseed = parallelseed, n.core = 3)\n\nimp5 %$% lm(chl ~ bmi) %&gt;% pool %$% pooled\n\n         term m   estimate        ubar           b           t dfcom       df\n1 (Intercept) 5 114.773367 3003.784878 679.3464599 3819.000630    23 14.02992\n2         bmi 5   3.047209    4.215382   0.6232608    4.963295    23 16.35719\n        riv    lambda       fmi\n1 0.2713962 0.2134631 0.3058343\n2 0.1774247 0.1506888 0.2384403\n\nimp6 %$% lm(chl ~ bmi) %&gt;% pool %$% pooled\n\n         term m  estimate        ubar           b           t dfcom       df\n1 (Intercept) 5 123.52773 3273.760850 1170.967485 4678.921832    23 11.12769\n2         bmi 5   2.68347    4.548798    1.763287    6.664742    23 10.61455\n        riv    lambda       fmi\n1 0.4292192 0.3003173 0.3993685\n2 0.4651656 0.3174833 0.4177461\n\n\nWARNING: Under unique circumstances, users might want to check whether imputations obtained under different streams are identical. This can be done by specifying the regular seed argument in the futuremice call. This seed is parsed to the separate mice calls within all futures, such that the results will be identical over the cores. If users specify the seed argument rather than the parallelseed argument, futuremice will ask if this is intended behavior if the user is in an interactive() session. Otherwise, a warning will be printed.\n\n\n\n8.0.8 Systems other than Windows\nFunction futuremice calls for function future_map with plan(\"multisession\") from the furrr package. Although other options are available, we have chosen for the plan(\"multisession\") because it allows for the use of multiple cores on all computers, including a Windows computer. The user may adjust this by specifying the future.plan argument within futuremice. Other options are for example future.plan = \"multicore\", which results in plan(\"multicore\") (which is not supported on Windows computers), future.plan = \"cluster\", resulting in plan(\"cluster\"). For all options regarding plan(), check ?future::plan().\n\n\n\n8.0.9 References\nManual R-package future, available at https://cran.r-project.org/web/packages/future/future.pdf\nManual R-package furrr, available at https://cran.r-project.org/web/packages/furrr/furrr.pdf\nManual package MICE, available at https://cran.r-project.org/web/packages/mice/mice.pdf\nSchouten, R.M., Lugtig, P.J. and Vink, G. (2016). Multiple amputation using ampute [manual]. Available at https://github.com/RianneSchouten/mice/blob/ampute/vignettes/Vignette_Ampute.pdf\nSchouten, R.M. and Vink, G. (2017). parlmice: faster, paraleller, micer. https://www.gerkovink.com/parlMICE/Vignette_parlMICE.html\n\nEnd of Vignette\n\n\n\n\nVan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software 45: 1–67.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Wrapper function `futuremice`</span>"
    ]
  },
  {
    "objectID": "9Mpmm/mpmm_vignette.html",
    "href": "9Mpmm/mpmm_vignette.html",
    "title": "10  Multivariate predictive mean matching",
    "section": "",
    "text": "This is the ninth vignette in a series of ten.\nMultivariate predictive mean matching is a generlised form of univariate predictive mean, which can impute incomplete variables simultaneously.\n\n11 Packages used\nThe following packages are used.\n\nlibrary(devtools)\ninstall_github(\"amices/mice\")\nlibrary(mice)  \n\n\n\n\n12 Data generation\n\nset.seed(123)\nB1 &lt;- .5\nB2 &lt;- .5\nX &lt;- rnorm(1000)\nXX &lt;- X^2\ne &lt;- rnorm(1000, 0, 1)\nY &lt;- B1 * X + B2 * XX + e\ndat &lt;- data.frame(x = X, xx = XX, y = Y)\n# Impose 25 percent MCAR Missingness\ndat[0 == rbinom(1000, 1, 1 - .25), 1:2] &lt;- NA\n\n\n\n\n13 Imputation\n\n# Prepare data for imputation\nblk &lt;- list(c(\"x\", \"xx\"), \"y\")\nmeth &lt;- c(\"mpmm\", \"\")\n# Impute data\nimp &lt;- mice(dat, blocks = blk, method = meth, print = FALSE)\n\n\n\n\n14 Plot results\n\n# Pool result\npool(with(imp, lm(y ~ x + xx)))\n\nClass: mipo    m = 5 \n         term m   estimate         ubar            b            t dfcom\n1 (Intercept) 5 0.05578371 0.0016216338 1.661769e-05 0.0016415750   997\n2           x 5 0.59094081 0.0011099659 3.690944e-04 0.0015528792   997\n3          xx 5 0.49449325 0.0006135508 7.122431e-05 0.0006990199   997\n         df       riv     lambda        fmi\n1 948.52459 0.0122970 0.01214762 0.01422397\n2  45.99019 0.3990332 0.28522070 0.31440121\n3 204.81271 0.1393025 0.12227001 0.13071733\n\nplot(dat$x, dat$xx, col = mdc(1), xlab = \"x\", ylab = \"xx\")\ncmp &lt;- complete(imp)\npoints(cmp$x[is.na(dat$x)], cmp$xx[is.na(dat$x)], col = mdc(2))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multivariate predictive mean matching</span>"
    ]
  },
  {
    "objectID": "10Synthetic/synthetic.html",
    "href": "10Synthetic/synthetic.html",
    "title": "10  Synthetic data in R: Generating synthetic data with high utility using mice",
    "section": "",
    "text": "11 Introduction\nIn this workshop, you will learn how to create and evaluate synthetic data in R. In the practical, we will work with the R package mice (van Buuren and Groothuis-Oudshoorn 2011). mice was originally developed to impute missing data, but, as you will experience, can also be used to impute synthetic data (see Volker and Vink 2021). Other alternatives to create synthetic data are, for example, the R-package synthpop (Nowok, Raab, and Dibben 2016), or the stand-alone software IVEware (“IVEware: Imputation and Variance Estimation Software,” n.d.).\nIn this workshop, you will (at least) use the packages mice (van Buuren and Groothuis-Oudshoorn 2011), ggmice (make sure to download the latest version from GitHub, Oberman 2022), ggplot2 (Wickham 2016), patchwork (Pedersen 2022), psych (Revelle 2022), purrr (Henry and Wickham 2022) and synthpop (Nowok, Raab, and Dibben 2016). Make sure to load them (in case you haven’t installed them already, install them first, using install.packages(\"package.name\")).\nremotes::install_github(\"amices/mice\")\nremotes::install_github(\"amices/ggmice\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"patchwork\")\ninstall.packages(\"psych\")\ninstall.packages(\"purrr\")\ninstall.packages(\"synthpop\")\nlibrary(mice)      # to create the synthetic data\nlibrary(ggmice)    # to make visualizations of the synthetic data\nlibrary(ggplot2)   # required when using ggmice\nlibrary(patchwork) # to stitch multiple figures together\nlibrary(psych)     # to obtain descriptive statistics\nlibrary(purrr)     # to work with multiply imputed synthetic datasets\nlibrary(synthpop)  # to assess the utility of our synthetic data\nAdditionally, make sure to set a seed, so that your results can be compared with our results.\nset.seed(1)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Synthetic data in `R`: Generating synthetic data with high utility using `mice`</span>"
    ]
  },
  {
    "objectID": "10Synthetic/synthetic.html#univariate-data-utility",
    "href": "10Synthetic/synthetic.html#univariate-data-utility",
    "title": "10  Synthetic data in R: Generating synthetic data with high utility using mice",
    "section": "14.1 Univariate data utility",
    "text": "14.1 Univariate data utility\n4. To get an idea of whether creating the synthetic data went accordingly, compare the 10 rows of the fourth synthetic data set with the first 10 rows of the original data.\nHint: You can use complete(syn_param, 4) to extract the fourth synthetic data set from the syn_param object.\n\ncomplete(syn_param, 4) |&gt; \n  head(10)\nheart_failure |&gt; \n  head(10)\n\n\n\n\n\n\n\nage\nanaemia\ncreatinine_phosphokinase\ndiabetes\nejection_fraction\nplatelets\nserum_creatinine\nserum_sodium\nsex\nsmoking\nhypertension\ndeceased\nfollow_up\n\n\n\n\n48.28538\nNo\n544.3140\nNo\n43.49309\n168606.67\n2.0029503\n134.1159\nFemale\nYes\nYes\nYes\n97.19049\n\n\n46.93784\nNo\n2242.8349\nYes\n17.44978\n53449.81\n1.6506376\n146.7847\nMale\nYes\nYes\nYes\n246.37033\n\n\n57.77604\nYes\n185.1961\nNo\n29.14009\n374855.04\n2.5081864\n128.1695\nMale\nNo\nNo\nYes\n143.69784\n\n\n80.37375\nYes\n659.5714\nNo\n60.69236\n314479.31\n0.9757324\n137.1460\nMale\nNo\nYes\nYes\n81.84389\n\n\n47.87052\nYes\n591.7346\nNo\n18.68507\n208443.43\n2.3928295\n129.7961\nMale\nNo\nNo\nYes\n36.84320\n\n\n67.23864\nYes\n1598.3000\nNo\n59.29677\n91290.12\n1.1576833\n138.0340\nMale\nNo\nYes\nYes\n37.53685\n\n\n70.53800\nNo\n1717.0082\nNo\n51.73267\n196696.78\n1.7746080\n134.2336\nMale\nYes\nNo\nYes\n133.19167\n\n\n72.59105\nNo\n1300.4336\nYes\n31.67865\n308340.49\n1.4321558\n138.1010\nMale\nNo\nNo\nNo\n40.17883\n\n\n64.17476\nYes\n2498.2933\nNo\n29.95634\n279412.00\n0.6372197\n138.3070\nFemale\nNo\nYes\nYes\n58.33464\n\n\n85.05320\nYes\n1020.7732\nNo\n31.92641\n185703.41\n0.9960159\n128.7269\nMale\nYes\nNo\nYes\n-109.81722\n\n\n\n\n\n\n\n\n\n\n\nage\nanaemia\ncreatinine_phosphokinase\ndiabetes\nejection_fraction\nplatelets\nserum_creatinine\nserum_sodium\nsex\nsmoking\nhypertension\ndeceased\nfollow_up\n\n\n\n\n75\nNo\n582\nNo\n20\n265000\n1.9\n130\nMale\nNo\nYes\nYes\n4\n\n\n55\nNo\n7861\nNo\n38\n263358\n1.1\n136\nMale\nNo\nNo\nYes\n6\n\n\n65\nNo\n146\nNo\n20\n162000\n1.3\n129\nMale\nYes\nNo\nYes\n7\n\n\n50\nYes\n111\nNo\n20\n210000\n1.9\n137\nMale\nNo\nNo\nYes\n7\n\n\n65\nYes\n160\nYes\n20\n327000\n2.7\n116\nFemale\nNo\nNo\nYes\n8\n\n\n90\nYes\n47\nNo\n40\n204000\n2.1\n132\nMale\nYes\nYes\nYes\n8\n\n\n75\nYes\n246\nNo\n15\n127000\n1.2\n137\nMale\nNo\nNo\nYes\n10\n\n\n60\nYes\n315\nYes\n60\n454000\n1.1\n131\nMale\nYes\nNo\nYes\n10\n\n\n65\nNo\n157\nNo\n65\n263358\n1.5\n138\nFemale\nNo\nNo\nYes\n10\n\n\n80\nYes\n123\nNo\n35\n388000\n9.4\n133\nMale\nYes\nYes\nYes\n10\n\n\n\n\n\n\n\nThe first thing we can notice, is that the continuous variables are not rounded, as in the original data, which is logical, because we draw these values from a normal distribution. Apart from that, there are negative values in the synthetic version of the variable creatinine_phosphokinase, while the original data is strictly positive. We will come to these issues at a later moment.\nApart from inspecting the data itself, we can assess distributional similarity between the observed and synthetic data. For simplicity, we will first focus on the sixth synthetic data set.\n5. Compare the descriptive statistics from the sixth synthetic data set with the descriptive statistics from the observed data.\nHint: Use the function describe() from the psych package to do this.\n\ncomplete(syn_param, 6) |&gt;\n  describe()\n\nheart_failure |&gt;\n  describe()\n\nThe descriptive statistics are not exactly similar, but come rather close in terms of mean and standard deviation. When looking at higher-order moments and the minimum and maximum, we see that there are some noticeable differences. We pay more attention to these issues when we visually inspect the synthetic data.\n\n6. Create a bar plot using geom_bar() for each categorical variable in the data, mapping these variables to the x-axis with one bar per category per imputed data set.\nHint 1: Within ggmice, set mapping = aes(x = VARIABLE, group = .imp), and within geom_bar(), set mapping = aes(y = ..prop..) and position = position_dodge() to make sure the bars are comparable.\nHint 2: You can map over all categorical variables by creating a vector with the column names of all categorical variables, and using purrr::map() in combination with aes_string() and patchwork::wrap_plots().\n\ncolnames(heart_failure)[map_lgl(heart_failure, is.factor)] %&gt;%\n  map(~ ggmice(syn_param, mapping = aes_string(.x, group = '.imp')) +\n        geom_bar(mapping = aes(y = ..prop..),\n                 position = position_dodge2(),\n                 fill = \"transparent\",\n                 show.legend = FALSE)) %&gt;% \n  patchwork::wrap_plots()\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(prop)` instead.\nℹ The deprecated feature was likely used in the ggmice package.\n  Please report the issue at &lt;https://github.com/amices/ggmice&gt;.\n\n\n\n\n\n\n\n\n\nFor the categorical variables, we seem to be doing a good job in recreating the data on a univariate level.\n\nNow we do the same for the continuous variables, but rather than creating a bar chart, we create a density plot.\n\n7. Create a density plot for each continuous variable with ggmice(), mapping these variables to the x-axis, using the function geom_density(), and make sure that each imputed set obtains its own density.\nHint: The code ggmice(syn, mapping = aes(x = VARIABLE, group = .imp)) creates a ggmice object per imputed set.\n\ncolnames(heart_failure)[map_lgl(heart_failure, is.numeric)] %&gt;%\n  map(~ ggmice(data = syn_param, \n               mapping = aes_string(x = .x, group = '.imp')) +\n        geom_density(show.legend = F)) %&gt;%\n  wrap_plots(ncol = 2)\n\n\n\n\n\n\n\n\nAgain, we see what we observed previously as well. For some of the continuous variables, we do a poor job in recreating a univariate distribution that is similar to the distribution of the observed variables. This gives a clear indication that something is wrong with our synthesis model.\nOf course, this could have been expected, since some of the variables are highly skewed, while we impose a normal distribution on each variable. It is quite likely that we could have done a better job by using more elaborate data manipulation (e.g., transforming variables such that there distribution corresponds more closely to a normal distribution (and back-transforming afterwards)).\nFor now, we will try a different approach, namely a non-parametric one.\n\n8. Use mice() to create m = 10 synthetic data sets in an object called syn_cart, using the previously specified where-matrix, but now use \"cart\" as the imputation method.\n\nsyn_cart &lt;- mice(heart_failure, \n                 m = 10, \n                 maxit = 1,\n                 method = \"cart\",\n                 where = where,\n                 printFlag = FALSE)\n\n\nFor now, we will skip looking at the synthetic data, and continue directly to a the visual inspection of the newly created synthetic data, using the previous visualizations as before.\n\n9. Create a bar plot using geom_bar() for each categorical variable in the data, mapping these variables to the x-axis with one bar per category per imputed data set.\n\ncolnames(heart_failure)[map_lgl(heart_failure, is.factor)] %&gt;%\n  map(~ ggmice(syn_cart, mapping = aes_string(.x, group = '.imp')) +\n        geom_bar(mapping = aes(y = ..prop..),\n                 position = position_dodge2(),\n                 fill = \"transparent\",\n                 show.legend = FALSE)) %&gt;% \n  patchwork::wrap_plots()\n\n\n\n\n\n\n\n\nFor the categorical variables, we again seem to be doing fine: all proportions are comparable across observed and synthetic data.\n\nAgain, we do the same for the continuous variables, using a density plot.\n\n10. Create a density plot for each continuous variable with ggmice(), mapping these variables to the x-axis, using the function geom_density(), and make sure that each imputed set obtains its own density. Compare these plots to the previous figures, what do you notice?\n\ncolnames(heart_failure)[map_lgl(heart_failure, is.numeric)] %&gt;%\n  map(~ ggmice(data = syn_cart, \n               mapping = aes_string(x = .x, group = '.imp')) +\n        geom_density(show.legend = F)) %&gt;%\n  wrap_plots(ncol = 2)\n\n\n\n\n\n\n\n\nWe do a much better job than we did before. The synthetic data seems to closely follow the distribution of the observed data, and all irregularities in the observed data are pretty much recreated in the synthetic data, which is what we hope to see.\n\nThere are also other, more formal ways to assess the utility of the synthetic data, although there is generally some critique against these methods (see, e.g., Drechsler 2022). Here, we will discuss the most formal utility measure, the \\(pMSE\\), but there are others (although all utility measures tend to correlate strongly). The \\(pMSE\\) is defined as \\[\npMSE = \\frac{1}{m}\\sum^m_{j=1} \\frac{1}{n_{obs} + n_{syn}}\n\\Bigg(\n\\sum^{n_{obs}}_{i=1} \\Big(\\hat{\\pi}_i - \\frac{n_{obs}}{n_{obs} + n_{syn}}\\Big)^2 +\n\\sum^{n_{obs} + n_{syn}}_{i={(n_{obs} + 1)}} \\Big(\\hat{\\pi_i} - \\frac{n_{syn}}{n_{obs} + n_{syn}}\\Big)^2\n\\Bigg),\n\\] which, in our case, simplifies to \\[\npMSE = \\frac{1}{10} \\sum^{10}_{j=1} \\frac{1}{598}\n\\Bigg(\n\\sum^{n_{obs} + n_{syn}}_{i=1} \\Big(\\hat{\\pi}_i - \\frac{1}{2}\\Big)^2\n\\Bigg),\n\\] where \\(n_{obs}\\) and \\(n_{syn}\\) are the sample sizes of the observed and synthetic data, \\(\\hat{\\pi}_i\\) is the probability of belonging to the synthetic data. Note that to calculate the this measure, each synthetic data set is stacked below the observed data, resulting in \\(m=10\\) sets of observed and synthetic data.\n\n11. Calculate the \\(pMSE\\) for the variable creatinine_phosphokinase over all ten synthetic data sets, for both synthetic sets and compare the values between both synthesis methods.\nHint: First, create a list with all \\(m=10\\) synthetic data sets, then calculate the predicted probabilities for each data set, take the mean over these predicted probabilities, and subsequently, take the mean over the synthetic sets.\n\nparam_dats &lt;- complete(syn_param, \"all\", include = F)\n\npi_param &lt;- param_dats |&gt;\n  map(~ dplyr::bind_rows(`0` = heart_failure,\n                         `1` = .x, \n                         .id = \"Synthetic\") |&gt;\n        dplyr::mutate(Synthetic = as.factor(Synthetic)) |&gt;\n        glm(formula = Synthetic ~ creatinine_phosphokinase, family = binomial) |&gt;\n        predict(type = \"response\"))\n\ncart_dats &lt;- complete(syn_cart, \"all\", include = F)\n\npi_cart &lt;- cart_dats |&gt;\n  map(~ dplyr::bind_rows(`0` = heart_failure,\n                         `1` = .x, \n                         .id = \"Synthetic\") |&gt;\n        dplyr::mutate(Synthetic = as.factor(Synthetic)) |&gt;\n        glm(formula = Synthetic ~ creatinine_phosphokinase, family = binomial) |&gt;\n        predict(type = \"response\"))\n\nmap_dbl(pi_param, ~mean((.x - 0.5)^2)) |&gt; \n  mean()\n\n[1] 0.0002518855\n\nmap_dbl(pi_cart, ~mean((.x - 0.5)^2)) |&gt;\n  mean()\n\n[1] 0.0001196812\n\n\nIf you don’t want to perform these calculations by hand, there is functionality in the R-package synthpop to calculate the \\(pMSE\\) for you.\n\ncomplete(syn_param, \"all\", include = FALSE) |&gt;\n  utility.gen.list(heart_failure, \n                   vars = \"creatinine_phosphokinase\",\n                   maxorder = 0, \n                   method = \"logit\")\n\nFitting syntheses: 1 2 3 4 5 6 7 8 9 10 \n\n\n\nUtility score calculated by method: logit\n\nCall:\nutility.gen.list(object = complete(syn_param, \"all\", include = FALSE), \n    data = heart_failure, method = \"logit\", maxorder = 0, vars = \"creatinine_phosphokinase\")\n\nMean utility results from 10 syntheses:\n    pMSE   S_pMSE \n0.000252 1.205020 \n\ncomplete(syn_cart, \"all\", include = FALSE) |&gt;\n  utility.gen.list(heart_failure, \n                   vars = \"creatinine_phosphokinase\", \n                   maxorder = 0, \n                   method = \"logit\")\n\nFitting syntheses: 1 2 3 4 5 6 7 8 9 10 \n\n\n\nUtility score calculated by method: logit\n\nCall:\nutility.gen.list(object = complete(syn_cart, \"all\", include = FALSE), \n    data = heart_failure, method = \"logit\", maxorder = 0, vars = \"creatinine_phosphokinase\")\n\nMean utility results from 10 syntheses:\n    pMSE   S_pMSE \n0.000120 0.572555 \n\n\nIt becomes immediately obvious that the \\(pMSE\\) is higher for the parametrically synthesized data sets, but it is hard to interpret these numbers. To get a more insightful measure, we can take ratio of the calculated \\(pMSE\\) over the expected \\(pMSE\\) under the null distribution of a correct synthesis model (i.e., in line with the data-generating model). The \\(pMSE\\)-ratio is given by \\[\n\\begin{aligned}\npMSE \\text{ ratio } &=\n\\frac{pMSE}\n{(k-1)(\\frac{n_{\\text{obs}}}{n_{\\text{syn}} + n_{\\text{obs}}})^2(\\frac{n_{\\text{syn}}}{n_{\\text{syn}} + n_{\\text{obs}}}) / (n_{\\text{obs}} + n_{\\text{syn}})} \\\\ &=\n\\frac{pMSE}{(k-1)(\\frac{1}{2})^3/(n_{obs} + n_{syn})},\n\\end{aligned}\n\\] where \\(k\\) denotes the number of predictors in the propensity score model, including the intercept.\nIn our case, we get\n\npMSE_param &lt;- map_dbl(pi_param, ~mean((.x - 0.5)^2)) |&gt; mean()\n\npMSE_param / ((2-1)*(1/2)^3/(2*nrow(heart_failure)))\n\n[1] 1.20502\n\npMSE_cart &lt;- map_dbl(pi_cart, ~mean((.x - 0.5)^2)) |&gt; mean()\n\npMSE_cart / ((2-1)*(1/2)^3/(2*nrow(heart_failure)))\n\n[1] 0.5725547\n\n\nIdeally, the \\(pMSE\\) ratio equals \\(1\\), but according to the synthpop authors, values below \\(10\\) are indicative of high quality synthetic data. This would indicate that both synthesis models are good models to synthesize the variable creatinine_phosphokinase. Yet, I would make some reservations with respect to the quality of the parametric synthesis model in this case.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Synthetic data in `R`: Generating synthetic data with high utility using `mice`</span>"
    ]
  },
  {
    "objectID": "10Synthetic/synthetic.html#multivariate-data-utility",
    "href": "10Synthetic/synthetic.html#multivariate-data-utility",
    "title": "10  Synthetic data in R: Generating synthetic data with high utility using mice",
    "section": "14.2 Multivariate data utility",
    "text": "14.2 Multivariate data utility\nBeing able to reproduce the original distributions is a good first step, but generally the goal of synthetic data reaches beyond that. Specifically, we generally also want to reproduce the relationships between the variables in the data. The problem here is that visualizations are often most insightful to assess the quality of synthetic data, but this is quite cumbersome for multivariate relationships. Creating visualizations beyond bivariate relationships is often not feasible, whereas displaying all bivariate relationships in the data already results in \\(p(p-1)/2\\) different figures.\nIn the synthetic data literature, a distinction is often made between general and specific utility measures. General utility measures assess to what extent the relationships between combinations of variables (and potential interactions between them) are preserved in the synthetic data set. These measures are often for pairs of variables, or for all combinations of variables. Specific utility measures focus, as the name already suggests, on a specific analysis. This analysis is performed on the observed data and the synthetic data, and the similarity between inferences on these data sets is quantified.\n\n14.2.1 General utility measures\nContinuing with our \\(pMSE\\) approach, we can inspect which interactions of variables can predict whether observations are “true” or “synthetic” using the standardized pMSE measure, similarly to what we just did using individual variables. Hence, we predict whether observations can be classified based on the interaction of two variables.\nUsing the functionality of synthpop, we can assess the utility of all bivariate relationships by calculating the \\(pMSE\\)-ratio for each pair of variables (including their interaction).\n12. Use the function utility.gen.list() from the synthpop package to calculate the \\(pMSE\\)-ratio for each pair of variables for both synthetic sets. What do you see?\n\nutility.gen.list(param_dats, heart_failure)\n\nRunning 50 permutations to get NULL utilities and printing every 10th.\nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \n\n\n\nUtility score calculated by method: cart\n\nCall:\nutility.gen.list(object = param_dats, data = heart_failure)\n\nNull utilities simulated from a permutation test with 50 replications.\n\n\nMean utility results from 10 syntheses:\n    pMSE   S_pMSE \n0.182293 3.268138 \n\nutility.gen.list(cart_dats, heart_failure)\n\nRunning 50 permutations to get NULL utilities and printing every 10th.\nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \nsynthesis 10 20 30 40 50 \n\n\n\nUtility score calculated by method: cart\n\nCall:\nutility.gen.list(object = cart_dats, data = heart_failure)\n\nNull utilities simulated from a permutation test with 50 replications.\n\n\nMean utility results from 10 syntheses:\n    pMSE   S_pMSE \n0.091402 1.779868 \n\n\nThe CART model was somewhat better, but the difference is relatively small. To get more insight into which variables and bivariate relationships were synthesized accordingly, and which can be improved, we can use utility.tables.list().\n13. Use the function utility.tables.list() from the synthpop package to calculate the \\(pMSE\\)-ratio for each pair of variables for both synthetic sets. What do you see?\n\nutility.tables.list(param_dats, heart_failure,\n                    min.scale = 0, max.scale = 40)\n\n\nTwo-way utility: S_pMSE value plotted for 78 pairs of variables.\n\nVariable combinations with worst 5 utility scores (S_pMSE):\n     02.anaemia:03.creatinine_phosphokinase \n                                    37.9215 \n     03.creatinine_phosphokinase:10.smoking \n                                    37.2384 \n03.creatinine_phosphokinase:11.hypertension \n                                    37.2016 \n         03.creatinine_phosphokinase:09.sex \n                                    37.1995 \n    03.creatinine_phosphokinase:12.deceased \n                                    37.0353 \n\n\n\n\n\n\n\n\n\n\nMedians and maxima of selected utility measures for all tables compared\n       Medians  Maxima\npMSE    0.0162  0.1115\nS_pMSE  5.1485 37.9215\ndf      9.0000 24.0000\n\nFor more details of all scores use print.tabs = TRUE.\n\nutility.tables.list(cart_dats, heart_failure,\n                    min.scale = 0, max.scale = 40)\n\n\nTwo-way utility: S_pMSE value plotted for 78 pairs of variables.\n\nVariable combinations with worst 5 utility scores (S_pMSE):\n                          09.sex:10.smoking \n                                     5.1058 \n                   12.deceased:13.follow_up \n                                     2.1157 \n03.creatinine_phosphokinase:08.serum_sodium \n                                     1.9585 \n          05.ejection_fraction:13.follow_up \n                                     1.8282 \n                         01.age:04.diabetes \n                                     1.7879 \n\n\n\n\n\n\n\n\n\n\nMedians and maxima of selected utility measures for all tables compared\n       Medians  Maxima\npMSE    0.0021  0.0098\nS_pMSE  1.0434  5.1058\ndf      9.0000 24.0000\n\nFor more details of all scores use print.tabs = TRUE.\n\n\nHere, we finally see that our parametric synthesis model is severely flawed. Some of the \\(pMSE\\) ratios are larger than 30, which means that these variables are close to useless when the goal is to make inferences. Our non-parametric synthesis model is doing very good. A maximum \\(pMSE\\)-ratio that is smaller than \\(5\\) actually indicates that our synthetic data are of high quality.\n\n\n\n14.2.2 Specific utility measures\nSpecific utility measures assess whether the same analysis on the observed and the synthetic data gives similar results. Say that we are interested in, for instance, the relationship between whether a person survives, the age of this person, whether this person has diabetes and whether or not this person smokes, including the follow-up time as a control variable in the model.\n\n14. Fit this model as a logistic regression model using with(), and pool the results over synthetic data sets. Compare the synthetic data results with the results obtained on the original data, what do you see?\n\nwith(syn_param, glm(deceased ~ age + diabetes + smoking + follow_up, \n                    family = binomial)) |&gt;\n  pool(rule = \"reiter2003\") |&gt;\n  summary() |&gt;\n  as.data.frame() |&gt;\n  tibble::column_to_rownames('term') |&gt;\n  round(3)\n\n            estimate std.error statistic       df p.value\n(Intercept)   -0.776     0.814    -0.953  481.110   0.341\nage            0.014     0.012     1.170  406.170   0.243\ndiabetesYes   -0.169     0.272    -0.620 1533.257   0.535\nsmokingYes     0.068     0.282     0.241 1420.343   0.810\nfollow_up     -0.006     0.002    -2.889  175.166   0.004\n\nwith(syn_cart, glm(deceased ~ age + diabetes + smoking + follow_up, \n                   family = binomial)) |&gt;\n  pool(rule = \"reiter2003\") |&gt;\n  summary() |&gt;\n  as.data.frame() |&gt;\n  tibble::column_to_rownames('term') |&gt;\n  round(3)\n\n            estimate std.error statistic       df p.value\n(Intercept)   -0.729     0.857    -0.851 1915.748   0.395\nage            0.022     0.012     1.813 2702.451   0.070\ndiabetesYes    0.269     0.295     0.910 1275.080   0.363\nsmokingYes    -0.068     0.308    -0.223 2547.864   0.824\nfollow_up     -0.013     0.002    -5.809 1090.560   0.000\n\n glm(deceased ~ age + diabetes + smoking + follow_up, \n     family = binomial,\n     data = heart_failure) |&gt;\n  broom::tidy() |&gt;\n  tibble::column_to_rownames('term') |&gt;\n  round(3)\n\n            estimate std.error statistic p.value\n(Intercept)   -0.847     0.903    -0.937   0.349\nage            0.037     0.013     2.740   0.006\ndiabetesYes    0.110     0.310     0.355   0.722\nsmokingYes    -0.206     0.326    -0.631   0.528\nfollow_up     -0.019     0.003    -7.486   0.000\n\n\nThe results obtained when using solely parametric methods deviate substantially from the results obtained on the original data. When using CART, the results are somewhat different, but are actually quite comparable. All coefficients are estimated with a similar sign, and the deviations from the true relationships are small compared to the standard errors.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Synthetic data in `R`: Generating synthetic data with high utility using `mice`</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ahmad, Assia AND Bhatti, Tanvir AND Munir. 2017. “Survival\nAnalysis of Heart Failure Patients: A Case Study.” PLOS\nONE 12 (7): 1–8. https://doi.org/10.1371/journal.pone.0181001.\n\n\nBreiman, Leo, Jerome Friedman, Charles J Stone, and Richard A Olshen.\n1984. Classification and Regression Trees. New York: CRC press.\nhttps://doi.org/10.1201/9781315139470.\n\n\nChicco, Davide, and Giuseppe Jurman. 2020. “Machine Learning Can\nPredict Survival of Patients with Heart Failure from Serum Creatinine\nand Ejection Fraction Alone.” BMC Medical Informatics and\nDecision Making 20 (1): 16. https://doi.org/10.1186/s12911-020-1023-5.\n\n\nDrechsler, Jörg. 2022. “Challenges in Measuring Utility for Fully\nSynthetic Data.” In Privacy in Statistical Databases,\nedited by Josep Domingo-Ferrer and Maryline Laurent, 220–33. Cham:\nSpringer International Publishing. https://doi.org/10.1007/978-3-031-13945-1_16.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. Cambridge university\npress.\n\n\nHenry, Lionel, and Hadley Wickham. 2022. Purrr: Functional\nProgramming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nHox, Joop, Mirjam Moerbeek, and Rens Van de Schoot. 2010. Multilevel\nAnalysis: Techniques and Applications. Routledge.\n\n\n“IVEware: Imputation and Variance Estimation\nSoftware.” n.d. https://www.src.isr.umich.edu/wp-content/uploads/iveware-manual-Version-0.3.pdf.\n\n\nNowok, Beata, Gillian M. Raab, and Chris Dibben. 2016. “synthpop: Bespoke Creation of Synthetic Data in\nR.” Journal of Statistical Software 74\n(11): 1–26. https://doi.org/10.18637/jss.v074.i11.\n\n\nOberman, Hanne. 2022. Ggmice: Visualizations for ’Mice’ with\n’Ggplot2’. https://CRAN.R-project.org/package=ggmice.\n\n\nPedersen, Thomas Lin. 2022. Patchwork: The Composer of Plots.\nhttps://CRAN.R-project.org/package=patchwork.\n\n\nReiter, Jerome P. 2003. “Inference for Partially Synthetic, Public\nUse Microdata Sets.” Survey Methodology 29 (2): 181–88.\n\n\nRevelle, William. 2022. Psych: Procedures for Psychological,\nPsychometric, and Personality Research. Evanston, Illinois:\nNorthwestern University. https://CRAN.R-project.org/package=psych.\n\n\nRubin, Donald B. 2004. Multiple Imputation for Nonresponse in\nSurveys. Vol. 81. John Wiley & Sons.\n\n\nSchafer, Joseph L. 1997. Analysis of Incomplete Multivariate\nData. CRC press.\n\n\nVan Buuren, Stef. 2012. Flexible Imputation of Missing Data.\nCRC press.\n\n\nVan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “Mice:\nMultivariate Imputation by Chained Equations in r.” Journal\nof Statistical Software 45: 1–67.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations\nin r.” Journal of Statistical Software 45 (3): 1–67. https://doi.org/10.18637/jss.v045.i03.\n\n\nVolker, Thom Benjamin, and Gerko Vink. 2021. “Anonymiced Shareable\nData: Using Mice to Create and Analyze Multiply Imputed Synthetic\nDatasets.” Psych 3 (4): 703–16. https://doi.org/10.3390/psych3040045.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nYucel, Recai M. 2008. “Multiple Imputation Inference for\nMultivariate Multilevel Continuous Data with Ignorable\nNon-Response.” Philosophical Transactions of the Royal\nSociety A: Mathematical, Physical and Engineering Sciences 366\n(1874): 2389–2403.",
    "crumbs": [
      "References"
    ]
  }
]